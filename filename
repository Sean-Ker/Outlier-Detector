 2/1: response.css("title").extract()
 2/2: response.css("title::text").extract()
 2/3: response.css("title::text")[0].extract()
 2/4: response.css("title::ext")[0].extract()
 2/5: response.css("ttle::text").extract()
 2/6: response.css("title::ext").extract_first()
 2/7: cls
 2/8: response.css("span.text").extract()
 2/9: response.css("span.text:text").extract()
2/10: response.css("span.text::text").extract()
2/11: response.css(".text::text").extract()
2/12: response.css(".author::text").extract()
2/13: scrapy shell "https://www.amazon.com/Books-Last-30-days/s?rh=n%3A283155%2Cp_n_publication_date%3A1250226011"
2/14: scrapy
2/15: scrapy shell "https://www.amazon.com/Books-Last-30-days/s?rh=n%3A283155%2Cp_n_publication_date%3A1250226011"
 3/1: response.css(".a-color-base.a-text-normal").extract()
 3/2: response.css(".a-color-base.a-text-normal::text").extract()
 4/1: response.css(".author::text").extract()
 5/1: response
 5/2: response.xpath("//title").extract()
 5/3: response.xpath("//title::text").extract()
 5/4: response.xpath("//title/text()").extract()
 5/5: response.xpath("//span[@class='text]").extract()
 5/6: response.xpath("//span[@class='text]/text()").extract()
 5/7: response.xpath("//span[@class='text']/text()").extract()
 5/8: response.xpath("//span[@class='text']/text()").extract()
 5/9: response.css("li.next" a).xpath("@href").extract()
5/10: response.css("li.next").xpath("@href").extract()
5/11: response.css("li.next a").xpath("@href").extract()
5/12: response.css("a").xpath("@href").extract()
 6/1: response.xpath("//head/title").extract()
 6/2: response.xpath("//head/title/text()").extract()
 6/3: response.xpath("//head/title/text()").extract_first()
 6/4: response.xpath("//head/title::text()").extract_first()
 6/5: response.xpath("//head/title/text()").extract_first()
 6/6: response.xpath("//div[@class=").extract_first()
 6/7: response.xpath("//div[@class='col-md-8'/*/text()").extract_first()
 6/8: response.xpath("//div[@class='col-md-8'/div/text()").extract_first()
 6/9: response.xpath("//div[@class='col-md-8']/div/text()").extract_first()
6/10: response.xpath("//div[@class='col-md-8']/*/text()").extract_first()
6/11: response.xpath("//span[class='text']/text()").extract_first()
6/12: response.xpath("//span[class='text']/text()").extract()
6/13: response.xpath("//div[@class='col-md-8']/*/text()").extract()
6/14: response.xpath("//span[class='text']/text()").extract()
6/15: response.xpath("//span[@class='text']/text()").extract()
6/16: response.xpath("//div[@class='quote']/span[@class='text']/text()").extract()
6/17: response.xpath("//").extract()
6/18: response.css("li.next" a).xpath("/@href").extract()
6/19: response.css("li.next" a).xpath("//@href").extract()
6/20: response.css("li.next" a).xpath("@href").extract()
6/21: response.css("li.next a").xpath("/@href").extract()
6/22: response.css("li.next a").xpath("@href").extract()
6/23: response.css("li.next a").xpath("//@href").extract()
6/24: response.css("/small.author").extract()
6/25: response.css("/small.author::text()").extract()
6/26: response.xpath("//li[@class='next']/a/@href").extract()
6/27: response.xpath("//li[@class='next']/a/@href")[0].extract()
6/28: response.css("/li.next").extract()
6/29: response.css("/li.next/text()").extract()
6/30: response.css("li.next/text()").extract()
6/31: response.css("li.next::text()").extract()
6/32: response.css("li.next").extract()
6/33: response.css("li.next::a").extract()
6/34: response.selector.xpath("//span/text()").get()
6/35: response.selector.xpath("//span/text()").extract()
6/36: response.selector.xpath('//span/text()').get()
6/37: response.xpath('//span/text()').get()
6/38: response.css("span::text")
6/39: response.css("span::text()")
6/40: response.css("span::text").get()
 7/1: response
 7/2: response.selector
 7/3: response.selector.xpath("//title/text()").get()
 7/4: response.selector.xpath("//title/text()").getall()
 7/5: response.css("div.images").xpath("/a/img/@src").getall()
 7/6: response.css("div.images").xpath("/a/@src").getall()
 7/7: response.css("div.images").getall()
 7/8: response.css("div").getall()
 7/9: response.css("a").getall()
7/10: response.css("a").xpath("/img").getall()
7/11: response.css("a").xpath("/img/text()").getall()
7/12: response.css("a").xpath("/img/@src").getall()
7/13: response.css("a").xpath("//img/@src").getall()
7/14: response.css("a").xpath("/img/@src").getall()
7/15: response.css("a").xpath("@src").getall()
7/16: response.css("a").xpath("@src").get()
7/17: response.css("a").xpath("@src").getall()
7/18: response.css("a").xpath("/img/@src").getall()
7/19: response.css("a").xpath("/img").getall()
7/20: response.css("a").xpath("@href").getall()
7/21: response.css("a::img").xpath("@href").getall()
7/22: response.css("a::img").xpath("@src").getall()
7/23: response.css("img").xpath("@src").getall()
7/24: response.xpath("/html/body/div/a/img/@src").getall()
7/25: response.xpath("//div[@id=]a/img/@src").getall()
7/26: response.xpath("//div[@id='images']/a/img/@src").getall()
7/27: response.xpath("//div[@id='images']/a/text()").getall()
7/28: response.xpath("//div[@id='imags']/a/img/@src").getall()
7/29: response.css("img").attrib['src']
7/30: response.css("img").attrib['src'].getall()
7/31: response.css("img").getall().attrib['src']
7/32: response.css("img").attrib['src']
7/33: response.css("img")
7/34: response.css("img").getall()
7/35: response.css("img")
7/36: img.attrib["src"]  for img in response.css("img").getall()
7/37: img.attrib["src"] for img in response.css("img").getall()
7/38: [img.attrib["src"] for img in response.css("img").getall()]
7/39: [img.attrib["src"] for img in response.css("img")]
7/40: img.attrib["src"] for img in response.css("img")
7/41: [img.attrib["src"] for img in response.css("img")]
7/42: response.css("img")
7/43: response.css("img").getall()
7/44: response.css("base").attrib["href"]
7/45: response.css("base").attrib["href"].get
7/46: response.css("base").attrib["href"].get()
7/47: response.css("base").attrib["href"]
7/48: response.css("img"::attr(href)).getall()
7/49: response.css("img::attr(href)").getall()
7/50: response.css("img::attr(href)").get()
7/51: response.css("img::attrib(href)").getall()
7/52: response.css("base::attr(href)").getall()
7/53: response.css("img::attr(src)").getall()
7/54: response.xpath("//a[contains(@href,'image')]/@href").getall()
7/55: response.css("a[href*=image]").getall()
7/56: response.css("a[href*=image]"::attr(href)).getall()
7/57: response.css("a[href*=image]::attr(href)").getall()
7/58: response.css("a[text()*=image]::attr(href)").getall()
7/59: response.css("a[text*=image]::attr(href)").getall()
7/60: response.css("a[text()*=image]).getall()
7/61: response.css("a[text()*=image]").getall()
7/62: response.css("a[text()*=image]/text()").getall()
7/63: response.css("a[text()*=image]/text()").getall()
7/64: response.css("a[href*=image]::attr(href)").getall()
7/65: response.css("a[::text*=image]::attr(href)").getall()
7/66: response.css("a[a::text*=image]::attr(href)").getall()
 8/1: response.css("li.next").xpath("/a/@href").getall()
 8/2: response.css(".next").xpath("/a/@href").getall()
 8/3: response.css("li.next").xpath("/@href").getall()
 8/4: response.css(".next").xpath("/a/@href").extract()
 8/5: response.css("li.next a").xpath("/@href").getall()
 8/6: response.css("li.next a").xpath("@href").getall()
 8/7: response.css("li.next").xpath("//a/@href").getall()
 8/8: response.css("li.next").xpath("/a/@href").getall()
 8/9: response.css("li.next").xpath("/a//@href").getall()
8/10: response.css("li.next").xpath("a/@href").getall()
8/11: response.css("li.next").xpath("//a/@href").getall()
8/12: response.xpath("//li[class='next']/a/@href").getall()
8/13: response.xpath("//li[class='next']/@href").getall()
8/14: response.xpath("//li[class='next']/a/@href").getall()
8/15: response.xpath("//li[class='next']/a::@href").getall()
8/16: response.xpath("//a/@href").getall()
8/17: response.xpath("//li[class='next']//a/@href").getall()
8/18: response.xpath("//li[class='next']/a/@href").getall()
8/19: response.xpath("//li[class=next]/a/@href").getall()
8/20: response.xpath("//li[class='next']/*/@href").getall()
8/21: response.xpath("//li[@class='next']/a/@href").getall()
 9/1: response.css("div.quote")[0].css("div.tags").xpath("/a/text()").getall()
 9/2: response.css("div.quote div.tags").xpath("/a/text()").getall()
 9/3: response.css("div.quote div.tags").getall()
 9/4: response.css("div.quote div.tags").xpath("a/text()").getall()
 9/5: response.css("div.quote div.tags").xpath("//a/text()").getall()
10/1: item = response.css("div.quote")[0]
10/2: item.xpath("//a[@class='tag']/text()").getall()
10/3: item
10/4: item.get()
10/5: item
10/6: item.get()
10/7: item.xpath("//a[@class='tag']/text()").getall()
10/8: item.select.xpath("//a[@class='tag']/text()").getall()
10/9: item.selector.xpath("//a[@class='tag']/text()").getall()
10/10: item.get().xpath("//a[@class='tag']/text()").getall()
10/11: item.xpath("a[@class='tag']/text()").getall()
10/12: item.xpath("/a[@class='tag']/text()").getall()
10/13: item.xpath("a[@class='tag']").getall()
10/14: item.xpath("a[@class='tag']")
10/15: response.xpath("a[@class='tag']").getall()
10/16: response.xpath("a[@class='tag']/text()").getall()
10/17: response.xpath("*[@class='tag']/text()").getall()
10/18: response.xpath("//a[@class='tag']/text()").getall()
10/19: response.xpath("/a[@class='tag']/text()").getall()
10/20: item.xpath("//a[@class='tag']").getall()
10/21: item.xpath("//a[@class='tag']/text()").getall()
10/22: item.xpath("/a[@class='tag']/text()").getall()
10/23: item.xpath("a[@class='tag']/text()").getall()
10/24: item.css(.tag::text).getall()
10/25: item.css(".tag::text").getall()
11/1: response.css("div.quote")
11/2: all_quotes = response.css("div.quote")
11/3:
for quotes in all_quotes:
    quote = quotes.css("span.text::text").getall()
    author = quotes.css(".author::text").getall()
    tags = quotes.css(".tag::text").getall()
11/4:
for quotes in all_quotes:
    quote = quotes.css("span.text::text").getall()
    author = quotes.css(".author::text").getall()
    tags = quotes.css(".tag::text").getall()
    yield { 'quote': quote,
                    'author': author,
                    'tags': tags}
11/5:
for quotes in all_quotes:
    quote = quotes.css("span.text::text").getall()
    author = quotes.css(".author::text").getall()
    tags = quotes.css(".tag::text").getall()
    yield { 'quote': quote,'author': author, 'tags': tags}
11/6:
for quotes in all_quotes:
    quote = quotes.css("span.text::text").getall()
    author = quotes.css(".author::text").getall()
    tags = quotes.css(".tag::text").getall()
    print( 'quote': quote,
                    'author': author,
                    'tags': tags)
11/7:
for quotes in all_quotes:
    quote = quotes.css("span.text::text").getall()
    author = quotes.css(".author::text").getall()
    tags = quotes.css(".tag::text").getall()
    print( 'quote: '+ quote+ 'author: '+'tags: '+tags)
11/8:
for quotes in all_quotes:
    quote = quotes.css("span.text::text").getall()
    author = quotes.css(".author::text").getall()
    tags = quotes.css(".tag::text").getall()
    print( 'quote: '+ quote+ 'author: '+'tags: ')
    print(tags)
11/9:
for quotes in all_quotes:
    quote = quotes.css("span.text::text").getall()
    author = quotes.css(".author::text").getall()
    tags = quotes.css(".tag::text").getall()
    print( 'quote: '+ quote+ 'author: '+'tags: ')
    print(*tags)
11/10:
for quotes in all_quotes:
    quote = quotes.css("span.text::text").getall()
    author = quotes.css(".author::text").getall()
    tags = quotes.css(".tag::text").getall()
    print( 'quote: '+ quote+ 'author: '+'tags: ')
    print(*tags,sep=' ')
11/11:
for quotes in all_quotes:
    quote = quotes.css("span.text::text").get()
    author = quotes.css(".author::text").get()
    tags = quotes.css(".tag::text").getall()
    print( 'quote: '+ quote+ 'author: '+'tags: ')
    print(*tags)
12/1: response.url
12/2: response.url.split("/")[-1] + '.html'
12/3: [response.url.split("/")[-1] + '.html']
12/4: filename = response.url.split("/")[-1] + '.html'
12/5: filename
12/6: filename = response.url.split("/")[-1]
12/7: filename
12/8: filename = response.url.split("/")[-1] + '.html'
12/9: filename
12/10: filename = response.url.split("/")
12/11: filename
13/1: response.url.split("/")[-1] + '.html'
14/1: response.url.split("/")[-1] + '.html'
14/2:
with open(filename, 'wb') as f:
            f.write(response.body)
            
14/3:
with open(filename, 'wb') as f:
    f.write(response.body)
14/4: filename = response.url.split("/")[-1] + '.html'
14/5:
with open(filename, 'wb') as f:
    f.write(response.body)
14/6:
with open(filename, 'wb') as f:
    f.write(response.css("div.quote"))
14/7: response.body
14/8: response.css("div.quote")
14/9: response.css("div.quote").getall()
14/10:
with open(filename, 'wb') as f:
    f.write(response.css("div.quote")).getall()
14/11:
with open(filename, 'wb') as f:
    f.write(response.css("div.quote")).get()
14/12: response.body
14/13: response.body.get()
14/14:
with open(filename, 'wb') as f:
    f.write(response.css("div.quote")).body()
14/15:
with open(filename, 'wb') as f:
    f.write(response.css("div.quote")).body
14/16:
with open(filename, 'wb') as f:
    f.write(response.css("div.quote").body())
14/17: response.css("div.quote")
14/18: response.css("div.quote").body()
14/19: response.css("div.quote").body
14/20: sel = Selector(response)
14/21: sel = scrapy.selector.Selector(response)
14/22: sel
14/23: sel.xpath("//h1")
14/24: sel.xpath("//h1").getall()
14/25: sel.xpath("//dif.quote").getall()
14/26: sel.xpath("//div").getall()
14/27: sel.xpath("//div[@class='quote']").getall()
14/28: sel.xpath("//div[@class='quote']").get()
14/29:
with open(filename, 'wb') as f:
    f.write(response.css("div.quote").body())
    exit
15/1:
filename = response.url.split("/")[-1] + '.html'
        sel = scrapy.selector.Selector(response)

        with open(filename, 'wb') as f:
            f.write(sel.xpath("//div[@class='quote']").getall())
15/2: filename = response.url.split("/")[-1] + '.html'
15/3:         sel = scrapy.selector.Selector(response)
15/4: sel
15/5:
 with open(filename, 'wb') as f:
    f.write(sel.xpath("//div[@class='quote']").getall())
15/6:
 with open(filename, 'wb') as f:
    f.write(sel.xpath("//div[@class='quote']").get())
15/7:
 with open(filename, 'w') as f:
    f.write(sel.xpath("//div[@class='quote']").getall())
15/8:
 with open(filename, 'w') as f:
    f.write(sel.xpath("//div[@class='quote']").get())
16/1: filename = response.url.split("/")[-1] + '.html'
16/2:         sel = scrapy.selector.Selector(response)
16/3: sel
16/4:         outPut = sel.css(".layout-two-column-wide header, .lesson-content").getall()
16/5: outPut
17/1: print("working")
17/2: print("working")
22/1: 2+2
22/2: debugfile('C:/Users/user/.spyder-py3/temp.py', wdir='C:/Users/user/.spyder-py3')
22/3: /exit
26/1: li = list(range(100))
26/2: d = {'a': 1, 'b': 2}
26/3:
spyder
version
--version
33/1: import tensorflow as tf
33/2: import tensorflow as tf
33/3: import tensorflow as tf
33/4: _start_server()
33/5: import tensorflow as tf
33/6: import tensorflow as tf
34/1: python --version
34/2: 2+2
34/3: import tensorflow as tf
34/4: python
41/1: import tensorflow as tf
42/1: import tensorflow as tf
42/2: print(tf.__version__)
43/1: !wget https://archive.ics.uci.edu/ml/machine-learning-databases/arrhythmia/arrhythmia.data
43/2: !ls
43/3: !head arrhythmia.data
43/4: !dir
43/5:
import pandas as pd
df = pd.read_csv('arrhythmia.data', header=None)
43/6:
data = df[[0,1,2,3,4,5]]
data.columns = ['age', 'sex', 'height', 'weight', 'QRS duration', 'P-R interval']
43/7: !wget https://archive.ics.uci.edu/ml/machine-learning-databases/arrhythmia/arrhythmia.data
43/8: !dir
43/9: !head arrhythmia.data
43/10:
import pandas as pd
df = pd.read_csv('arrhythmia.data', header=None)
43/11:
data = df[[0,1,2,3,4,5]]
data.columns = ['age', 'sex', 'height', 'weight', 'QRS duration', 'P-R interval']
43/12:
import matplotlib.pyplot as plt
plt.rcParams['figure.figsize'] = [15, 15] # make the plot bigger so the subplots don't overlap
data.hist(); # use a semicolon to supress return value
43/13: !pip install matplotlib
43/14:
from pandas.plotting import scatter_matrix
scatter_matrix(data);
43/15: !wget https://archive.ics.uci.edu/ml/machine-learning-databases/arrhythmia/arrhythmia.data
43/16: !dir
43/17: !head arrhythmia.data
43/18:
import pandas as pd
df = pd.read_csv('arrhythmia.data', header=None)
43/19:
data = df[[0,1,2,3,4,5]]
data.columns = ['age', 'sex', 'height', 'weight', 'QRS duration', 'P-R interval']
43/20:
import matplotlib.pyplot as plt
plt.rcParams['figure.figsize'] = [15, 15] # make the plot bigger so the subplots don't overlap
data.hist(); # use a semicolon to supress return value
43/21:
from pandas.plotting import scatter_matrix
scatter_matrix(data);
43/22: url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data'
43/23:
!pip install -q tensorflow==2.0.0-beta1
import tensorflow as tf
print(tf.__version__)
43/24: tf.keras.utils.get_file('auto-mpg.data', url)
43/25: !head /root/.keras/datasets/auto-mpg.data
43/26:
df = pd.read_csv('/root/.keras/datasets/auto-mpg.data', header=None, delim_whitespace=True)
df.head()
43/27:
import tensorflow as tf
print(tf.__version__)
43/28:
df = pd.read_csv('auto-mpg.data', header=None, delim_whitespace=True)
df.head()
43/29:
df = pd.read_csv('auto-mpg.data', header=None, delim_whitespace=True)
df.head()
43/30:
df = pd.read_csv('/auto-mpg.data', header=None, delim_whitespace=True)
df.head()
43/31:
df = pd.read_csv('C:\\Users\\user\\.keras\\datasets\\auto-mpg.data', header=None, delim_whitespace=True)
df.head()
44/1: import gym
44/2: from stable_baselines import PPO2
44/3: import tensorflow as tf
44/4: tf.__version__
45/1: from stable_baselines import PPO2
46/1: import tensorflow as tf
47/1: import tensorflow as tf
47/2: tf.__version__
47/3: tf -h
48/1: import tensorflow as tf
48/2: tf.version
48/3: tf.__version__
48/4: eixt()
49/1: import tensorflow
49/2: tensorflow.__version
49/3: tensorflow.__version__
49/4: tensorflow.version
49/5: eixt
50/1: import tensorflow as tf
50/2: tf.version
50/3: tf.__version__
50/4: mnist = tf.keras.datasets.mnist  (x_train, y_train), (x_test, y_test) = mnist.load_data() x_train, x_test = x_train / 255.0, x_test / 255.0
50/5: mnist = tf.keras.datasets.mnist
51/1: import tensorflow as tf
51/2: tf.__version__
51/3: tf.version
52/1: import tensorflow as tf
52/2: sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))
53/1: import tensorflow
54/1: from stable_baselines import PPO2
54/2: model = PPO2('MlpPolicy', 'CartPole-v1').learn(10000)
54/3: model.learn(total_timesteps=10000)
54/4: obs = env.reset()
54/5: pip install pytest pytest-cov
54/6: make pytest
55/1: import gym
55/2: 
55/3: 
56/1: import gym
56/2: conda list gym
57/1: import gym
57/2: env = gym.make('CartPole-v0')
57/3: env.reset()
57/4: for _ in range(1000):     env.render()     env.step(env.action_space.sample()) # take a random action env.close()
57/5:
for _ in range(1000):
        env.step(env.action_space.sample()) # take a random action
        env.close()
env.close()
57/6: print(env.action_space)
57/7: print(env.observation_space)
57/8: 
58/1:
import tensorflow as tf
print( tf.__version__)
a = 5
print(a)
print(a+1)
60/1: import numpy as np
60/2: import numpy as np
60/3: import tensorflow as tf
60/4: tf.__version__
60/5: tf.__version__
60/6: tf.__version__
60/7: tf.__version__
60/8: tf.__version__
60/9: import tensorflow as tf
69/1: a =5
69/2: a =5
69/3: print a
69/4: print(a)
69/5: print(a)
73/1:
import tensorflow as tf
import sys

print(tf.__version__)
print(sys.version)
print(sys.executable)

a = 5
75/1:
print(tf.__version__)
print(sys.version)
print(sys.executable)

a = 5
87/1: LIVES = 4
87/2: LIVES = 4
87/3:
import pygame
import math
import pygame.mixer
import numpy as np
import time
import seaborn as sns
import webcolors

from grid_world import Grid, Tile
# Constants

x_grid = 6
y_grid = 6

LIVES = 4
# the WIDTH and HEIGHT of each grid
# This sets the margin between each cell

PIXELS_PER_TILE = 160

MARGIN = 10
# Define some colors

color_platte = sns.color_palette(["#9b59b6", "#3498db", "#95a5a6", "#e74c3c", "#34495e", "#2ecc71"])
BLACK = (0, 0, 0)
WHITE = (255, 255, 255)
GREEN = (0, 255, 0)
RED = (255, 0, 0)
PURPLE = webcolors.hex_to_rgb("#9b59b6")
SIZE = [(PIXELS_PER_TILE+MARGIN)*x_grid + MARGIN, (PIXELS_PER_TILE+MARGIN)*y_grid+MARGIN]
print(f"Grid Size: {x_grid}x{y_grid}: ({SIZE[0]}, {SIZE[1]})px")
pygame.init()

screen = pygame.display.set_mode(SIZE)
# screen = pygame.display.set_mode(SIZE, pygame.HWSURFACE | pygame.DOUBLEBUF | pygame.RESIZABLE| pygame.FULLSCREEN)

pygame.display.set_caption('Krabby Patty Hunter')
clock = pygame.time.Clock()
setattr(Tile, 'PIXELS_PER_TILE', PIXELS_PER_TILE)
setattr(Tile, 'MARGIN', MARGIN)

grid = None
# print("Right Path: ", grid.path)
# print(grid.player,grid.player.screen_coords())
# print(grid.burger, grid.burger.screen_coords())
# for col in grid.array:
#     for tile in col:
#         print(tile)
# print(grid.actions)

heart = pygame.image.load(Tile.ASSETS['heart'])
heart = pygame.transform.scale(heart,(int(PIXELS_PER_TILE/2),int(PIXELS_PER_TILE/2)))

def initialize_round():
    global grid
    grid = Grid(x_grid, y_grid, lives=LIVES)
    print("Right Path: ", grid.path)
    # print(grid.score_max)

def game_loop():
    # print(f'User Grid: {[player.x, player.y]}')
    # print(f'User Coords: {player.screen_coords()}')
    # print(f'Player Size: {player.image.get_size()}')
    global game_over
    # pygame.event.pump()
    for event in pygame.event.get():
        if event.type == pygame.QUIT:
            game_over = True
        if event.type == pygame.KEYDOWN:
            if event.key == pygame.K_ESCAPE:
                game_over = True
            elif event.key == pygame.K_RIGHT:
                grid.move(1, 0)
            elif event.key == pygame.K_UP:
                grid.move(0, -1)
            print(grid.score, grid.lives)
            if grid.done and grid.score == grid.score_max:
                print("You Won!")
    # return game_over

def draw_loop():
    screen.fill(PURPLE)
    for col in grid.array:
        for tile in col:
            draw(tile)
    if not grid.burger == None:
        draw(grid.burger)
    for plank in grid.plankton:
        draw(plank)
    draw(grid.player)
    for i in range(grid.lives):
        screen.blit(heart,(MARGIN + i*(heart.get_width())+MARGIN*i%2,MARGIN))
    pygame.display.update()
    clock.tick(60)
# Contributed code by pygame.com

def preprocess_image(asset, bx, by):
    ix, iy = asset.get_size()
    if ix > iy:  # fit to width
        scale_factor = bx/float(ix)
        sy = scale_factor * iy
        if sy > by:
            scale_factor = by/float(iy)
            sx = scale_factor * ix
            sy = by
        else:
            sx = bx
    else:  # fit to height
        scale_factor = by/float(iy)
        sx = scale_factor * ix
        if sx > bx:
            scale_factor = bx/float(ix)
            sx = bx
            sy = scale_factor * iy
        else:
            sy = by
    return pygame.transform.scale(asset, (int(sx), int(sy)))

def draw(tile):
    screen.blit(preprocess_image(tile.image, PIXELS_PER_TILE,
                                 PIXELS_PER_TILE), tile.screen_coords())

game_over = False

while not game_over:
    initialize_round()
    while not grid.done and not game_over:
        game_loop()
        draw_loop()
    else:
        print(f'Score: {grid.score}')
else:
    pygame.quit()
    quit()
87/4: PURPLE
87/5: PURPLE
87/6: LIVES
87/7: heart = pygame.image.load(Tile.ASSETS['heart'])
89/1: heart
89/2: heart = pygame.image.load(Tile.ASSETS['heart'])
89/3:
import pygame
import math
import pygame.mixer
import numpy as np
import time
import seaborn as sns
import webcolors

from grid_world import Grid, Tile
# Constants

x_grid = 6
y_grid = 6

LIVES = 4
# the WIDTH and HEIGHT of each grid
# This sets the margin between each cell

PIXELS_PER_TILE = 160

MARGIN = 10
# Define some colors

color_platte = sns.color_palette(["#9b59b6", "#3498db", "#95a5a6", "#e74c3c", "#34495e", "#2ecc71"])
BLACK = (0, 0, 0)
WHITE = (255, 255, 255)
GREEN = (0, 255, 0)
RED = (255, 0, 0)
PURPLE = webcolors.hex_to_rgb("#9b59b6")
SIZE = [(PIXELS_PER_TILE+MARGIN)*x_grid + MARGIN, (PIXELS_PER_TILE+MARGIN)*y_grid+MARGIN]
print(f"Grid Size: {x_grid}x{y_grid}: ({SIZE[0]}, {SIZE[1]})px")
pygame.init()

screen = pygame.display.set_mode(SIZE)
# screen = pygame.display.set_mode(SIZE, pygame.HWSURFACE | pygame.DOUBLEBUF | pygame.RESIZABLE| pygame.FULLSCREEN)

pygame.display.set_caption('Krabby Patty Hunter')
clock = pygame.time.Clock()
setattr(Tile, 'PIXELS_PER_TILE', PIXELS_PER_TILE)
setattr(Tile, 'MARGIN', MARGIN)

grid = None
# print("Right Path: ", grid.path)
# print(grid.player,grid.player.screen_coords())
# print(grid.burger, grid.burger.screen_coords())
# for col in grid.array:
#     for tile in col:
#         print(tile)
# print(grid.actions)
89/4: !ls
89/5: !dier
89/6: !dir
89/7: cd C:\_Workspace\Python\zother_projects\data_patty_hunter\data_patty_hunter\game.py
89/8: cd C:\\_Workspace\\Python\\zother_projects\\data_patty_hunter\\data_patty_hunter\\game.py
89/9: cd "C:\_Workspace\Python\zother_projects\data_patty_hunter\data_patty_hunter\game.py"
89/10: cd C:\_Workspace\Python\zother_projects\data_patty_hunter\data_patty_hunter\
89/11:
import pygame
import math
import pygame.mixer
import numpy as np
import time
import seaborn as sns
import webcolors

from grid_world import Grid, Tile
# Constants

x_grid = 6
y_grid = 6

LIVES = 4
# the WIDTH and HEIGHT of each grid
# This sets the margin between each cell

PIXELS_PER_TILE = 160

MARGIN = 10
# Define some colors

color_platte = sns.color_palette(["#9b59b6", "#3498db", "#95a5a6", "#e74c3c", "#34495e", "#2ecc71"])
BLACK = (0, 0, 0)
WHITE = (255, 255, 255)
GREEN = (0, 255, 0)
RED = (255, 0, 0)
PURPLE = webcolors.hex_to_rgb("#9b59b6")
SIZE = [(PIXELS_PER_TILE+MARGIN)*x_grid + MARGIN, (PIXELS_PER_TILE+MARGIN)*y_grid+MARGIN]
print(f"Grid Size: {x_grid}x{y_grid}: ({SIZE[0]}, {SIZE[1]})px")
pygame.init()

screen = pygame.display.set_mode(SIZE)
# screen = pygame.display.set_mode(SIZE, pygame.HWSURFACE | pygame.DOUBLEBUF | pygame.RESIZABLE| pygame.FULLSCREEN)

pygame.display.set_caption('Krabby Patty Hunter')
clock = pygame.time.Clock()
setattr(Tile, 'PIXELS_PER_TILE', PIXELS_PER_TILE)
setattr(Tile, 'MARGIN', MARGIN)

grid = None
# print("Right Path: ", grid.path)
# print(grid.player,grid.player.screen_coords())
# print(grid.burger, grid.burger.screen_coords())
# for col in grid.array:
#     for tile in col:
#         print(tile)
# print(grid.actions)
90/1: heart
90/2: LIVES
90/3: LIVES
90/4: LIVEs
90/5: LIVES
90/6: heart = pygame.image.load(Tile.ASSETS['heart'])
98/1: # Only supports a 4 by 4 grid and 1 live gird OR
98/2: # Only supports a 4 by 4 grid and 1 live gird OR
98/3: games_log = []
101/1: from util_funcs import load_data, read_pickle, write_pickle
101/2: df = load_data('btcusdt','30m')
101/3:
breakpoint()
interval
103/1:
breakpoint()
current_ms = lambda: int(time.time() * 1000.0)
current_datetime = lambda: ms_to_datetime(current_ms())

interval_to_min = lambda interval: interval[:-1]

datetime_to_ms = lambda dt: int(dt.timestamp() * 1000.0)
ms_to_datetime = lambda ms: datetime.fromtimestamp(ms/1000.0)

def read_pickle(file_name):
    pkl = None
    with open(f'{DATA_DIR}{file_name}.pkl', 'rb') as f:
        pkl = pickle.load(f)
    return pkl

def write_pickle(obj, file_name):
    with open(f'{DATA_DIR}{file_name}.pkl', 'wb') as f:
        pickle.dump(obj,f)

def write_fsb_csv(df, symbol):
    assert(not df.empty)
    symbol = symbol[:6]
    # print(df.head())
    interval = min_to_interval[int(round((df.close_time[0].timestamp() - df.index[0].timestamp())/60, 0))]
    time_frame = interval_to_milliseconds(interval)
    # print(time_frame)

    # new_dates, new_times = zip(*[(d.date(), d.time().strftime('%H:%M')) for d in df['open_time']])
    # df = df.assign(new_date=new_dates, new_time=new_times)
    # cols = ['new_date','new_time','open','high','low','close','volume']
    df.volume = round(df['volume'], 0)

    cols = ['open', 'high', 'low', 'close', 'volume']
    df_new = df.copy()[cols]
    df_new.index = [x.strftime('%Y-%m-%d %H:%M').lstrip("0").replace(" 0", " ") for x in df_new.index.copy()]
    df_new.update(df_new.select_dtypes(include=np.float64).applymap('{:g}'.format))
    df_new = df_new.astype({'volume': 'int32', 'open': 'float64',
                    'high': 'float64', 'low': 'float64', 'close': 'float64'})
    # print(df_new)
    # print(df_new.dtypes)
    df_new.to_csv(f'{FSB_DIR}{symbol}{time_frame}.csv', sep="\t", index=True, header=False)

def update_data(df,symbol):
    assert(not df.empty)

    interval = min_to_interval[int(round((df.close_time[0].timestamp() - df.index[0].timestamp())/60, 0))]
    time_frame = interval_to_milliseconds(interval)
    symbol = symbol.upper()

    df_start_dt =  ms_to_datetime(datetime_to_ms(df.index[0]))
    df_end_dt = ms_to_datetime(datetime_to_ms(df.index[-1]))
    print(f'Local DataFrame: {df_start_dt} --> {df_end_dt} ({df_end_dt-df_start_dt}).')

    earlies_dt  = ms_to_datetime(client.get_klines(symbol=symbol,interval=interval,limit=1,startTime=0)[0][0])
    latest_dt = current_datetime()

    # print('earliest and latest valid ts: ',earlies_ts,latest_ts)

    if earlies_dt<df_start_dt:
        print(f'Getting preceding {symbol}{interval} klines between {earlies_dt} -> {df_start_dt} ({df_start_dt-earlies_dt})...')
        df_before = preprocess_klines(client.get_historical_klines(symbol,interval, datetime_to_ms(earlies_dt),datetime_to_ms(df_start_dt),limit = 1000))
        df = pd.concat([df_before,df])

    if latest_dt>df_end_dt:
        print(f'Getting postceding {symbol}{interval} klines between {df_end_dt} -> {latest_dt} ({latest_dt-df_end_dt})...')
        df_after = preprocess_klines(client.get_historical_klines(symbol,interval,datetime_to_ms(df_end_dt),datetime_to_ms(latest_dt),limit = 1000))
        df = pd.concat([df,df_after])
    df.sort_index(inplace = True)
    return df

def preprocess_klines(klines):
    df = pd.DataFrame(klines, columns=['open_time', 'open', 'high', 'low', 'close', 'volume', 'close_time',
                                       'quote_asset_volume', 'trades', 'taker_buy_base_asset_volume',
                                       'taker_buy_quote_asset_volume', 'symbol'], dtype='float64')
    df.drop('symbol', 1, inplace=True)
    df.round({'open': 2, 'high': 2, 'low': 2, 'close': 2,
              'volume': 6, 'taker_buy_base_asset_volume': 6})

    df.quote_asset_volume = df.quote_asset_volume.astype(int)
    df.trades = df.trades.astype(int)
    df.taker_buy_quote_asset_volume = df.taker_buy_quote_asset_volume.astype(int)

    df.open_time = pd.to_datetime(df.open_time, unit='ms')
    df.close_time = pd.to_datetime(df.close_time, unit='ms')

    df.set_index('open_time', inplace=True)
    df.sort_index(inplace=True)
    return df

def scrape_all(symbols, intervals):
    for symbol in symbols:
        for interval in intervals:
            load_data(symbol, interval)

def load_data(symbol, interval):
    assert(interval in ['1m', '5m', '15m', '30m','1h', '4h', '1d', '1w'])
    tf = interval_to_min(interval)
    symbol = symbol.upper()
    file_name = f'{symbol}{interval}'

    df = df_current= pd.DataFrame()
    if os.path.isfile(DATA_DIR + file_name + '.pkl'):
        df = df_current = read_pickle(file_name) 

    if df.empty:
        klines = client.get_historical_klines(symbol, interval, date_to_milliseconds('1970-01-02'), limit = 1000)
        df = preprocess_klines(klines)
    else:
        df = update_data(df,symbol)
    print(f'Finished Loading {symbol}{interval}')
    
    if not df_current.equals(df):
        write_pickle(df,file_name)
        write_fsb_csv(df,symbol)
    return df
105/1:
from util_funcs import load_data, read_pickle, write_pickle
df = load_data('btcusdt','30m')
105/2: ceil(3)
105/3:
round(3.4
)
105/4: round(3.5)
105/5: import math
105/6:
from util_funcs import load_data, read_pickle, write_pickle
df = load_data('btcusdt','30m')
print(df)
105/7: df = load_data('btcusdt','30m')
105/8:
breakpoint()
def build_features(df):
    features=pd.DataFrame(index=df.index)
    func_names = get_talib_func_names()
    for func_name in tqdm(func_names):
        # print(func_name)
        func = abstract.Function(func_name)
        if (len(func.info['output_names'])==1):
            res=func(df)
        elif (len(func.info['output_names'])==2): 
            out=func(df)
            res=out.iloc[:,1]-out.iloc[:,0]
        else: pass
    
        res=pd.DataFrame(res,columns=[func_name])
        
        if features.empty:
            features=res
        else:
            features = features.join(res)
    return features

# Includes classifying function
def build_targets(data_frame, mode = 'atr_multiplier', param = 1):
    assert(mode in ['atr_multiplier', 'pct_change', 'take_profit'])
    df=data_frame.copy()

    if mode=='atr_multiplier' and not 'atr' in df.columns:
        df['atr'] = talib.abstract.Function('ATR')(df)
    # targets2 = pd.DataFrame([np.nan] * len(df), index = df.index, columns=['target'])
    targets=[]
    # df.dropna(inplace = True)
    for i in tqdm(range(len(df))):
        price = df.iloc[i]['close']

        if mode=='atr_multiplier':
            val = df.iloc[i]['atr'] * param
        elif mode == 'pct_change':
            val = (1+param)*price
        elif mode =='take_profit': 
            val = param
        else:
            raise 
        
        if np.isnan(val):
            targets.append(np.nan)
            continue
            
        buy_target = price + val 
        sell_target = price - val
        
        for k in range(i+1,len(df)):
            res=np.nan
            if df.iloc[k]['close'] >= buy_target:
                res = k-i
                break
            elif df.iloc[k]['close'] <= sell_target:
                res = -(k-i)
                break
        targets.append(res)
    return pd.Series(targets,index=df.index)

def build_signals(targets,threshold = float('inf')):
    res = []*len(targets)
    for t in targets:
        if np.isnan(t): res.append(np.nan) 
        elif abs(t) >= threshold: res.append(0)
        elif t > 0: res.append(1)
        elif t < 0: res.append(-1)
        else: assert()
    return pd.Series(res,index=targets.index)

def build_buy_sell(signals,close, margin=0.01):
    assert(len(signals)==len(close))
    # array_buy = signals.copy()
    # array_sell = signals.copy()
    # array_buy[array_buy>0] = np.nan
    # array_sell[array_sell<0] = np.nan
    array_buy=[]
    array_sell=[]
    for i in range(len(close)):
        array_buy.append(close[i]*(1-margin) if signals[i]>0 else np.nan)
        array_sell.append(close[i]*(1+margin) if signals[i]<0 else np.nan)
    return (array_buy,array_sell)

def write_dfts_pickle(symbol, interval, data_features_targets_signals):
    assert(interval in intervals)
    symbol = symbol.upper()
    write_pickle(data_features_targets_signals, f'{DFTS_DIT}{symbol}{interval}')

def read_dfts_pickle(symbol,interval):
    assert(interval in intervals)
    symbol = symbol.upper()
    read_pickle(DFTS_DIT+symbol+interval)

def get_talib_func_names(): [x for x in talib.get_functions() if x not in BAD_IND]
105/9:
breakpoint()
def build_features(df):
    features=pd.DataFrame(index=df.index)
    func_names = get_talib_func_names()
    for func_name in tqdm(get_talib_func_names()):
        # print(func_name)
        func = abstract.Function(func_name)
        if (len(func.info['output_names'])==1):
            res=func(df)
        elif (len(func.info['output_names'])==2): 
            out=func(df)
            res=out.iloc[:,1]-out.iloc[:,0]
        else: pass
    
        res=pd.DataFrame(res,columns=[func_name])
        
        if features.empty:
            features=res
        else:
            features = features.join(res)
    return features

# Includes classifying function
def build_targets(data_frame, mode = 'atr_multiplier', param = 1):
    assert(mode in ['atr_multiplier', 'pct_change', 'take_profit'])
    df=data_frame.copy()

    if mode=='atr_multiplier' and not 'atr' in df.columns:
        df['atr'] = talib.abstract.Function('ATR')(df)
    # targets2 = pd.DataFrame([np.nan] * len(df), index = df.index, columns=['target'])
    targets=[]
    # df.dropna(inplace = True)
    for i in tqdm(range(len(df))):
        price = df.iloc[i]['close']

        if mode=='atr_multiplier':
            val = df.iloc[i]['atr'] * param
        elif mode == 'pct_change':
            val = (1+param)*price
        elif mode =='take_profit': 
            val = param
        else:
            raise 
        
        if np.isnan(val):
            targets.append(np.nan)
            continue
            
        buy_target = price + val 
        sell_target = price - val
        
        for k in range(i+1,len(df)):
            res=np.nan
            if df.iloc[k]['close'] >= buy_target:
                res = k-i
                break
            elif df.iloc[k]['close'] <= sell_target:
                res = -(k-i)
                break
        targets.append(res)
    return pd.Series(targets,index=df.index)

def build_signals(targets,threshold = float('inf')):
    res = []*len(targets)
    for t in targets:
        if np.isnan(t): res.append(np.nan) 
        elif abs(t) >= threshold: res.append(0)
        elif t > 0: res.append(1)
        elif t < 0: res.append(-1)
        else: assert()
    return pd.Series(res,index=targets.index)

def build_buy_sell(signals,close, margin=0.01):
    assert(len(signals)==len(close))
    # array_buy = signals.copy()
    # array_sell = signals.copy()
    # array_buy[array_buy>0] = np.nan
    # array_sell[array_sell<0] = np.nan
    array_buy=[]
    array_sell=[]
    for i in range(len(close)):
        array_buy.append(close[i]*(1-margin) if signals[i]>0 else np.nan)
        array_sell.append(close[i]*(1+margin) if signals[i]<0 else np.nan)
    return (array_buy,array_sell)

def write_dfts_pickle(symbol, interval, data_features_targets_signals):
    assert(interval in intervals)
    symbol = symbol.upper()
    write_pickle(data_features_targets_signals, f'{DFTS_DIT}{symbol}{interval}')

def read_dfts_pickle(symbol,interval):
    assert(interval in intervals)
    symbol = symbol.upper()
    read_pickle(DFTS_DIT+symbol+interval)

def get_talib_func_names(): [x for x in talib.get_functions() if x not in BAD_IND]
105/10: df = load_data(symbol, interval)
105/11:
symbol = 'BTCUSDT'
interval = '30m'
105/12: df = load_data(symbol, interval)
105/13:
df.tail(
)
105/14: df = load_data(symbol, interval)
105/15: def preprocess_features(features):
105/16: [df,features, targets,signals]=read_dfts_pickle(symbol,interval)
105/17: [df,features, targets,signals]=read_dfts_pickle(symbol,interval)
106/1:
from util_funcs import *
symbol = 'BTCUSDT'

interval = '30m'
# df = load_data(symbol, interval)
# # print(df.tail())
# features = build_features(df)
# targets = build_targets(df,mode='atr_multiplier',param=1.5)
# signals = build_signals(targets)
# write_dfts_pickle(symbol, interval, [df,features,targets,signals])
# def preprocess_features(features):

[df,features, targets,signals]=read_dfts_pickle(symbol,interval)
# assert()
107/1:
from util_funcs import *
symbol = 'BTCUSDT'
interval = '15m'
mode = 'atr_multiplier'
param = 1
107/2: [df,features, targets,signals]=read_dfts_pickle(symbol,interval)
107/3: [df,features, targets,signals]=read_dfts_pickle(symbol,interval,mode,param)
107/4:
from util_funcs import *
symbol = 'BTCUSDT'
interval = '30m'
mode = 'atr_multiplier'
param = 1

def fix_data(n):
    assert(n>0)
    df = read_pickle(f'{symbol}{interval}')
    is_good_data(df,interval)
    write_pickle(df[:-n],f'{symbol}{interval}')

def build_dfts():
    df = load_data(symbol, interval)
    features = build_features(df)
    targets = build_targets(df,mode,param)
    signals = build_signals(targets)
    write_dfts_pickle(symbol, interval,mode,param, [df,features,targets,signals])

def preprocess_features(features):
    return 1

def main():
    [df,features, targets,signals]=read_dfts_pickle(symbol,interval,mode,param)
    preprocess_features(features)
# main()
# fix_data(1)

df = load_data(symbol,interval)
107/5: df = read_pickle(f'{symbol}{interval}')
107/6: df = load_data(symbol,interval)
107/7: ms_to_datetime(time.time())
107/8:
ms_to_datetime(time.time()*1000
)
107/9: from tensorflow.keras.models import Sequential
107/10: import tensorflow as tf
107/11: tf.__version__
107/12: [df,features, targets,signals]=read_dfts_pickle(symbol,interval,mode,param)
107/13: [df,features, targets,signals]=read_dfts_pickle(symbol,interval,mode,param)
107/14: from util_funcs import *
107/15:
symbol = 'BTCUSDT'
interval = '15m'
mode = 'atr_multiplier'
param = 1
107/16: [df,features, targets,signals]=read_dfts_pickle(symbol,interval,mode,param)
107/17: df
107/18:
df.head(20
)
107/19:
df.head(30
)
110/1:
import random
from collections import deque

import numpy as np
import tensorflow as tf
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard
from tensorflow.keras.layers import LSTM, BatchNormalization, Dense, Dropout
from tensorflow.keras.models import Sequential

from util_funcs import *

# Paramaters
symbol = 'BTCUSDT'
interval = '15m'
mode = 'atr_multiplier'
param = 1

TESTING_PCT = 0.3
SEQ_LEN = 60
EPOCHS = 10  # how many passes through our data
BATCH_SIZE = 64  # how many batches? Try smaller batch if you're getting OOM (out of memory) errors.

NAME = f"{symbol+interval}_{SEQ_LEN}_SEQ_{int(time.time())}"

scaler = MinMaxScaler(feature_range=(0,1))

def fix_data(n):
    assert(n>0)
    df = read_pickle(f'{symbol}{interval}')
    is_good_data(df,interval)
    write_pickle(df[:-n],f'{symbol}{interval}')

def build_dfts():
    df = load_data(symbol, interval)
    features = build_features(df)
    targets = build_targets(df,mode,param)
    signals = build_signals(targets)
    write_dfts_pickle(symbol, interval,mode,param, [df,features,targets,signals])

def preprocess_features(data_frame):
    df = data_frame.copy()

    df.drop([x for x in BAD_IND if x in df.columns.tolist()],axis = 1,inplace = True)

    df.dropna(axis=1,how='all',inplace=True)
    df.dropna(axis =0, how='any',inplace = True)
    df.fillna(method='ffill',inplace = True)

    assert np.all(np.isfinite(df)) # Get inf columns: np.isfinite(df).all()[np.isfinite(df).all() ==False]
    assert not np.any(np.isnan(df)) # np.isnan(df).any()[np.isnan(df).any()]

    df = pd.DataFrame(scaler.fit_transform(df.values), columns=df.columns)
    df.signals = df.signals.astype('int32')

    sequential_data = []  # this is a list that will CONTAIN the sequences
    prev_days = deque(maxlen=SEQ_LEN)  # These will be our actual sequences. They are made with deque, which keeps the maximum length by popping out older values as new ones come in

    for i in tqdm(df.values):  # iterate over the values
        # print(i[-1])
        prev_days.append([n for n in i[:-1]])  # store all but the target
        if len(prev_days) == SEQ_LEN:  # make sure we have 60 sequences!
            sequential_data.append([np.array(prev_days),i[-1]])  # append those bad boys!

    print('done')
    random.shuffle(sequential_data)  # shuffle for good measure.

    buys = []  # list that will store our buy sequences and targets
    sells = []  # list that will store our sell sequences and targets

    for seq, target in tqdm(sequential_data):  # iterate over the sequential data
        if target == 0:  # if it's a "not buy"
            sells.append([seq, target])  # append to sells list
        elif target == 1:  # otherwise if the target is a 1...
            buys.append([seq, target])  # it's a buy!

    random.shuffle(buys)  # shuffle the buys
    random.shuffle(sells)  # shuffle the sells!

    lower = min(len(buys), len(sells))  # what's the shorter length?

    buys = buys[:lower]  # make sure both lists are only up to the shortest length.
    sells = sells[:lower]  # make sure both lists are only up to the shortest length.

    sequential_data = buys+sells  # add them together
    random.shuffle(sequential_data)  # another shuffle, so the model doesn't get confused with all 1 class then the other.

    X = []
    y = []

    for seq, target in tqdm(sequential_data):  # going over our new sequential data
        X.append(seq)  # X is the sequences
        y.append(target)  # y is the targets/labels (buys vs sell/notbuy)

    return np.array(X), y  # return X and y...and make X a numpy array!

def main():
    [df,features, targets,signals]=read_dfts_pickle(symbol,interval,mode,param)
    is_good_data(df,1)
    
    signals.name = 'signals'
    features_and_signals = features.join(signals)

    testing_mark = sorted(features_and_signals.index.values)[-int(0.05*len(features_and_signals))]
    features_signals_train = features_and_signals[(features_and_signals.index <testing_mark)]
    features_signals_test = features_and_signals[(features_and_signals.index >= testing_mark)]

    train_x, train_y = preprocess_features(features_signals_train)
    test_x, test_y = preprocess_features(features_signals_test)

    print(f"train data: {len(train_x)} validation: {len(test_x)}")
    print(f"Dont buys: {train_y.count(0)}, buys: {train_y.count(1)}")
    print(f"VALIDATION Dont buys: {test_y.count(0)}, buys: {test_y.count(1)}")

    model = Sequential()
    model.add(LSTM(128, input_shape=(train_x.shape[1:]), return_sequences=True))
    model.add(Dropout(0.2))
    model.add(BatchNormalization())

    model.add(LSTM(128, return_sequences=True))
    model.add(Dropout(0.1))
    model.add(BatchNormalization())

    model.add(LSTM(128))
    model.add(Dropout(0.2))
    model.add(BatchNormalization())

    model.add(Dense(32, activation='relu'))
    model.add(Dropout(0.2))

    model.add(Dense(2, activation='softmax'))

    opt = tf.keras.optimizers.Adam(lr=0.001, decay=1e-6)

    # Compile model
    model.compile(
        loss='sparse_categorical_crossentropy',
        optimizer=opt,
        metrics=['accuracy']
    )
    
    tensorboard = TensorBoard(log_dir="logs/{}".format(NAME))

    filepath = "RNN_Final-{epoch:02d}-{val_acc:.3f}"  # unique file name that will include the epoch and the validation acc for that epoch
    checkpoint = ModelCheckpoint("models/{}.model".format(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')) # saves only the best ones

    # Train model
    history = model.fit(
        train_x, train_y,
        batch_size=BATCH_SIZE,
        epochs=EPOCHS,
        validation_data=(test_x, test_y),
        callbacks=[tensorboard, checkpoint],
    )

    # Score model
    score = model.evaluate(test_x, test_y, verbose=0)
    print('Test loss:', score[0])
    print('Test accuracy:', score[1])
    # Save model
    model.save(f"{DATA_DIR}\\models\\{NAME}")

main()
# fix_data(50)
# df = load_data(symbol,interval)
110/2:
breakpoint()
import random
from collections import deque

import numpy as np
import tensorflow as tf
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard
from tensorflow.keras.layers import LSTM, BatchNormalization, Dense, Dropout
from tensorflow.keras.models import Sequential

from util_funcs import *

# Paramaters
symbol = 'BTCUSDT'
interval = '15m'
mode = 'atr_multiplier'
param = 1

TESTING_PCT = 0.3
SEQ_LEN = 60
EPOCHS = 10  # how many passes through our data
BATCH_SIZE = 64  # how many batches? Try smaller batch if you're getting OOM (out of memory) errors.

NAME = f"{symbol+interval}_{SEQ_LEN}_SEQ_{int(time.time())}"

scaler = MinMaxScaler(feature_range=(0,1))

def fix_data(n):
    assert(n>0)
    df = read_pickle(f'{symbol}{interval}')
    is_good_data(df,interval)
    write_pickle(df[:-n],f'{symbol}{interval}')

def build_dfts():
    df = load_data(symbol, interval)
    features = build_features(df)
    targets = build_targets(df,mode,param)
    signals = build_signals(targets)
    write_dfts_pickle(symbol, interval,mode,param, [df,features,targets,signals])

def preprocess_features(data_frame):
    df = data_frame.copy()

    df.drop([x for x in BAD_IND if x in df.columns.tolist()],axis = 1,inplace = True)

    df.dropna(axis=1,how='all',inplace=True)
    df.dropna(axis =0, how='any',inplace = True)
    df.fillna(method='ffill',inplace = True)

    assert np.all(np.isfinite(df)) # Get inf columns: np.isfinite(df).all()[np.isfinite(df).all() ==False]
    assert not np.any(np.isnan(df)) # np.isnan(df).any()[np.isnan(df).any()]

    df = pd.DataFrame(scaler.fit_transform(df.values), columns=df.columns)
    df.signals = df.signals.astype('int32')

    sequential_data = []  # this is a list that will CONTAIN the sequences
    prev_days = deque(maxlen=SEQ_LEN)  # These will be our actual sequences. They are made with deque, which keeps the maximum length by popping out older values as new ones come in

    for i in tqdm(df.values):  # iterate over the values
        # print(i[-1])
        prev_days.append([n for n in i[:-1]])  # store all but the target
        if len(prev_days) == SEQ_LEN:  # make sure we have 60 sequences!
            sequential_data.append([np.array(prev_days),i[-1]])  # append those bad boys!

    print('done')
    random.shuffle(sequential_data)  # shuffle for good measure.

    buys = []  # list that will store our buy sequences and targets
    sells = []  # list that will store our sell sequences and targets

    for seq, target in tqdm(sequential_data):  # iterate over the sequential data
        if target == 0:  # if it's a "not buy"
            sells.append([seq, target])  # append to sells list
        elif target == 1:  # otherwise if the target is a 1...
            buys.append([seq, target])  # it's a buy!

    random.shuffle(buys)  # shuffle the buys
    random.shuffle(sells)  # shuffle the sells!

    lower = min(len(buys), len(sells))  # what's the shorter length?

    buys = buys[:lower]  # make sure both lists are only up to the shortest length.
    sells = sells[:lower]  # make sure both lists are only up to the shortest length.

    sequential_data = buys+sells  # add them together
    random.shuffle(sequential_data)  # another shuffle, so the model doesn't get confused with all 1 class then the other.

    X = []
    y = []

    for seq, target in tqdm(sequential_data):  # going over our new sequential data
        X.append(seq)  # X is the sequences
        y.append(target)  # y is the targets/labels (buys vs sell/notbuy)

    return np.array(X), y  # return X and y...and make X a numpy array!

def main():
    [df,features, targets,signals]=read_dfts_pickle(symbol,interval,mode,param)
    is_good_data(df,1)
    
    signals.name = 'signals'
    features_and_signals = features.join(signals)

    testing_mark = sorted(features_and_signals.index.values)[-int(0.05*len(features_and_signals))]
    features_signals_train = features_and_signals[(features_and_signals.index <testing_mark)]
    features_signals_test = features_and_signals[(features_and_signals.index >= testing_mark)]

    train_x, train_y = preprocess_features(features_signals_train)
    test_x, test_y = preprocess_features(features_signals_test)

    print(f"train data: {len(train_x)} validation: {len(test_x)}")
    print(f"Dont buys: {train_y.count(0)}, buys: {train_y.count(1)}")
    print(f"VALIDATION Dont buys: {test_y.count(0)}, buys: {test_y.count(1)}")

    model = Sequential()
    model.add(LSTM(128, input_shape=(train_x.shape[1:]), return_sequences=True))
    model.add(Dropout(0.2))
    model.add(BatchNormalization())

    model.add(LSTM(128, return_sequences=True))
    model.add(Dropout(0.1))
    model.add(BatchNormalization())

    model.add(LSTM(128))
    model.add(Dropout(0.2))
    model.add(BatchNormalization())

    model.add(Dense(32, activation='relu'))
    model.add(Dropout(0.2))

    model.add(Dense(2, activation='softmax'))

    opt = tf.keras.optimizers.Adam(lr=0.001, decay=1e-6)

    # Compile model
    model.compile(
        loss='sparse_categorical_crossentropy',
        optimizer=opt,
        metrics=['accuracy']
    )
    
    tensorboard = TensorBoard(log_dir="logs/{}".format(NAME))

    filepath = "RNN_Final-{epoch:02d}-{val_acc:.3f}"  # unique file name that will include the epoch and the validation acc for that epoch
    checkpoint = ModelCheckpoint("models/{}.model".format(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')) # saves only the best ones

    # Train model
    history = model.fit(
        train_x, train_y,
        batch_size=BATCH_SIZE,
        epochs=EPOCHS,
        validation_data=(test_x, test_y),
        callbacks=[tensorboard, checkpoint],
    )

    # Score model
    score = model.evaluate(test_x, test_y, verbose=0)
    print('Test loss:', score[0])
    print('Test accuracy:', score[1])
    # Save model
    model.save(f"{DATA_DIR}\\models\\{NAME}")

main()
# fix_data(50)
# df = load_data(symbol,interval)
114/1: l = [[[3,6,3],'a'],[[3,6,3,65],'b'],[[375,5,2,5],'c']]
114/2: l
114/3: a,b = l
114/4: a,b = zip(l)
114/5: a,b = zip(*l)
116/1: import pandas as pd
116/2:
df = pd.DataFrame([[6,1],[3,1],[5,0],[5,1],[4,1],[7,0]],columns=['A','B'])
s = df.groupby('B').count()
116/3: s
116/4: type(s)
116/5: d.1
116/6: d[['1']]
116/7: s[['1']]
116/8: s[['1']]
116/9: s
116/10: s.iloc[0]
116/11: s.iloc[0].values
116/12: s.iloc[0]
116/13: s.iloc[0,0]
116/14: s.iloc[1,0]
116/15: a,b = s.iloc[,0]
116/16: a,b = s.iloc[:,0]
116/17: a,b
116/18: df
116/19: min(s.iloc[:,0])
116/20: min(*s.iloc[:,0])
116/21: df[0]
116/22: df.loc[0]
116/23: df.loc[1]
116/24: df.iloc[1]
116/25: df.iloc[0]
116/26: df.iloc[0]
116/27: df.iloc[0]
116/28: df.iloc[0]
116/29: df.iloc[1]
116/30: df.iloc[:1]
116/31: df.iloc[1:]
116/32: df
116/33: df.iloc[:1]
116/34: df.iloc[:0]
116/35: df.iloc[0]
116/36: df.iloc[1]
116/37: df.loc[1]
116/38: df.loc[:0]
116/39: df.iloc[:0]
116/40: df.iloc[,:0]
116/41: df[['B']<1]
116/42: df['B'<1]
116/43: df[df.B<1]
116/44:

df.B<1
116/45: df.loc[0,-1]
116/46: df.loc[0][-1]
116/47: df.loc[0]
116/48: df.loc[0:-1]
116/49: df.loc[0:]
116/50: df.loc[0]
116/51: df.loc[0][0]
116/52: df.loc[0][-1]
116/53: df.iloc[0][-1]
116/54: df.iloc[0:-1]
116/55: df.loc[0:-1]
116/56: df.iloc[0:-1]
116/57: df.iloc[0:]
116/58: df.iloc[0]
116/59: df.iloc[0,]
116/60: df.iloc[0,-1]
116/61: df.iloc[0]
116/62: df.iloc[0][-1]
116/63: df.iloc[0,]
116/64: df.iloc[0,-1]
116/65: df.loc[0,-1]
116/66: df.loc[0,]
116/67: s
116/68: a,b = s
116/69: s.iloc[:,0]
118/1:
import random
from collections import deque

import numpy as np
import tensorflow as tf
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard
from tensorflow.keras.layers import LSTM, BatchNormalization, Dense, Dropout
from tensorflow.keras.models import Sequential

from util_funcs import *

# Paramaters
symbol = 'BTCUSDT'
interval = '15m'
mode = 'atr_multiplier'
param = 1

TESTING_PCT = 0.3
SEQ_LEN = 60
EPOCHS = 10  # how many passes through our data
BATCH_SIZE = 64  # how many batches? Try smaller batch if you're getting OOM (out of memory) errors.

NAME = f"{symbol+interval}_{SEQ_LEN}_SEQ_{int(time.time())}"

scaler = MinMaxScaler(feature_range=(0,1))

def fix_data(n):
    assert(n>0)
    df = read_pickle(f'{symbol}{interval}')
    is_good_data(df,interval)
    write_pickle(df[:-n],f'{symbol}{interval}')

def build_dfts():
    df = load_data(symbol, interval)
    features = build_features(df)
    targets = build_targets(df,mode,param)
    signals = build_signals(targets)
    write_dfts_pickle(symbol, interval,mode,param, [df,features,targets,signals])

def preprocess_features(data_frame):
    df = data_frame.copy()

    df.drop([x for x in BAD_IND if x in df.columns.tolist()],axis = 1,inplace = True)

    df.dropna(axis=1,how='all',inplace=True)
    df.dropna(axis =0, how='any',inplace = True)
    df.fillna(method='ffill',inplace = True)

    assert np.all(np.isfinite(df)) # Get inf columns: np.isfinite(df).all()[np.isfinite(df).all() ==False]
    assert not np.any(np.isnan(df)) # np.isnan(df).any()[np.isnan(df).any()]

    df = pd.DataFrame(scaler.fit_transform(df.values), columns=df.columns)
    df.signals = df.signals.astype('int32')

    sequential_data = []  # this is a list that will CONTAIN the sequences
    counter = {0:0,1:0}
    groupby = df.loc[SEQ_LEN:].groupby(['signals']).count().iloc[:,0]
    max_count = min(*groupby)
    # df_new = df.copy().loc[SEQ_LEN:]

    print("Preprocessing Data...")
    for i in tqdm(range(SEQ_LEN,len(df))):
       if counter[df.iloc[i,-1]] < max_count:
           counter[df.iloc[i,-1]] += 1
           sequential_data.append([df.iloc[(i-SEQ_LEN):i,:-1].values , df.iloc[i,-1]])

    random.shuffle(sequential_data)
    X , y = sequential_data.iloc[:,0]
    
    return np.array(X), y  # return X and y...and make X a numpy array!

def main():
    [df,features, targets,signals]=read_dfts_pickle(symbol,interval,mode,param)
    is_good_data(df,1)
    
    signals.name = 'signals'
    features_and_signals = features.join(signals)

    testing_mark = sorted(features_and_signals.index.values)[-int(0.05*len(features_and_signals))]
    features_signals_train = features_and_signals[(features_and_signals.index <testing_mark)]
    features_signals_test = features_and_signals[(features_and_signals.index >= testing_mark)]

    train_x, train_y = preprocess_features(features_signals_train)
    test_x, test_y = preprocess_features(features_signals_test)

    print(f"train data: {len(train_x)} validation: {len(test_x)}")
    print(f"Dont buys: {train_y.count(0)}, buys: {train_y.count(1)}")
    print(f"VALIDATION Dont buys: {test_y.count(0)}, buys: {test_y.count(1)}")

    model = Sequential()
    model.add(LSTM(128, input_shape=(train_x.shape[1:]), return_sequences=True))
    model.add(Dropout(0.2))
    model.add(BatchNormalization())

    model.add(LSTM(128, return_sequences=True))
    model.add(Dropout(0.1))
    model.add(BatchNormalization())

    model.add(LSTM(128))
    model.add(Dropout(0.2))
    model.add(BatchNormalization())

    model.add(Dense(32, activation='relu'))
    model.add(Dropout(0.2))

    model.add(Dense(2, activation='softmax'))

    opt = tf.keras.optimizers.Adam(lr=0.001, decay=1e-6)

    # Compile model
    model.compile(
        loss='sparse_categorical_crossentropy',
        optimizer=opt,
        metrics=['accuracy']
    )
    
    tensorboard = TensorBoard(log_dir="logs/{}".format(NAME))

    filepath = "RNN_Final-{epoch:02d}-{val_acc:.3f}"  # unique file name that will include the epoch and the validation acc for that epoch
    checkpoint = ModelCheckpoint("models/{}.model".format(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')) # saves only the best ones

    # Train model
    history = model.fit(
        train_x, train_y,
        batch_size=BATCH_SIZE,
        epochs=EPOCHS,
        validation_data=(test_x, test_y),
        callbacks=[tensorboard, checkpoint],
    )

    # Score model
    score = model.evaluate(test_x, test_y, verbose=0)
    print('Test loss:', score[0])
    print('Test accuracy:', score[1])
    # Save model
    model.save(f"{DATA_DIR}\\models\\{NAME}")

main()
# fix_data(50)
# df = load_data(symbol,interval)
118/2: sequential_data
118/3: sequential_data
118/4: df
118/5: df
118/6: d= [[24,4],[2,6],[5,3]]
118/7:
d= [[24,4],[2,6],[5,3]]
d
118/8:
d= [[24,4],[2,6],[5,3]]
a,b = d
118/9:
d= [[24,4],[2,6],[5,3]]
a,b = zip(d)
118/10:
d= [[24,4],[2,6],[5,3]]
a,b = zip(*d)
118/11:
d= [[24,4],[2,6],[5,3]]
a,b = zip(*d)
a,b
118/12:
d= [[24,4],[2,6],[5,3]]
a,b = zip(d)
a,b
118/13:
d= [[24,4],[2,6],[5,3]]
a,b = zip(*d)
a,b
118/14:
import random
from collections import deque

import numpy as np
import tensorflow as tf
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard
from tensorflow.keras.layers import LSTM, BatchNormalization, Dense, Dropout
from tensorflow.keras.models import Sequential

from util_funcs import *

# Paramaters
symbol = 'BTCUSDT'
interval = '15m'
mode = 'atr_multiplier'
param = 1

TESTING_PCT = 0.3
SEQ_LEN = 60
EPOCHS = 10  # how many passes through our data
BATCH_SIZE = 64  # how many batches? Try smaller batch if you're getting OOM (out of memory) errors.

NAME = f"{symbol+interval}_{SEQ_LEN}_SEQ_{int(time.time())}"

scaler = MinMaxScaler(feature_range=(0,1))

def fix_data(n):
    assert(n>0)
    df = read_pickle(f'{symbol}{interval}')
    is_good_data(df,interval)
    write_pickle(df[:-n],f'{symbol}{interval}')

def build_dfts():
    df = load_data(symbol, interval)
    features = build_features(df)
    targets = build_targets(df,mode,param)
    signals = build_signals(targets)
    write_dfts_pickle(symbol, interval,mode,param, [df,features,targets,signals])

def preprocess_features(data_frame):
    df = data_frame.copy()

    df.drop([x for x in BAD_IND if x in df.columns.tolist()],axis = 1,inplace = True)

    df.dropna(axis=1,how='all',inplace=True)
    df.dropna(axis =0, how='any',inplace = True)
    df.fillna(method='ffill',inplace = True)

    assert np.all(np.isfinite(df)) # Get inf columns: np.isfinite(df).all()[np.isfinite(df).all() ==False]
    assert not np.any(np.isnan(df)) # np.isnan(df).any()[np.isnan(df).any()]

    df = pd.DataFrame(scaler.fit_transform(df.values), columns=df.columns)
    df.signals = df.signals.astype('int32')

    sequential_data = []  # this is a list that will CONTAIN the sequences
    counter = {0:0,1:0}
    groupby = df.loc[SEQ_LEN:].groupby(['signals']).count().iloc[:,0]
    max_count = min(*groupby)
    # df_new = df.copy().loc[SEQ_LEN:]

    print("Preprocessing Data...")
    for i in tqdm(range(SEQ_LEN,len(df))):
       if counter[df.iloc[i,-1]] < max_count:
           counter[df.iloc[i,-1]] += 1
           sequential_data.append([df.iloc[(i-SEQ_LEN):i,:-1].values , df.iloc[i,-1]])

    random.shuffle(sequential_data)
    X , y = sequential_data.iloc[:,0]
    
    return np.array(X), y  # return X and y...and make X a numpy array!
118/15:
[df,features, targets,signals]=read_dfts_pickle(symbol,interval,mode,param)
is_good_data(df,1)

signals.name = 'signals'
features_and_signals = features.join(signals)

testing_mark = sorted(features_and_signals.index.values)[-int(0.05*len(features_and_signals))]
features_signals_train = features_and_signals[(features_and_signals.index <testing_mark)]
features_signals_test = features_and_signals[(features_and_signals.index >= testing_mark)]

train_x, train_y = preprocess_features(features_signals_train)
test_x, test_y = preprocess_features(features_signals_test)

print(f"train data: {len(train_x)} validation: {len(test_x)}")
print(f"Dont buys: {train_y.count(0)}, buys: {train_y.count(1)}")
print(f"VALIDATION Dont buys: {test_y.count(0)}, buys: {test_y.count(1)}")

model = Sequential()
model.add(LSTM(128, input_shape=(train_x.shape[1:]), return_sequences=True))
model.add(Dropout(0.2))
model.add(BatchNormalization())

model.add(LSTM(128, return_sequences=True))
model.add(Dropout(0.1))
model.add(BatchNormalization())

model.add(LSTM(128))
model.add(Dropout(0.2))
model.add(BatchNormalization())

model.add(Dense(32, activation='relu'))
model.add(Dropout(0.2))

model.add(Dense(2, activation='softmax'))

opt = tf.keras.optimizers.Adam(lr=0.001, decay=1e-6)

# Compile model
model.compile(
    loss='sparse_categorical_crossentropy',
    optimizer=opt,
    metrics=['accuracy']
)

tensorboard = TensorBoard(log_dir="logs/{}".format(NAME))

filepath = "RNN_Final-{epoch:02d}-{val_acc:.3f}"  # unique file name that will include the epoch and the validation acc for that epoch
checkpoint = ModelCheckpoint("models/{}.model".format(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')) # saves only the best ones

# Train model
history = model.fit(
    train_x, train_y,
    batch_size=BATCH_SIZE,
    epochs=EPOCHS,
    validation_data=(test_x, test_y),
    callbacks=[tensorboard, checkpoint],
)

# Score model
score = model.evaluate(test_x, test_y, verbose=0)
print('Test loss:', score[0])
print('Test accuracy:', score[1])
# Save model
model.save(f"{DATA_DIR}\\models\\{NAME}")
118/16:
[df,features, targets,signals]=read_dfts_pickle(symbol,interval,mode,param)
is_good_data(df,1)

signals.name = 'signals'
features_and_signals = features.join(signals)

testing_mark = sorted(features_and_signals.index.values)[-int(0.05*len(features_and_signals))]
features_signals_train = features_and_signals[(features_and_signals.index <testing_mark)]
features_signals_test = features_and_signals[(features_and_signals.index >= testing_mark)]

train_x, train_y = preprocess_features(features_signals_train)
test_x, test_y = preprocess_features(features_signals_test)

print(f"train data: {len(train_x)} validation: {len(test_x)}")
print(f"Dont buys: {train_y.count(0)}, buys: {train_y.count(1)}")
print(f"VALIDATION Dont buys: {test_y.count(0)}, buys: {test_y.count(1)}")
118/17:
import random
from collections import deque

import numpy as np
import tensorflow as tf
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard
from tensorflow.keras.layers import LSTM, BatchNormalization, Dense, Dropout
from tensorflow.keras.models import Sequential

from util_funcs import *

# Paramaters
symbol = 'BTCUSDT'
interval = '15m'
mode = 'atr_multiplier'
param = 1

TESTING_PCT = 0.3
SEQ_LEN = 60
EPOCHS = 10  # how many passes through our data
BATCH_SIZE = 64  # how many batches? Try smaller batch if you're getting OOM (out of memory) errors.

NAME = f"{symbol+interval}_{SEQ_LEN}_SEQ_{int(time.time())}"

scaler = MinMaxScaler(feature_range=(0,1))

def fix_data(n):
    assert(n>0)
    df = read_pickle(f'{symbol}{interval}')
    is_good_data(df,interval)
    write_pickle(df[:-n],f'{symbol}{interval}')

def build_dfts():
    df = load_data(symbol, interval)
    features = build_features(df)
    targets = build_targets(df,mode,param)
    signals = build_signals(targets)
    write_dfts_pickle(symbol, interval,mode,param, [df,features,targets,signals])

def preprocess_features(data_frame):
    df = data_frame.copy()

    df.drop([x for x in BAD_IND if x in df.columns.tolist()],axis = 1,inplace = True)

    df.dropna(axis=1,how='all',inplace=True)
    df.dropna(axis =0, how='any',inplace = True)
    df.fillna(method='ffill',inplace = True)

    assert np.all(np.isfinite(df)) # Get inf columns: np.isfinite(df).all()[np.isfinite(df).all() ==False]
    assert not np.any(np.isnan(df)) # np.isnan(df).any()[np.isnan(df).any()]

    df = pd.DataFrame(scaler.fit_transform(df.values), columns=df.columns)
    df.signals = df.signals.astype('int32')

    sequential_data = []  # this is a list that will CONTAIN the sequences
    counter = {0:0,1:0}
    groupby = df.loc[SEQ_LEN:].groupby(['signals']).count().iloc[:,0]
    max_count = min(*groupby)
    # df_new = df.copy().loc[SEQ_LEN:]

    print("Preprocessing Data...")
    for i in tqdm(range(SEQ_LEN,len(df))):
       if counter[df.iloc[i,-1]] < max_count:
           counter[df.iloc[i,-1]] += 1
           sequential_data.append([df.iloc[(i-SEQ_LEN):i,:-1].values , df.iloc[i,-1]])

    random.shuffle(sequential_data)
    X , y = zip(*sequential_data)
    
    return np.array(X), y  # return X and y...and make X a numpy array!
118/18:
import random
from collections import deque

import numpy as np
import tensorflow as tf
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard
from tensorflow.keras.layers import LSTM, BatchNormalization, Dense, Dropout
from tensorflow.keras.models import Sequential

from util_funcs import *

# Paramaters
symbol = 'BTCUSDT'
interval = '15m'
mode = 'atr_multiplier'
param = 1

TESTING_PCT = 0.3
SEQ_LEN = 60
EPOCHS = 10  # how many passes through our data
BATCH_SIZE = 64  # how many batches? Try smaller batch if you're getting OOM (out of memory) errors.

NAME = f"{symbol+interval}_{SEQ_LEN}_SEQ_{int(time.time())}"

scaler = MinMaxScaler(feature_range=(0,1))

def fix_data(n):
    assert(n>0)
    df = read_pickle(f'{symbol}{interval}')
    is_good_data(df,interval)
    write_pickle(df[:-n],f'{symbol}{interval}')

def build_dfts():
    df = load_data(symbol, interval)
    features = build_features(df)
    targets = build_targets(df,mode,param)
    signals = build_signals(targets)
    write_dfts_pickle(symbol, interval,mode,param, [df,features,targets,signals])

def preprocess_features(data_frame):
    df = data_frame.copy()

    df.drop([x for x in BAD_IND if x in df.columns.tolist()],axis = 1,inplace = True)

    df.dropna(axis=1,how='all',inplace=True)
    df.dropna(axis =0, how='any',inplace = True)
    df.fillna(method='ffill',inplace = True)

    assert np.all(np.isfinite(df)) # Get inf columns: np.isfinite(df).all()[np.isfinite(df).all() ==False]
    assert not np.any(np.isnan(df)) # np.isnan(df).any()[np.isnan(df).any()]

    df = pd.DataFrame(scaler.fit_transform(df.values), columns=df.columns)
    df.signals = df.signals.astype('int32')

    sequential_data = []  # this is a list that will CONTAIN the sequences
    counter = {0:0,1:0}
    groupby = df.loc[SEQ_LEN:].groupby(['signals']).count().iloc[:,0]
    max_count = min(*groupby)
    # df_new = df.copy().loc[SEQ_LEN:]

    print("Preprocessing Data...")
    for i in tqdm(range(SEQ_LEN,len(df))):
       if counter[df.iloc[i,-1]] < max_count:
           counter[df.iloc[i,-1]] += 1
           sequential_data.append([df.iloc[(i-SEQ_LEN):i,:-1].values , df.iloc[i,-1]])

    random.shuffle(sequential_data)
    X , y = zip(*sequential_data)
    
    return np.array(X), y  # return X and y...and make X a numpy array!
118/19:
import random
from collections import deque

import numpy as np
import tensorflow as tf
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard
from tensorflow.keras.layers import LSTM, BatchNormalization, Dense, Dropout
from tensorflow.keras.models import Sequential

from util_funcs import *

# Paramaters
symbol = 'BTCUSDT'
interval = '15m'
mode = 'atr_multiplier'
param = 1

TESTING_PCT = 0.3
SEQ_LEN = 60
EPOCHS = 10  # how many passes through our data
BATCH_SIZE = 64  # how many batches? Try smaller batch if you're getting OOM (out of memory) errors.

NAME = f"{symbol+interval}_{SEQ_LEN}_SEQ_{int(time.time())}"

scaler = MinMaxScaler(feature_range=(0,1))

def fix_data(n):
    assert(n>0)
    df = read_pickle(f'{symbol}{interval}')
    is_good_data(df,interval)
    write_pickle(df[:-n],f'{symbol}{interval}')

def build_dfts():
    df = load_data(symbol, interval)
    features = build_features(df)
    targets = build_targets(df,mode,param)
    signals = build_signals(targets)
    write_dfts_pickle(symbol, interval,mode,param, [df,features,targets,signals])

def preprocess_features(data_frame):
    df = data_frame.copy()

    df.drop([x for x in BAD_IND if x in df.columns.tolist()],axis = 1,inplace = True)

    df.dropna(axis=1,how='all',inplace=True)
    df.dropna(axis =0, how='any',inplace = True)
    df.fillna(method='ffill',inplace = True)

    assert np.all(np.isfinite(df)) # Get inf columns: np.isfinite(df).all()[np.isfinite(df).all() ==False]
    assert not np.any(np.isnan(df)) # np.isnan(df).any()[np.isnan(df).any()]

    df = pd.DataFrame(scaler.fit_transform(df.values), columns=df.columns)
    df.signals = df.signals.astype('int32')

    sequential_data = []  # this is a list that will CONTAIN the sequences
    counter = {0:0,1:0}
    groupby = df.loc[SEQ_LEN:].groupby(['signals']).count().iloc[:,0]
    max_count = min(*groupby)
    # df_new = df.copy().loc[SEQ_LEN:]

    print("Preprocessing Data...")
    for i in tqdm(range(SEQ_LEN,len(df))):
       if counter[df.iloc[i,-1]] < max_count:
           counter[df.iloc[i,-1]] += 1
           sequential_data.append([df.iloc[(i-SEQ_LEN):i,:-1].values , df.iloc[i,-1]])

    random.shuffle(sequential_data)
    X , y = zip(*sequential_data)
    
    return np.array(X), y  # return X and y...and make X a numpy array!
118/20:
[df,features, targets,signals]=read_dfts_pickle(symbol,interval,mode,param)
is_good_data(df,1)

signals.name = 'signals'
features_and_signals = features.join(signals)

testing_mark = sorted(features_and_signals.index.values)[-int(0.05*len(features_and_signals))]
features_signals_train = features_and_signals[(features_and_signals.index <testing_mark)]
features_signals_test = features_and_signals[(features_and_signals.index >= testing_mark)]

train_x, train_y = preprocess_features(features_signals_train)
test_x, test_y = preprocess_features(features_signals_test)

print(f"train data: {len(train_x)} validation: {len(test_x)}")
print(f"Dont buys: {train_y.count(0)}, buys: {train_y.count(1)}")
print(f"VALIDATION Dont buys: {test_y.count(0)}, buys: {test_y.count(1)}")
119/1:
import random
from collections import deque

import numpy as np
import tensorflow as tf
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard
from tensorflow.keras.layers import LSTM, BatchNormalization, Dense, Dropout
from tensorflow.keras.models import Sequential

from util_funcs import *

# Paramaters
symbol = 'BTCUSDT'
interval = '15m'
mode = 'atr_multiplier'
param = 1

TESTING_PCT = 0.3
SEQ_LEN = 60
EPOCHS = 10  # how many passes through our data
BATCH_SIZE = 64  # how many batches? Try smaller batch if you're getting OOM (out of memory) errors.

NAME = f"{symbol+interval}_{SEQ_LEN}_SEQ_{int(time.time())}"

scaler = MinMaxScaler(feature_range=(0,1))

def fix_data(n):
    assert(n>0)
    df = read_pickle(f'{symbol}{interval}')
    is_good_data(df,interval)
    write_pickle(df[:-n],f'{symbol}{interval}')

def build_dfts():
    df = load_data(symbol, interval)
    features = build_features(df)
    targets = build_targets(df,mode,param)
    signals = build_signals(targets)
    write_dfts_pickle(symbol, interval,mode,param, [df,features,targets,signals])

def preprocess_features(data_frame):
    df = data_frame.copy()

    df.drop([x for x in BAD_IND if x in df.columns.tolist()],axis = 1,inplace = True)

    df.dropna(axis=1,how='all',inplace=True)
    df.dropna(axis =0, how='any',inplace = True)
    df.fillna(method='ffill',inplace = True)

    assert np.all(np.isfinite(df)) # Get inf columns: np.isfinite(df).all()[np.isfinite(df).all() ==False]
    assert not np.any(np.isnan(df)) # np.isnan(df).any()[np.isnan(df).any()]

    df = pd.DataFrame(scaler.fit_transform(df.values), columns=df.columns)
    df.signals = df.signals.astype('int32')

    sequential_data = []  # this is a list that will CONTAIN the sequences
    counter = {0:0,1:0}
    groupby = df.loc[SEQ_LEN:].groupby(['signals']).count().iloc[:,0]
    max_count = min(*groupby)
    # df_new = df.copy().loc[SEQ_LEN:]

    print("Preprocessing Data...")
    for i in tqdm(range(SEQ_LEN,len(df))):
       if counter[df.iloc[i,-1]] < max_count:
           counter[df.iloc[i,-1]] += 1
           sequential_data.append([df.iloc[(i-SEQ_LEN):i,:-1].values , df.iloc[i,-1]])

    random.shuffle(sequential_data)
    X , y = zip(*sequential_data)
    
    return np.array(X), y  # return X and y...and make X a numpy array!
119/2:
[df,features, targets,signals]=read_dfts_pickle(symbol,interval,mode,param)
is_good_data(df,1)

signals.name = 'signals'
features_and_signals = features.join(signals)

testing_mark = sorted(features_and_signals.index.values)[-int(0.05*len(features_and_signals))]
features_signals_train = features_and_signals[(features_and_signals.index <testing_mark)]
features_signals_test = features_and_signals[(features_and_signals.index >= testing_mark)]

train_x, train_y = preprocess_features(features_signals_train)
test_x, test_y = preprocess_features(features_signals_test)

print(f"train data: {len(train_x)} validation: {len(test_x)}")
print(f"Dont buys: {train_y.count(0)}, buys: {train_y.count(1)}")
print(f"VALIDATION Dont buys: {test_y.count(0)}, buys: {test_y.count(1)}")
119/3:
import random
from collections import deque

import numpy as np
import tensorflow as tf
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard
from tensorflow.keras.layers import LSTM, BatchNormalization, Dense, Dropout
from tensorflow.keras.models import Sequential

from util_funcs import *

# Paramaters
symbol = 'BTCUSDT'
interval = '15m'
mode = 'atr_multiplier'
param = 1

TESTING_PCT = 0.3
SEQ_LEN = 60
EPOCHS = 10  # how many passes through our data
BATCH_SIZE = 64  # how many batches? Try smaller batch if you're getting OOM (out of memory) errors.

NAME = f"{symbol+interval}_{SEQ_LEN}_SEQ_{int(time.time())}"

scaler = MinMaxScaler(feature_range=(0,1))

def fix_data(n):
    assert(n>0)
    df = read_pickle(f'{symbol}{interval}')
    is_good_data(df,interval)
    write_pickle(df[:-n],f'{symbol}{interval}')

def build_dfts():
    df = load_data(symbol, interval)
    features = build_features(df)
    targets = build_targets(df,mode,param)
    signals = build_signals(targets)
    write_dfts_pickle(symbol, interval,mode,param, [df,features,targets,signals])

def preprocess_features(data_frame):
    print("Preprocessing Data...")
    
    df = data_frame.copy()

    df.drop([x for x in BAD_IND if x in df.columns.tolist()],axis = 1,inplace = True)

    df.dropna(axis=1,how='all',inplace=True)
    df.dropna(axis =0, how='any',inplace = True)
    df.fillna(method='ffill',inplace = True)

    assert np.all(np.isfinite(df)) # Get inf columns: np.isfinite(df).all()[np.isfinite(df).all() ==False]
    assert not np.any(np.isnan(df)) # np.isnan(df).any()[np.isnan(df).any()]

    df = pd.DataFrame(scaler.fit_transform(df.values), columns=df.columns)
    df.signals = df.signals.astype('int32')

    sequential_data = []  # this is a list that will CONTAIN the sequences
    counter = {0:0,1:0}
    groupby = df.loc[SEQ_LEN:].groupby(['signals']).count().iloc[:,0]
    max_count = min(*groupby)
    # df_new = df.copy().loc[SEQ_LEN:]

    for i in tqdm(range(SEQ_LEN,len(df))):
       if counter[df.iloc[i,-1]] < max_count:
           counter[df.iloc[i,-1]] += 1
           sequential_data.append([df.iloc[(i-SEQ_LEN):i,:-1].values , df.iloc[i,-1]])

    random.shuffle(sequential_data)
    X , y = zip(*sequential_data)
    
    return np.array(X), y  # return X and y...and make X a numpy array!
119/4:
import random
from collections import deque

import numpy as np
import tensorflow as tf
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard
from tensorflow.keras.layers import LSTM, BatchNormalization, Dense, Dropout
from tensorflow.keras.models import Sequential

from util_funcs import *

# Paramaters
symbol = 'BTCUSDT'
interval = '15m'
mode = 'atr_multiplier'
param = 1

TESTING_PCT = 0.3
SEQ_LEN = 60
EPOCHS = 10  # how many passes through our data
BATCH_SIZE = 64  # how many batches? Try smaller batch if you're getting OOM (out of memory) errors.

NAME = f"{symbol+interval}_{SEQ_LEN}_SEQ_{int(time.time())}"

scaler = MinMaxScaler(feature_range=(0,1))

def fix_data(n):
    assert(n>0)
    df = read_pickle(f'{symbol}{interval}')
    is_good_data(df,interval)
    write_pickle(df[:-n],f'{symbol}{interval}')

def build_dfts():
    df = load_data(symbol, interval)
    features = build_features(df)
    targets = build_targets(df,mode,param)
    signals = build_signals(targets)
    write_dfts_pickle(symbol, interval,mode,param, [df,features,targets,signals])

def preprocess_features(data_frame):
    print("Preprocessing Data...")

    df = data_frame.copy()

    df.drop([x for x in BAD_IND if x in df.columns.tolist()],axis = 1,inplace = True)

    df.dropna(axis=1,how='all',inplace=True)
    df.dropna(axis =0, how='any',inplace = True)
    df.fillna(method='ffill',inplace = True)

    assert np.all(np.isfinite(df)) # Get inf columns: np.isfinite(df).all()[np.isfinite(df).all() ==False]
    assert not np.any(np.isnan(df)) # np.isnan(df).any()[np.isnan(df).any()]

    df = pd.DataFrame(scaler.fit_transform(df.values), columns=df.columns)
    df.signals = df.signals.astype('int32')

    sequential_data = []  # this is a list that will CONTAIN the sequences
    counter = {0:0,1:0}
    groupby = df.loc[SEQ_LEN:].groupby(['signals']).count().iloc[:,0]
    max_count = min(*groupby)
    # df_new = df.copy().loc[SEQ_LEN:]

    for i in tqdm(range(SEQ_LEN,len(df))):
       if counter[df.iloc[i,-1]] < max_count:
           counter[df.iloc[i,-1]] += 1
           sequential_data.append([df.iloc[(i-SEQ_LEN):i,:-1].values , df.iloc[i,-1]])

    random.shuffle(sequential_data)
    X , y = zip(*sequential_data)
    
    return np.array(X), list(y)  # return X and y...and make X a numpy array!
119/5: train_y = list(train_y)
119/6: train_y, test_y = list(train_y), list(test_y)
119/7:
# train_y, test_y = list(train_y), list(test_y)
train_x
119/8:
model = Sequential()
model.add(LSTM(128, input_shape=(train_x.shape[1:]), return_sequences=True))
model.add(Dropout(0.2))
model.add(BatchNormalization())

model.add(LSTM(128, return_sequences=True))
model.add(Dropout(0.1))
model.add(BatchNormalization())

model.add(LSTM(128))
model.add(Dropout(0.2))
model.add(BatchNormalization())

model.add(Dense(32, activation='relu'))
model.add(Dropout(0.2))

model.add(Dense(2, activation='softmax'))

opt = tf.keras.optimizers.Adam(lr=0.001, decay=1e-6)

# Compile model
model.compile(
    loss='sparse_categorical_crossentropy',
    optimizer=opt,
    metrics=['accuracy']
)

tensorboard = TensorBoard(log_dir="logs/{}".format(NAME))

filepath = "RNN_Final-{epoch:02d}-{val_acc:.3f}"  # unique file name that will include the epoch and the validation acc for that epoch
checkpoint = ModelCheckpoint("models/{}.model".format(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')) # saves only the best ones

# Train model
history = model.fit(
    train_x, train_y,
    batch_size=BATCH_SIZE,
    epochs=EPOCHS,
    validation_data=(test_x, test_y),
    callbacks=[tensorboard, checkpoint],
)

# Score model
score = model.evaluate(test_x, test_y, verbose=0)
print('Test loss:', score[0])
print('Test accuracy:', score[1])
# Save model
model.save(f"{DATA_DIR}\\models\\{NAME}")
119/9:
model = Sequential()
model.add(LSTM(128, input_shape=(train_x.shape[1:]), return_sequences=True))
model.add(Dropout(0.2))
model.add(BatchNormalization())

model.add(LSTM(128, return_sequences=True))
model.add(Dropout(0.1))
model.add(BatchNormalization())

model.add(LSTM(128))
model.add(Dropout(0.2))
model.add(BatchNormalization())

model.add(Dense(32, activation='relu'))
model.add(Dropout(0.2))

model.add(Dense(2, activation='softmax'))

opt = tf.keras.optimizers.Adam(lr=0.001, decay=1e-6)

# Compile model
model.compile(
    loss='sparse_categorical_crossentropy',
    optimizer=opt,
    metrics=['accuracy']
)

tensorboard = TensorBoard(log_dir="logs/{}".format(NAME))

filepath = "RNN_Final-{epoch:02d}-{val_acc:.3f}"  # unique file name that will include the epoch and the validation acc for that epoch
checkpoint = ModelCheckpoint("models/{}.model".format(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')) # saves only the best ones

# Train model
history = model.fit(
    train_x, train_y,
    batch_size=BATCH_SIZE,
    epochs=EPOCHS,
    validation_data=(test_x, test_y),
    callbacks=[tensorboard, checkpoint],
)

# Score model
score = model.evaluate(test_x, test_y, verbose=0)
print('Test loss:', score[0])
print('Test accuracy:', score[1])
# Save model
model.save(f"{DATA_DIR}\\models\\{NAME}")
119/10: train_y, test_y = np.array(train_y), np.array(test_y)
119/11: train_y, test_y = np.array(train_y), np.array(test_y)
119/12:
model = Sequential()
model.add(LSTM(128, input_shape=(train_x.shape[1:]), return_sequences=True))
model.add(Dropout(0.2))
model.add(BatchNormalization())

model.add(LSTM(128, return_sequences=True))
model.add(Dropout(0.1))
model.add(BatchNormalization())

model.add(LSTM(128))
model.add(Dropout(0.2))
model.add(BatchNormalization())

model.add(Dense(32, activation='relu'))
model.add(Dropout(0.2))

model.add(Dense(2, activation='softmax'))

opt = tf.keras.optimizers.Adam(lr=0.001, decay=1e-6)

# Compile model
model.compile(
    loss='sparse_categorical_crossentropy',
    optimizer=opt,
    metrics=['accuracy']
)

tensorboard = TensorBoard(log_dir="logs/{}".format(NAME))

filepath = "RNN_Final-{epoch:02d}-{val_acc:.3f}"  # unique file name that will include the epoch and the validation acc for that epoch
checkpoint = ModelCheckpoint("models/{}.model".format(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')) # saves only the best ones

# Train model
history = model.fit(
    train_x, train_y,
    batch_size=BATCH_SIZE,
    epochs=EPOCHS,
    validation_data=(test_x, test_y),
    callbacks=[tensorboard, checkpoint],
)

# Score model
score = model.evaluate(test_x, test_y, verbose=0)
print('Test loss:', score[0])
print('Test accuracy:', score[1])
# Save model
model.save(f"{DATA_DIR}\\models\\{NAME}")
119/13:
import random
from collections import deque

import numpy as np
import tensorflow as tf
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard
from tensorflow.keras.layers import LSTM, BatchNormalization, Dense, Dropout
from tensorflow.keras.models import Sequential

from util_funcs import *

# Paramaters
symbol = 'BTCUSDT'
interval = '15m'
mode = 'atr_multiplier'
param = 1

TESTING_PCT = 0.3
SEQ_LEN = 60
EPOCHS = 10  # how many passes through our data
BATCH_SIZE = 64  # how many batches? Try smaller batch if you're getting OOM (out of memory) errors.

NAME = f"{symbol+interval}_{SEQ_LEN}_SEQ_{int(time.time())}"

scaler = MinMaxScaler(feature_range=(0,1))

def fix_data(n):
    assert(n>0)
    df = read_pickle(f'{symbol}{interval}')
    is_good_data(df,interval)
    write_pickle(df[:-n],f'{symbol}{interval}')

def build_dfts():
    df = load_data(symbol, interval)
    features = build_features(df)
    targets = build_targets(df,mode,param)
    signals = build_signals(targets)
    write_dfts_pickle(symbol, interval,mode,param, [df,features,targets,signals])

def preprocess_features(data_frame):
    print("Preprocessing Data...")

    df = data_frame.copy()

    df.drop([x for x in BAD_IND if x in df.columns.tolist()],axis = 1,inplace = True)

    df.dropna(axis=1,how='all',inplace=True)
    df.dropna(axis =0, how='any',inplace = True)
    df.fillna(method='ffill',inplace = True)

    assert np.all(np.isfinite(df)) # Get inf columns: np.isfinite(df).all()[np.isfinite(df).all() ==False]
    assert not np.any(np.isnan(df)) # np.isnan(df).any()[np.isnan(df).any()]

    df = pd.DataFrame(scaler.fit_transform(df.values), columns=df.columns)
    df.signals = df.signals.astype('int32')

    sequential_data = []  # this is a list that will CONTAIN the sequences
    counter = {0:0,1:0}
    groupby = df.loc[SEQ_LEN:].groupby(['signals']).count().iloc[:,0]
    max_count = min(*groupby)
    # df_new = df.copy().loc[SEQ_LEN:]

    for i in tqdm(range(SEQ_LEN,len(df))):
       if counter[df.iloc[i,-1]] < max_count:
           counter[df.iloc[i,-1]] += 1
           sequential_data.append([df.iloc[(i-SEQ_LEN):i,:-1].values , df.iloc[i,-1]])

    random.shuffle(sequential_data)
    X , y = zip(*sequential_data)
    
    return np.array(X), np.array(y)  # return X and y...and make X a numpy array!
119/14:
# train_y, test_y = np.array(train_y), np.array(test_y)
print(tf.__version__)
119/15:
# train_y, test_y = np.array(train_y), np.array(test_y)
import tensorflow as tf
print(tf.__version__)
119/16:
# train_y, test_y = np.array(train_y), np.array(test_y)
import tensorflow as tf
with tf.device('/gpu:0'):
    print(tf.__version__)
119/17:
# train_y, test_y = np.array(train_y), np.array(test_y)
import tensorflow as tf

with tf.device('/gpu:0') as d:
    print(tf.__version__,d)
119/18:
# train_y, test_y = np.array(train_y), np.array(test_y)
import tensorflow as tf

with tf.device('/gpu:0') as d:
    print(tf.__version__,type(d))
119/19:
# train_y, test_y = np.array(train_y), np.array(test_y)
import tensorflow as tf
from tensorflow.python.client import device_lib
print(device_lib.list_local_devices())

with tf.device('/gpu:0'):
    print(tf.__version__)
121/1: import tensorflow as tf from tensorflow.python.client import device_lib print(device_lib.list_local_devices())  with tf.device('/gpu:0') as d:     print(tf.__version__,d)
121/2: import tensorflow as tf from tensorflow.python.client import device_lib; print(device_lib.list_local_devices());  with tf.device('/gpu:0') as d:\     print(tf.__version__,d)
121/3: import tensorflow as tf from tensorflow.python.client import device_lib; print(device_lib.list_local_devices());  with tf.device('/gpu:0') as d:     print(tf.__version__,d)
121/4: import tensorflow as tf from tensorflow.python.client import device_lib; print(device_lib.list_local_devices());  with tf.device('/gpu:0') as d:     print(tf.__version__,d)
121/5:
import tensorflow as tf from tensorflow.python.client import device_lib; print(device_lib.list_local_devices());  with tf.device('/gpu:0') as d:
    print(tf.__version__,d)
121/6:
import tensorflow as tf from tensorflow.python.client import device_lib; print(device_lib.list_local_devices());
with tf.device('/gpu:0') as d:
    print(tf.__version__,d)
121/7:
import tensorflow as tf;
from tensorflow.python.client import device_lib; print(device_lib.list_local_devices());
with tf.device('/gpu:0') as d:
    print(tf.__version__,d)
122/1:
import random
from collections import deque

import numpy as np
import tensorflow as tf
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard
from tensorflow.keras.layers import LSTM, BatchNormalization, Dense, Dropout
from tensorflow.keras.models import Sequential

from util_funcs import *

# Paramaters
symbol = 'BTCUSDT'
interval = '15m'
mode = 'atr_multiplier'
param = 1

TESTING_PCT = 0.3
SEQ_LEN = 60
EPOCHS = 10  # how many passes through our data
BATCH_SIZE = 64  # how many batches? Try smaller batch if you're getting OOM (out of memory) errors.

NAME = f"{symbol+interval}_{SEQ_LEN}_SEQ_{int(time.time())}"

scaler = MinMaxScaler(feature_range=(0,1))

def fix_data(n):
    assert(n>0)
    df = read_pickle(f'{symbol}{interval}')
    is_good_data(df,interval)
    write_pickle(df[:-n],f'{symbol}{interval}')

def build_dfts():
    df = load_data(symbol, interval)
    features = build_features(df)
    targets = build_targets(df,mode,param)
    signals = build_signals(targets)
    write_dfts_pickle(symbol, interval,mode,param, [df,features,targets,signals])

def preprocess_features(data_frame):
    print("Preprocessing Data...")

    df = data_frame.copy()

    df.drop([x for x in BAD_IND if x in df.columns.tolist()],axis = 1,inplace = True)

    df.dropna(axis=1,how='all',inplace=True)
    df.dropna(axis =0, how='any',inplace = True)
    df.fillna(method='ffill',inplace = True)

    assert np.all(np.isfinite(df)) # Get inf columns: np.isfinite(df).all()[np.isfinite(df).all() ==False]
    assert not np.any(np.isnan(df)) # np.isnan(df).any()[np.isnan(df).any()]

    df = pd.DataFrame(scaler.fit_transform(df.values), columns=df.columns)
    df.signals = df.signals.astype('int32')

    sequential_data = []  # this is a list that will CONTAIN the sequences
    counter = {0:0,1:0}
    groupby = df.loc[SEQ_LEN:].groupby(['signals']).count().iloc[:,0]
    max_count = min(*groupby)
    # df_new = df.copy().loc[SEQ_LEN:]

    for i in tqdm(range(SEQ_LEN,len(df))):
       if counter[df.iloc[i,-1]] < max_count:
           counter[df.iloc[i,-1]] += 1
           sequential_data.append([df.iloc[(i-SEQ_LEN):i,:-1].values , df.iloc[i,-1]])

    random.shuffle(sequential_data)
    X , y = zip(*sequential_data)
    
    return np.array(X), np.array(y)  # return X and y...and make X a numpy array!
122/2:
[df,features, targets,signals]=read_dfts_pickle(symbol,interval,mode,param)
is_good_data(df,1)

signals.name = 'signals'
features_and_signals = features.join(signals)

testing_mark = sorted(features_and_signals.index.values)[-int(0.05*len(features_and_signals))]
features_signals_train = features_and_signals[(features_and_signals.index <testing_mark)]
features_signals_test = features_and_signals[(features_and_signals.index >= testing_mark)]

train_x, train_y = preprocess_features(features_signals_train)
test_x, test_y = preprocess_features(features_signals_test)

print(f"train data: {len(train_x)} validation: {len(test_x)}")
print(f"Dont buys: {train_y.count(0)}, buys: {train_y.count(1)}")
print(f"VALIDATION Dont buys: {test_y.count(0)}, buys: {test_y.count(1)}")
122/3:
# train_y, test_y = np.array(train_y), np.array(test_y)
import tensorflow as tf
from tensorflow.python.client import device_lib
print(device_lib.list_local_devices())

with tf.device('/gpu:0') as d:
    print(tf.__version__,d)
122/4:
# train_y, test_y = np.array(train_y), np.array(test_y)
import tensorflow as tf
from tensorflow.python.client import device_lib
print(device_lib.list_local_devices())

with tf.device('/gpu:0') as d:
    print(tf.__version__,d)
122/5:
model = Sequential()
model.add(LSTM(128, input_shape=(train_x.shape[1:]), return_sequences=True))
model.add(Dropout(0.2))
model.add(BatchNormalization())

model.add(LSTM(128, return_sequences=True))
model.add(Dropout(0.1))
model.add(BatchNormalization())

model.add(LSTM(128))
model.add(Dropout(0.2))
model.add(BatchNormalization())

model.add(Dense(32, activation='relu'))
model.add(Dropout(0.2))

model.add(Dense(2, activation='softmax'))

opt = tf.keras.optimizers.Adam(lr=0.001, decay=1e-6)

# Compile model
model.compile(
    loss='sparse_categorical_crossentropy',
    optimizer=opt,
    metrics=['accuracy']
)

tensorboard = TensorBoard(log_dir="logs/{}".format(NAME))

filepath = "RNN_Final-{epoch:02d}-{val_acc:.3f}"  # unique file name that will include the epoch and the validation acc for that epoch
checkpoint = ModelCheckpoint("models/{}.model".format(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')) # saves only the best ones

# Train model
history = model.fit(
    train_x, train_y,
    batch_size=BATCH_SIZE,
    epochs=EPOCHS,
    validation_data=(test_x, test_y),
    callbacks=[tensorboard, checkpoint],
)

# Score model
score = model.evaluate(test_x, test_y, verbose=0)
print('Test loss:', score[0])
print('Test accuracy:', score[1])
# Save model
model.save(f"{DATA_DIR}\\models\\{NAME}")
122/6:
model = Sequential()
model.add(LSTM(128, input_shape=(train_x.shape[1:]), return_sequences=True))
model.add(Dropout(0.2))
model.add(BatchNormalization())

model.add(LSTM(128, return_sequences=True))
model.add(Dropout(0.1))
model.add(BatchNormalization())

model.add(LSTM(128))
model.add(Dropout(0.2))
model.add(BatchNormalization())

model.add(Dense(32, activation='relu'))
model.add(Dropout(0.2))

model.add(Dense(2, activation='softmax'))

opt = tf.keras.optimizers.Adam(lr=0.001, decay=1e-6)

# Compile model
model.compile(
    loss='sparse_categorical_crossentropy',
    optimizer=opt,
    metrics=['accuracy']
)

tensorboard = TensorBoard(log_dir="logs\\{}".format(NAME))

filepath = "RNN_Final-{epoch:02d}-{val_acc:.3f}"  # unique file name that will include the epoch and the validation acc for that epoch
checkpoint = ModelCheckpoint("models\\{}.model".format(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')) # saves only the best ones

# Train model
history = model.fit(
    train_x, train_y,
    batch_size=BATCH_SIZE,
    epochs=EPOCHS,
    validation_data=(test_x, test_y),
    callbacks=[tensorboard, checkpoint],
)

# Score model
score = model.evaluate(test_x, test_y, verbose=0)
print('Test loss:', score[0])
print('Test accuracy:', score[1])
# Save model
model.save(f"{DATA_DIR}\\models\\{NAME}")
122/7:
[df,features, targets,signals]=read_dfts_pickle(symbol,interval,mode,param)
is_good_data(df,1)

signals.name = 'signals'
features_and_signals = features.join(signals)

testing_mark = sorted(features_and_signals.index.values)[-int(0.05*len(features_and_signals))]
features_signals_train = features_and_signals[(features_and_signals.index <testing_mark)]
features_signals_test = features_and_signals[(features_and_signals.index >= testing_mark)]

# train_x, train_y = preprocess_features(features_signals_train)
# test_x, test_y = preprocess_features(features_signals_test)

print(f"train data: {len(train_x)} validation: {len(test_x)}")
print(f"Dont buys: {collections.Counter(train_y)[0]}, buys: {collections.Counter(train_y)[1]}")
print(f"VALIDATION Dont buys: {test_y.count(0)}, buys: {test_y.count(1)}")
122/8:
import random
from collections import deque

import numpy as np
import tensorflow as tf
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard
from tensorflow.keras.layers import LSTM, BatchNormalization, Dense, Dropout
from tensorflow.keras.models import Sequential
import collections
from util_funcs import *

# Paramaters
symbol = 'BTCUSDT'
interval = '15m'
mode = 'atr_multiplier'
param = 1

TESTING_PCT = 0.3
SEQ_LEN = 60
EPOCHS = 10  # how many passes through our data
BATCH_SIZE = 64  # how many batches? Try smaller batch if you're getting OOM (out of memory) errors.

NAME = f"{symbol+interval}_{SEQ_LEN}_SEQ_{int(time.time())}"

scaler = MinMaxScaler(feature_range=(0,1))

def fix_data(n):
    assert(n>0)
    df = read_pickle(f'{symbol}{interval}')
    is_good_data(df,interval)
    write_pickle(df[:-n],f'{symbol}{interval}')

def build_dfts():
    df = load_data(symbol, interval)
    features = build_features(df)
    targets = build_targets(df,mode,param)
    signals = build_signals(targets)
    write_dfts_pickle(symbol, interval,mode,param, [df,features,targets,signals])

def preprocess_features(data_frame):
    print("Preprocessing Data...")

    df = data_frame.copy()

    df.drop([x for x in BAD_IND if x in df.columns.tolist()],axis = 1,inplace = True)

    df.dropna(axis=1,how='all',inplace=True)
    df.dropna(axis =0, how='any',inplace = True)
    df.fillna(method='ffill',inplace = True)

    assert np.all(np.isfinite(df)) # Get inf columns: np.isfinite(df).all()[np.isfinite(df).all() ==False]
    assert not np.any(np.isnan(df)) # np.isnan(df).any()[np.isnan(df).any()]

    df = pd.DataFrame(scaler.fit_transform(df.values), columns=df.columns)
    df.signals = df.signals.astype('int32')

    sequential_data = []  # this is a list that will CONTAIN the sequences
    counter = {0:0,1:0}
    groupby = df.loc[SEQ_LEN:].groupby(['signals']).count().iloc[:,0]
    max_count = min(*groupby)
    # df_new = df.copy().loc[SEQ_LEN:]

    for i in tqdm(range(SEQ_LEN,len(df))):
       if counter[df.iloc[i,-1]] < max_count:
           counter[df.iloc[i,-1]] += 1
           sequential_data.append([df.iloc[(i-SEQ_LEN):i,:-1].values , df.iloc[i,-1]])

    random.shuffle(sequential_data)
    X , y = zip(*sequential_data)
    
    return np.array(X), np.array(y)  # return X and y...and make X a numpy array!
122/9:
[df,features, targets,signals]=read_dfts_pickle(symbol,interval,mode,param)
is_good_data(df,1)

signals.name = 'signals'
features_and_signals = features.join(signals)

testing_mark = sorted(features_and_signals.index.values)[-int(0.05*len(features_and_signals))]
features_signals_train = features_and_signals[(features_and_signals.index <testing_mark)]
features_signals_test = features_and_signals[(features_and_signals.index >= testing_mark)]

# train_x, train_y = preprocess_features(features_signals_train)
# test_x, test_y = preprocess_features(features_signals_test)

print(f"train data: {len(train_x)} validation: {len(test_x)}")
print(f"Dont buys: {collections.Counter(train_y)[0]}, buys: {collections.Counter(train_y)[1]}")
print(f"VALIDATION Dont buys: {test_y.count(0)}, buys: {test_y.count(1)}")
122/10:
[df,features, targets,signals]=read_dfts_pickle(symbol,interval,mode,param)
is_good_data(df,1)

signals.name = 'signals'
features_and_signals = features.join(signals)

testing_mark = sorted(features_and_signals.index.values)[-int(0.05*len(features_and_signals))]
features_signals_train = features_and_signals[(features_and_signals.index <testing_mark)]
features_signals_test = features_and_signals[(features_and_signals.index >= testing_mark)]

# train_x, train_y = preprocess_features(features_signals_train)
# test_x, test_y = preprocess_features(features_signals_test)

print(f"train data: {len(train_x)} validation: {len(test_x)}")
print(f"Dont buys: {collections.Counter(train_y)[0]}, buys: {collections.Counter(train_y)[1]}")
print(f"VALIDATION Dont buys: {collections.Counter(test_y)[0]}, buys: {collections.Counter(test_y)[0]}")
122/11:
# train_y, test_y = np.array(train_y), np.array(test_y)
import tensorflow as tf
from tensorflow.python.client import device_lib
print(device_lib.list_local_devices())

with tf.device('/gpu:0') as d:
    print(tf.__version__,d)
122/12:
model = Sequential()
model.add(LSTM(128, input_shape=(train_x.shape[1:]), return_sequences=True))
model.add(Dropout(0.2))
model.add(BatchNormalization())

model.add(LSTM(128, return_sequences=True))
model.add(Dropout(0.1))
model.add(BatchNormalization())

model.add(LSTM(128))
model.add(Dropout(0.2))
model.add(BatchNormalization())

model.add(Dense(32, activation='relu'))
model.add(Dropout(0.2))

model.add(Dense(2, activation='softmax'))

opt = tf.keras.optimizers.Adam(lr=0.001, decay=1e-6)

# Compile model
model.compile(
    loss='sparse_categorical_crossentropy',
    optimizer=opt,
    metrics=['accuracy']
)

tensorboard = TensorBoard(log_dir="logs\\{}".format(NAME))

filepath = "RNN_Final-{epoch:02d}-{val_acc:.3f}"  # unique file name that will include the epoch and the validation acc for that epoch
checkpoint = ModelCheckpoint("models\\{}.model".format(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')) # saves only the best ones

# Train model
history = model.fit(
    train_x, train_y,
    batch_size=BATCH_SIZE,
    epochs=EPOCHS,
    validation_data=(test_x, test_y),
    callbacks=[tensorboard, checkpoint],
)

# Score model
score = model.evaluate(test_x, test_y, verbose=0)
print('Test loss:', score[0])
print('Test accuracy:', score[1])
# Save model
model.save(f"{DATA_DIR}\\models\\{NAME}")
124/1:
a = [3,5]
x,y = map(lambda x: x+1, a)
x,y
130/1:
__file__ = r'c:\_Workspace\arnold\main.py'
import sys
import os
C:/ProgramData/Anaconda3/Scripts/activate
sys.path.append(os.path.dirname(__file__))
%load c:\_Workspace\arnold\main.py


conda activate ptf
130/2:
__file__ = r'c:\_Workspace\arnold\main.py'
import sys
import os
C:/ProgramData/Anaconda3/Scripts/activate
sys.path.append(os.path.dirname(__file__))
%load c:\_Workspace\arnold\main.py


conda activate ptf
136/1:
import numpy as np
import matplotlib.pyplot as plt
136/2:
x = np.linspace(0, 10*np.pi, 1000)
y = np.sin(x)
136/3: plt.plot(x,y)
140/1:
# Install TensorFlow
# !pip install -q tensorflow-gpu==2.0.0-beta1

try:
  %tensorflow_version 2.x  # Colab only.
except Exception:
  pass

import tensorflow as tf
print(tf.__version__)
140/2:
# Load in the data
from sklearn.datasets import load_breast_cancer
140/3:
# Install TensorFlow
# !pip install -q tensorflow-gpu==2.0.0-beta1

try:
  %tensorflow_version 2.x  # Colab only.
except Exception:
  pass

import tensorflow as tf
print(tf.__version__)
140/4:
# Load in the data
from sklearn.datasets import load_breast_cancer
140/5:
# load the data
data = load_breast_cancer()
140/6:
# check the type of 'data'
type(data)
140/7:
# note: it is a Bunch object
# this basically acts like a dictionary where you can treat the keys like attributes
data.keys()
140/8:
# 'data' (the attribute) means the input data
data.data.shape
# it has 569 samples, 30 features
140/9:
# 'targets'
data.target
# note how the targets are just 0s and 1s
# normally, when you have K targets, they are labeled 0..K-1
140/10:
# their meaning is not lost
data.target_names
140/11:
# there are also 569 corresponding targets
data.target.shape
140/12:
# you can also determine the meaning of each feature
data.feature_names
140/13:
# normally we would put all of our imports at the top
# but this lets us tell a story
from sklearn.model_selection import train_test_split


# split the data into train and test sets
# this lets us simulate how our model will perform in the future
X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.33)
N, D = X_train.shape
140/14:
# Scale the data
# you'll learn why scaling is needed in a later course
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
140/15:
# Now all the fun Tensorflow stuff
# Build the model

model = tf.keras.models.Sequential([
  tf.keras.layers.Input(shape=(D,)),
  tf.keras.layers.Dense(1, activation='sigmoid')
])

# Alternatively, you can do:
# model = tf.keras.models.Sequential()
# model.add(tf.keras.layers.Dense(1, input_shape=(D,), activation='sigmoid'))

model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])


# Train the model
r = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100)


# Evaluate the model - evaluate() returns loss and accuracy
print("Train score:", model.evaluate(X_train, y_train))
print("Test score:", model.evaluate(X_test, y_test))
140/16:
# Plot what's returned by model.fit()
import matplotlib.pyplot as plt
plt.plot(r.history['loss'], label='loss')
plt.plot(r.history['val_loss'], label='val_loss')
plt.legend()
140/17:
# Plot the accuracy too
plt.plot(r.history['accuracy'], label='acc')
plt.plot(r.history['val_accuracy'], label='val_acc')
plt.legend()
140/18:
# Make predictions
P = model.predict(X_test)
print(P) # they are outputs of the sigmoid, interpreted as probabilities p(y = 1 | x)
140/19:
# Round to get the actual predictions
# Note: has to be flattened since the targets are size (N,) while the predictions are size (N,1)
import numpy as np
P = np.round(P).flatten()
print(P)
140/20:
# Calculate the accuracy, compare it to evaluate() output
print("Manually calculated accuracy:", np.mean(P == y_test))
print("Evaluate output:", model.evaluate(X_test, y_test))
140/21:
# Let's now save our model to a file
model.save('linearclassifier.h5')
140/22:
# Check that the model file exists
!ls -lh
140/23:
# Let's load the model and confirm that it still works
# Note: there is a bug in Keras where load/save only works if you DON'T use the Input() layer explicitly
# So, make sure you define the model with ONLY Dense(1, input_shape=(D,))
# At least, until the bug is fixed
# https://github.com/keras-team/keras/issues/10417
model = tf.keras.models.load_model('linearclassifier.h5')
print(model.layers)
model.evaluate(X_test, y_test)
140/24:

# Download the file - requires Chrome (at this point)
from google.colab import files
files.download('linearclassifier.h5')
144/1:
# Install TensorFlow
# !pip install -q tensorflow-gpu==2.0.0-beta1

try:
  %tensorflow_version 2.x  # Colab only.
except Exception:
  pass

import tensorflow as tf
print(tf.__version__)
144/2:
# Load in the data
from sklearn.datasets import load_breast_cancer
144/3:
# load the data
data = load_breast_cancer()
144/4:
# check the type of 'data'
type(data)
144/5:
# note: it is a Bunch object
# this basically acts like a dictionary where you can treat the keys like attributes
data.keys()
144/6:
# 'data' (the attribute) means the input data
data.data.shape
# it has 569 samples, 30 features
144/7:
# 'targets'
data.target
# note how the targets are just 0s and 1s
# normally, when you have K targets, they are labeled 0..K-1
144/8:
# their meaning is not lost
data.target_names
144/9:
# there are also 569 corresponding targets
data.target.shape
144/10:
# you can also determine the meaning of each feature
data.feature_names
144/11:
# normally we would put all of our imports at the top
# but this lets us tell a story
from sklearn.model_selection import train_test_split


# split the data into train and test sets
# this lets us simulate how our model will perform in the future
X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.33)
N, D = X_train.shape
144/12:
# Scale the data
# you'll learn why scaling is needed in a later course
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
144/13:
# Now all the fun Tensorflow stuff
# Build the model

model = tf.keras.models.Sequential([
  tf.keras.layers.Input(shape=(D,)),
  tf.keras.layers.Dense(1, activation='sigmoid')
])

# Alternatively, you can do:
# model = tf.keras.models.Sequential()
# model.add(tf.keras.layers.Dense(1, input_shape=(D,), activation='sigmoid'))

model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])


# Train the model
r = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100)


# Evaluate the model - evaluate() returns loss and accuracy
print("Train score:", model.evaluate(X_train, y_train))
print("Test score:", model.evaluate(X_test, y_test))
144/14:
# Plot what's returned by model.fit()
import matplotlib.pyplot as plt
plt.plot(r.history['loss'], label='loss')
plt.plot(r.history['val_loss'], label='val_loss')
plt.legend()
144/15:
# Plot the accuracy too
plt.plot(r.history['accuracy'], label='acc')
plt.plot(r.history['val_accuracy'], label='val_acc')
plt.legend()
144/16:
# Make predictions
P = model.predict(X_test)
print(P) # they are outputs of the sigmoid, interpreted as probabilities p(y = 1 | x)
144/17:
# Round to get the actual predictions
# Note: has to be flattened since the targets are size (N,) while the predictions are size (N,1)
import numpy as np
P = np.round(P).flatten()
print(P)
144/18:
# Calculate the accuracy, compare it to evaluate() output
print("Manually calculated accuracy:", np.mean(P == y_test))
print("Evaluate output:", model.evaluate(X_test, y_test))
144/19:
# Let's now save our model to a file
model.save('linearclassifier.h5')
144/20:
# Check that the model file exists
!ls -lh
144/21:
# Let's load the model and confirm that it still works
# Note: there is a bug in Keras where load/save only works if you DON'T use the Input() layer explicitly
# So, make sure you define the model with ONLY Dense(1, input_shape=(D,))
# At least, until the bug is fixed
# https://github.com/keras-team/keras/issues/10417
model = tf.keras.models.load_model('linearclassifier.h5')
print(model.layers)
model.evaluate(X_test, y_test)
144/22:

# Download the file - requires Chrome (at this point)
from google.colab import files
files.download('linearclassifier.h5')
144/23:
# Install TensorFlow
# !pip install -q tensorflow-gpu==2.0.0-beta1

try:
  %tensorflow_version 2.x  # Colab only.
except Exception:
  pass

import tensorflow as tf
print(tf.__version__)
144/24:
# Load in the data
from sklearn.datasets import load_breast_cancer
144/25:
# load the data
data = load_breast_cancer()
144/26:
# check the type of 'data'
type(data)
144/27:
# note: it is a Bunch object
# this basically acts like a dictionary where you can treat the keys like attributes
data.keys()
144/28:
# 'data' (the attribute) means the input data
data.data.shape
# it has 569 samples, 30 features
144/29:
# 'targets'
data.target
# note how the targets are just 0s and 1s
# normally, when you have K targets, they are labeled 0..K-1
144/30:
# their meaning is not lost
data.target_names
144/31:
# there are also 569 corresponding targets
data.target.shape
144/32:
# you can also determine the meaning of each feature
data.feature_names
144/33:
# normally we would put all of our imports at the top
# but this lets us tell a story
from sklearn.model_selection import train_test_split


# split the data into train and test sets
# this lets us simulate how our model will perform in the future
X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.33)
N, D = X_train.shape
144/34:
# Scale the data
# you'll learn why scaling is needed in a later course
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
144/35:
# Now all the fun Tensorflow stuff
# Build the model

model = tf.keras.models.Sequential([
  tf.keras.layers.Input(shape=(D,)),
  tf.keras.layers.Dense(1, activation='sigmoid')
])

# Alternatively, you can do:
# model = tf.keras.models.Sequential()
# model.add(tf.keras.layers.Dense(1, input_shape=(D,), activation='sigmoid'))

model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])


# Train the model
r = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100)


# Evaluate the model - evaluate() returns loss and accuracy
print("Train score:", model.evaluate(X_train, y_train))
print("Test score:", model.evaluate(X_test, y_test))
144/36:
# Plot what's returned by model.fit()
import matplotlib.pyplot as plt
plt.plot(r.history['loss'], label='loss')
plt.plot(r.history['val_loss'], label='val_loss')
plt.legend()
144/37:
# Plot the accuracy too
plt.plot(r.history['accuracy'], label='acc')
plt.plot(r.history['val_accuracy'], label='val_acc')
plt.legend()
144/38:
# Make predictions
P = model.predict(X_test)
print(P) # they are outputs of the sigmoid, interpreted as probabilities p(y = 1 | x)
144/39:
# Round to get the actual predictions
# Note: has to be flattened since the targets are size (N,) while the predictions are size (N,1)
import numpy as np
P = np.round(P).flatten()
print(P)
144/40:
# Calculate the accuracy, compare it to evaluate() output
print("Manually calculated accuracy:", np.mean(P == y_test))
print("Evaluate output:", model.evaluate(X_test, y_test))
144/41:
# Let's now save our model to a file
model.save('linearclassifier.h5')
144/42:
# Check that the model file exists
!ls -lh
144/43:
# Let's load the model and confirm that it still works
# Note: there is a bug in Keras where load/save only works if you DON'T use the Input() layer explicitly
# So, make sure you define the model with ONLY Dense(1, input_shape=(D,))
# At least, until the bug is fixed
# https://github.com/keras-team/keras/issues/10417
model = tf.keras.models.load_model('linearclassifier.h5')
print(model.layers)
model.evaluate(X_test, y_test)
144/44:

# Download the file - requires Chrome (at this point)
from google.colab import files
files.download('linearclassifier.h5')
144/45:
# Install TensorFlow
# !pip install -q tensorflow-gpu==2.0.0-beta1

try:
  %tensorflow_version 2.x  # Colab only.
except Exception:
  pass

import tensorflow as tf
print(tf.__version__)
144/46:
# Install TensorFlow
# !pip install -q tensorflow-gpu==2.0.0-beta1

try:
  %tensorflow_version 2.x  # Colab only.
except Exception:
  pass

import tensorflow as tf
print(tf.__version__)
144/47:
# Load in the data
from sklearn.datasets import load_breast_cancer
144/48:
# load the data
data = load_breast_cancer()
144/49:
# check the type of 'data'
type(data)
144/50:
# note: it is a Bunch object
# this basically acts like a dictionary where you can treat the keys like attributes
data.keys()
144/51:
# 'data' (the attribute) means the input data
data.data.shape
# it has 569 samples, 30 features
144/52:
# 'targets'
data.target
# note how the targets are just 0s and 1s
# normally, when you have K targets, they are labeled 0..K-1
144/53:
# their meaning is not lost
data.target_names
144/54:
# there are also 569 corresponding targets
data.target.shape
144/55:
# you can also determine the meaning of each feature
data.feature_names
144/56:
# normally we would put all of our imports at the top
# but this lets us tell a story
from sklearn.model_selection import train_test_split


# split the data into train and test sets
# this lets us simulate how our model will perform in the future
X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.33)
N, D = X_train.shape
144/57:
# Scale the data
# you'll learn why scaling is needed in a later course
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
144/58:
# Now all the fun Tensorflow stuff
# Build the model

model = tf.keras.models.Sequential([
  tf.keras.layers.Input(shape=(D,)),
  tf.keras.layers.Dense(1, activation='sigmoid')
])

# Alternatively, you can do:
# model = tf.keras.models.Sequential()
# model.add(tf.keras.layers.Dense(1, input_shape=(D,), activation='sigmoid'))

model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])


# Train the model
r = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100)


# Evaluate the model - evaluate() returns loss and accuracy
print("Train score:", model.evaluate(X_train, y_train))
print("Test score:", model.evaluate(X_test, y_test))
144/59:
# Plot what's returned by model.fit()
import matplotlib.pyplot as plt
plt.plot(r.history['loss'], label='loss')
plt.plot(r.history['val_loss'], label='val_loss')
plt.legend()
144/60:
# Plot the accuracy too
plt.plot(r.history['accuracy'], label='acc')
plt.plot(r.history['val_accuracy'], label='val_acc')
plt.legend()
144/61:
# Make predictions
P = model.predict(X_test)
print(P) # they are outputs of the sigmoid, interpreted as probabilities p(y = 1 | x)
144/62:
# Round to get the actual predictions
# Note: has to be flattened since the targets are size (N,) while the predictions are size (N,1)
import numpy as np
P = np.round(P).flatten()
print(P)
144/63:
# Calculate the accuracy, compare it to evaluate() output
print("Manually calculated accuracy:", np.mean(P == y_test))
print("Evaluate output:", model.evaluate(X_test, y_test))
144/64:
# Let's now save our model to a file
model.save('linearclassifier.h5')
144/65:
# Check that the model file exists
!ls -lh
144/66:
# Let's load the model and confirm that it still works
# Note: there is a bug in Keras where load/save only works if you DON'T use the Input() layer explicitly
# So, make sure you define the model with ONLY Dense(1, input_shape=(D,))
# At least, until the bug is fixed
# https://github.com/keras-team/keras/issues/10417
model = tf.keras.models.load_model('linearclassifier.h5')
print(model.layers)
model.evaluate(X_test, y_test)
144/67:

# Download the file - requires Chrome (at this point)
from google.colab import files
files.download('linearclassifier.h5')
144/68:
# Install TensorFlow
# !pip install -q tensorflow-gpu==2.0.0-beta1

try:
  %tensorflow_version 2.x  # Colab only.
except Exception:
  pass

import tensorflow as tf
print(tf.__version__)
154/1:
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
from sklearn.metrics import confusion_matrix, plot_confusion_matrix, plot_roc_curve
154/2: df = load_breast_cancer()
154/3: df.keys()
156/1:
# Install TensorFlow
# !pip install -q tensorflow-gpu==2.0.0-beta1

try:
  %tensorflow_version 2.x  # Colab only.
except Exception:
  pass

import tensorflow as tf
print(tf.__version__)
156/2:
# Install TensorFlow
# !pip install -q tensorflow-gpu==2.0.0-beta1

try:
  %tensorflow_version 2.x  # Colab only.
except Exception:
  pass

import tensorflow as tf
print(tf.__version__)
156/3:
# Load in the data
from sklearn.datasets import load_breast_cancer
156/4:
# load the data
data = load_breast_cancer()
156/5:
# check the type of 'data'
type(data)
156/6:
# note: it is a Bunch object
# this basically acts like a dictionary where you can treat the keys like attributes
data.keys()
156/7:
# 'data' (the attribute) means the input data
data.data.shape
# it has 569 samples, 30 features
156/8:
# 'targets'
data.target
# note how the targets are just 0s and 1s
# normally, when you have K targets, they are labeled 0..K-1
156/9:
# their meaning is not lost
data.target_names
156/10:
# there are also 569 corresponding targets
data.target.shape
156/11:
# you can also determine the meaning of each feature
data.feature_names
156/12:
# normally we would put all of our imports at the top
# but this lets us tell a story
from sklearn.model_selection import train_test_split


# split the data into train and test sets
# this lets us simulate how our model will perform in the future
X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.33)
N, D = X_train.shape
156/13:
# Scale the data
# you'll learn why scaling is needed in a later course
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
156/14:
# Now all the fun Tensorflow stuff
# Build the model

model = tf.keras.models.Sequential([
  tf.keras.layers.Input(shape=(D,)),
  tf.keras.layers.Dense(1, activation='sigmoid')
])

# Alternatively, you can do:
# model = tf.keras.models.Sequential()
# model.add(tf.keras.layers.Dense(1, input_shape=(D,), activation='sigmoid'))

model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])


# Train the model
r = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100)


# Evaluate the model - evaluate() returns loss and accuracy
print("Train score:", model.evaluate(X_train, y_train))
print("Test score:", model.evaluate(X_test, y_test))
154/4:
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
from sklearn.metrics import confusion_matrix, plot_confusion_matrix, plot_roc_curve
154/5: df = load_breast_cancer()
154/6: df.keys()
154/7: df.feature_names
154/8: df.target_names
154/9: df.data.shape
154/10: df.target.shape
154/11:
x_train, x_test, y_train, y_test = train_test_split(df.data,df.target,test_size=0.33)
N,D = x_train.shape
154/12:
scaler = StandardScaler()
x_train = scaler.fit_transform(x_train)
x_test = scaler.transform(x_test)
154/13:
model = keras.models.Sequential([
    keras.layers.Input(shape=(D,)),
    keras.layers.Dense(1, activation='sigmoid')])

opt = keras.optimizers.Adam(learning_rate=.01)
model.compile(optimizer = opt, loss = 'binary_crossentropy', metrics=['mae','accuracy']) # 'binary_accuracy','acc','accuracy' are the same!

r= model.fit(x_train,y_train,batch_size=32,epochs=100,validation_data=(x_test,y_test))
156/15:
# Plot what's returned by model.fit()
import matplotlib.pyplot as plt
plt.plot(r.history['loss'], label='loss')
plt.plot(r.history['val_loss'], label='val_loss')
plt.legend()
156/16:
# Plot the accuracy too
plt.plot(r.history['accuracy'], label='acc')
plt.plot(r.history['val_accuracy'], label='val_acc')
plt.legend()
156/17:
# Make predictions
P = model.predict(X_test)
print(P) # they are outputs of the sigmoid, interpreted as probabilities p(y = 1 | x)
156/18:
# Round to get the actual predictions
# Note: has to be flattened since the targets are size (N,) while the predictions are size (N,1)
import numpy as np
P = np.round(P).flatten()
print(P)
156/19:
# Calculate the accuracy, compare it to evaluate() output
print("Manually calculated accuracy:", np.mean(P == y_test))
print("Evaluate output:", model.evaluate(X_test, y_test))
156/20:
# Let's now save our model to a file
model.save('linearclassifier.h5')
156/21:
# Check that the model file exists
!ls -lh
156/22:
# Let's load the model and confirm that it still works
# Note: there is a bug in Keras where load/save only works if you DON'T use the Input() layer explicitly
# So, make sure you define the model with ONLY Dense(1, input_shape=(D,))
# At least, until the bug is fixed
# https://github.com/keras-team/keras/issues/10417
model = tf.keras.models.load_model('linearclassifier.h5')
print(model.layers)
model.evaluate(X_test, y_test)
156/23:

# Download the file - requires Chrome (at this point)
from google.colab import files
files.download('linearclassifier.h5')
154/14:
print("Train score:", model.evaluate(x_train, y_train))
print("Test score:", model.evaluate(x_test, y_test))
158/1:
print(r.history.keys())
plt.plot(r.history['loss'],label='loss')
plt.plot(r.history['val_loss'],label='val_loss')
plt.legend()
158/2:
fig, (ax0,ax1) = plt.subplots(2,1,sharex=True)

ax0.plot(r.history['loss'],label='loss')
ax0.plot(r.history['val_loss'],label='val_loss')
ax0.legend()

ax1.plot(r.history['accuracy'],label='accuracy')
ax1.plot(r.history['val_accuracy'],label='val_accuracy')
ax1.legend()
plt.show()
158/3: plot_confusion_matrix(model,x_test,y_test)
158/4:
model = keras.models.Sequential([
    keras.layers.Input(shape=(D,)),
    keras.layers.Dense(1, activation='sigmoid')])

opt = keras.optimizers.Adam(learning_rate=.01)
model.compile(optimizer = opt, loss = 'binary_crossentropy', metrics=['mae','accuracy']) # 'binary_accuracy','acc','accuracy' are the same!

r= model.fit(x_train,y_train,batch_size=32,epochs=100,validation_data=(x_test,y_test))
158/5:
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
from sklearn.metrics import confusion_matrix, plot_confusion_matrix, plot_roc_curve
158/6:
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
from sklearn.metrics import confusion_matrix, plot_confusion_matrix, plot_roc_curve
print(tf.__version__)
158/7:
model = keras.models.Sequential([
    keras.layers.Input(shape=(D,)),
    keras.layers.Dense(1, activation='sigmoid')])

opt = keras.optimizers.Adam(learning_rate=.01)
model.compile(optimizer = opt, loss = 'binary_crossentropy', metrics=['mae','accuracy']) # 'binary_accuracy','acc','accuracy' are the same!

r= model.fit(x_train,y_train,batch_size=32,epochs=100,validation_data=(x_test,y_test))
158/8:
model = tf.keras.models.Sequential([
    keras.layers.Input(shape=(D,)),
    keras.layers.Dense(1, activation='sigmoid')])

opt = keras.optimizers.Adam(learning_rate=.01)
model.compile(optimizer = opt, loss = 'binary_crossentropy', metrics=['mae','accuracy']) # 'binary_accuracy','acc','accuracy' are the same!

r= model.fit(x_train,y_train,batch_size=32,epochs=100,validation_data=(x_test,y_test))
158/9: df = load_breast_cancer()
158/10: df.keys()
158/11: df.feature_names
158/12: df.target_names
158/13: df.data.shape
158/14: df.target.shape
158/15:
x_train, x_test, y_train, y_test = train_test_split(df.data,df.target,test_size=0.33)
N,D = x_train.shape
158/16:
scaler = StandardScaler()
x_train = scaler.fit_transform(x_train)
x_test = scaler.transform(x_test)
158/17:
model = tf.keras.models.Sequential([
    keras.layers.Input(shape=(D,)),
    keras.layers.Dense(1, activation='sigmoid')])

opt = keras.optimizers.Adam(learning_rate=.01)
model.compile(optimizer = opt, loss = 'binary_crossentropy', metrics=['mae','accuracy']) # 'binary_accuracy','acc','accuracy' are the same!

r= model.fit(x_train,y_train,batch_size=32,epochs=100,validation_data=(x_test,y_test))
159/1:
model = tf.keras.models.Sequential([
    keras.layers.Input(shape=(D,)),
    keras.layers.Dense(1, activation='sigmoid')])

opt = keras.optimizers.Adam(learning_rate=.01)
model.compile(optimizer = opt, loss = 'binary_crossentropy', metrics=['mae','accuracy']) # 'binary_accuracy','acc','accuracy' are the same!

r= model.fit(x_train,y_train,batch_size=16,epochs=100,validation_data=(x_test,y_test))
159/2:
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import tensorflow as tf
# from tensorflow import keras
from sklearn.metrics import confusion_matrix, plot_confusion_matrix, plot_roc_curve
print(tf.__version__)
159/3:
model = tf.keras.models.Sequential([
    keras.layers.Input(shape=(D,)),
    keras.layers.Dense(1, activation='sigmoid')])

opt = keras.optimizers.Adam(learning_rate=.01)
model.compile(optimizer = opt, loss = 'binary_crossentropy', metrics=['mae','accuracy']) # 'binary_accuracy','acc','accuracy' are the same!

r= model.fit(x_train,y_train,batch_size=16,epochs=100,validation_data=(x_test,y_test))
159/4: tf.keras.Sequential
159/5:
model = tf.keras.models.Sequential([
    keras.layers.Input(shape=(D,)),
    keras.layers.Dense(1, activation='sigmoid')])

opt = keras.optimizers.Adam(learning_rate=.01)
model.compile(optimizer = opt, loss = 'binary_crossentropy', metrics=['mae','accuracy']) # 'binary_accuracy','acc','accuracy' are the same!

r= model.fit(x_train,y_train,batch_size=16,epochs=100,validation_data=(x_test,y_test))
159/6:
model = tensorflow.keras.models.Sequential([
    keras.layers.Input(shape=(D,)),
    keras.layers.Dense(1, activation='sigmoid')])

opt = keras.optimizers.Adam(learning_rate=.01)
model.compile(optimizer = opt, loss = 'binary_crossentropy', metrics=['mae','accuracy']) # 'binary_accuracy','acc','accuracy' are the same!

r= model.fit(x_train,y_train,batch_size=16,epochs=100,validation_data=(x_test,y_test))
159/7:
model = tensorflow.keras.Sequential([
    keras.layers.Input(shape=(D,)),
    keras.layers.Dense(1, activation='sigmoid')])

opt = keras.optimizers.Adam(learning_rate=.01)
model.compile(optimizer = opt, loss = 'binary_crossentropy', metrics=['mae','accuracy']) # 'binary_accuracy','acc','accuracy' are the same!

r= model.fit(x_train,y_train,batch_size=16,epochs=100,validation_data=(x_test,y_test))
159/8:
model = keras.models.Sequential([
    keras.layers.Input(shape=(D,)),
    keras.layers.Dense(1, activation='sigmoid')])

opt = keras.optimizers.Adam(learning_rate=.01)
model.compile(optimizer = opt, loss = 'binary_crossentropy', metrics=['mae','accuracy']) # 'binary_accuracy','acc','accuracy' are the same!

r= model.fit(x_train,y_train,batch_size=16,epochs=100,validation_data=(x_test,y_test))
159/9:
model = tf.keras.models.Sequential([
    keras.layers.Input(shape=(D,)),
    keras.layers.Dense(1, activation='sigmoid')])

opt = keras.optimizers.Adam(learning_rate=.01)
model.compile(optimizer = opt, loss = 'binary_crossentropy', metrics=['mae','accuracy']) # 'binary_accuracy','acc','accuracy' are the same!

r= model.fit(x_train,y_train,batch_size=16,epochs=100,validation_data=(x_test,y_test))
159/10:
tf.keras.models.Sequential([
    keras.layers.Input(shape=(D,)),
    keras.layers.Dense(1, activation='sigmoid')])

opt = keras.optimizers.Adam(learning_rate=.01)
model.compile(optimizer = opt, loss = 'binary_crossentropy', metrics=['mae','accuracy']) # 'binary_accuracy','acc','accuracy' are the same!

r= model.fit(x_train,y_train,batch_size=16,epochs=100,validation_data=(x_test,y_test))
159/11: tf.keras.models.Sequential()
159/12:
model = tf.keras.models.Sequential([
    keras.layers.Input(shape=(D,)),
    keras.layers.Dense(1, activation='sigmoid')])
159/13: model = tf.keras.models.Sequential()
159/14:
model = tf.keras.models.Sequential()

model.add(keras.layers.Input(shape=(D,)),)
model.add(keras.layers.Dense(1, activation='sigmoid'))

opt = keras.optimizers.Adam(learning_rate=.01)
model.compile(optimizer = opt, loss = 'binary_crossentropy', metrics=['mae','accuracy']) # 'binary_accuracy','acc','accuracy' are the same!

r= model.fit(x_train,y_train,batch_size=16,epochs=100,validation_data=(x_test,y_test))
159/15:
model = tf.keras.models.Sequential()

model.add(tf.keras.layers.Input(shape=(D,)),)
model.add(tf.keras.layers.Dense(1, activation='sigmoid'))

opt = keras.optimizers.Adam(learning_rate=.01)
model.compile(optimizer = opt, loss = 'binary_crossentropy', metrics=['mae','accuracy']) # 'binary_accuracy','acc','accuracy' are the same!

r= model.fit(x_train,y_train,batch_size=16,epochs=100,validation_data=(x_test,y_test))
159/16:
# model = tf.keras.models.Sequential()
D
159/17: D
159/18: print(D)
159/19:
x_train, x_test, y_train, y_test = train_test_split(df.data,df.target,test_size=0.33)
N,D = x_train.shape
159/20:
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import tensorflow as tf
# from tensorflow import keras
from sklearn.metrics import confusion_matrix, plot_confusion_matrix, plot_roc_curve
print(tf.__version__)
159/21: df = load_breast_cancer()
159/22: df.keys()
159/23: df.feature_names
159/24: df.target_names
159/25: df.data.shape
159/26: df.target.shape
159/27:
x_train, x_test, y_train, y_test = train_test_split(df.data,df.target,test_size=0.33)
N,D = x_train.shape
159/28:
scaler = StandardScaler()
x_train = scaler.fit_transform(x_train)
x_test = scaler.transform(x_test)
159/29:
model = tf.keras.models.Sequential()

model.add(tf.keras.layers.Input(shape=(D,)),)
model.add(tf.keras.layers.Dense(1, activation='sigmoid'))

opt = keras.optimizers.Adam(learning_rate=.01)
model.compile(optimizer = opt, loss = 'binary_crossentropy', metrics=['mae','accuracy']) # 'binary_accuracy','acc','accuracy' are the same!

r= model.fit(x_train,y_train,batch_size=16,epochs=100,validation_data=(x_test,y_test))
159/30:
print("Train score:", model.evaluate(x_train, y_train))
print("Test score:", model.evaluate(x_test, y_test))
159/31:
print(r.history.keys())
plt.plot(r.history['loss'],label='loss')
plt.plot(r.history['val_loss'],label='val_loss')
plt.legend()
159/32:
fig, (ax0,ax1) = plt.subplots(2,1,sharex=True)

ax0.plot(r.history['loss'],label='loss')
ax0.plot(r.history['val_loss'],label='val_loss')
ax0.legend()

ax1.plot(r.history['accuracy'],label='accuracy')
ax1.plot(r.history['val_accuracy'],label='val_accuracy')
ax1.legend()
plt.show()
159/33: plot_confusion_matrix(model,x_test,y_test)
159/34:
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
from sklearn.metrics import confusion_matrix, plot_confusion_matrix, plot_roc_curve
print(tf.__version__)
159/35: df = load_breast_cancer()
159/36: df.keys()
159/37: df.feature_names
159/38: df.target_names
159/39: df.data.shape
159/40: df.target.shape
159/41:
x_train, x_test, y_train, y_test = train_test_split(df.data,df.target,test_size=0.33)
N,D = x_train.shape
159/42:
scaler = StandardScaler()
x_train = scaler.fit_transform(x_train)
x_test = scaler.transform(x_test)
159/43:
model = tf.keras.models.Sequential()

model.add(tf.keras.layers.Input(shape=(D,)),)
model.add(tf.keras.layers.Dense(1, activation='sigmoid'))

opt = keras.optimizers.Adam(learning_rate=.01)
model.compile(optimizer = opt, loss = 'binary_crossentropy', metrics=['mae','accuracy']) # 'binary_accuracy','acc','accuracy' are the same!

r= model.fit(x_train,y_train,batch_size=16,epochs=100,validation_data=(x_test,y_test))
159/44:
print("Train score:", model.evaluate(x_train, y_train))
print("Test score:", model.evaluate(x_test, y_test))
160/1: D
160/2:
print(r.history.keys())
plt.plot(r.history['loss'],label='loss')
plt.plot(r.history['val_loss'],label='val_loss')
plt.legend()
160/3: D
160/4:
fig, (ax0,ax1) = plt.subplots(2,1,sharex=True)

ax0.plot(r.history['loss'],label='loss')
ax0.plot(r.history['val_loss'],label='val_loss')
ax0.legend()

ax1.plot(r.history['accuracy'],label='accuracy')
ax1.plot(r.history['val_accuracy'],label='val_accuracy')
ax1.legend()
plt.show()
160/5: plot_confusion_matrix(model,x_test,y_test)
160/6: print(D)
155/1:
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
from sklearn.metrics import confusion_matrix, plot_confusion_matrix, plot_roc_curve
print(tf.__version__)
155/2: df = load_breast_cancer()
155/3: df.keys()
155/4: df.feature_names
155/5: df.target_names
155/6: df.data.shape
155/7: df.target.shape
155/8:
x_train, x_test, y_train, y_test = train_test_split(df.data,df.target,test_size=0.33)
N,D = x_train.shape
155/9:
scaler = StandardScaler()
x_train = scaler.fit_transform(x_train)
x_test = scaler.transform(x_test)
155/10: print(D)
155/11:
model = tf.keras.models.Sequential()

model.add(tf.keras.layers.Input(shape=(D,)),)
model.add(tf.keras.layers.Dense(1, activation='sigmoid'))

opt = keras.optimizers.Adam(learning_rate=.01)
model.compile(optimizer = opt, loss = 'binary_crossentropy', metrics=['mae','accuracy']) # 'binary_accuracy','acc','accuracy' are the same!

r= model.fit(x_train,y_train,batch_size=16,epochs=100,validation_data=(x_test,y_test))
155/12:
print("Train score:", model.evaluate(x_train, y_train))
print("Test score:", model.evaluate(x_test, y_test))
162/1:
print(r.history.keys())
plt.plot(r.history['loss'],label='loss')
plt.plot(r.history['val_loss'],label='val_loss')
plt.legend()
162/2:
fig, (ax0,ax1) = plt.subplots(2,1,sharex=True)

ax0.plot(r.history['loss'],label='loss')
ax0.plot(r.history['val_loss'],label='val_loss')
ax0.legend()

ax1.plot(r.history['accuracy'],label='accuracy')
ax1.plot(r.history['val_accuracy'],label='val_accuracy')
ax1.legend()
plt.show()
162/3: plot_confusion_matrix(model,x_test,y_test)
162/4: D
162/5:
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
from sklearn.metrics import confusion_matrix, plot_confusion_matrix, plot_roc_curve
print(tf.__version__)
162/6:
import tensorflow as tf
a = tf.constant(3)
a*4
162/7:
import tensorflow as tf
a = tf.constant(3)
print(a*4)
162/8:
import tensorflow as tf
with tf.device('/gpu:0'):
    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')
    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')
    c = tf.matmul(a, b)
163/1:
import tensorflow as tf
with tf.device('/gpu:0'):
    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')
    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')
    c = tf.matmul(a, b)
163/2:
import tensorflow as tf
with tf.device('/gpu:0'):
    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')
    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')
    c = tf.matmul(a, b)
163/3:
import tensorflow as tf
with tf.device('/gpu:0'):
    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')
    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')
    c = tf.matmul(a, b)
163/4:
import tensorflow as tf
with tf.device('/gpu:0'):
    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')
    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')
    c = tf.matmul(a, b)
163/5:
import tensorflow as tf
with tf.device('/gpu:0'):
    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')
    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')
    c = tf.matmul(a, b)
164/1:
import tensorflow as tf
with tf.device('/gpu:0'):
    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')
    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2,3], name='b')
    c = a+b
164/2:
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
from sklearn.metrics import confusion_matrix, plot_confusion_matrix, plot_roc_curve
print(tf.__version__)
164/3:
# import tensorflow as tf
# with tf.device('/gpu:0'):
#     a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')
#     b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2,3], name='b')
#     c = a+b
164/4: df = load_breast_cancer()
164/5: df.keys()
164/6: df.feature_names
164/7: df.target_names
164/8: df.data.shape
164/9: df.target.shape
164/10:
x_train, x_test, y_train, y_test = train_test_split(df.data,df.target,test_size=0.33)
N,D = x_train.shape
164/11:
scaler = StandardScaler()
x_train = scaler.fit_transform(x_train)
x_test = scaler.transform(x_test)
164/12: print(D)
164/13:
model = tf.keras.models.Sequential()

model.add(tf.keras.layers.Input(shape=(D,)),)
model.add(tf.keras.layers.Dense(1, activation='sigmoid'))

opt = keras.optimizers.Adam(learning_rate=.01)
model.compile(optimizer = opt, loss = 'binary_crossentropy', metrics=['mae','accuracy']) # 'binary_accuracy','acc','accuracy' are the same!

r= model.fit(x_train,y_train,batch_size=16,epochs=100,validation_data=(x_test,y_test))
164/14:
print("Train score:", model.evaluate(x_train, y_train))
print("Test score:", model.evaluate(x_test, y_test))
166/1:
print(r.history.keys())
plt.plot(r.history['loss'],label='loss')
plt.plot(r.history['val_loss'],label='val_loss')
plt.legend()
166/2:
model = tf.keras.models.Sequential()

model.add(tf.keras.layers.Input(shape=(D,)))
model.add(tf.keras.layers.Dense(1, activation='sigmoid'))

opt = keras.optimizers.Adam(learning_rate=.01)
model.compile(optimizer = opt, loss = 'binary_crossentropy', metrics=['mae','accuracy']) # 'binary_accuracy','acc','accuracy' are the same!

r= model.fit(x_train,y_train,batch_size=16,epochs=100,validation_data=(x_test,y_test))
166/3:
fig, (ax0,ax1) = plt.subplots(2,1,sharex=True)

ax0.plot(r.history['loss'],label='loss')
ax0.plot(r.history['val_loss'],label='val_loss')
ax0.legend()

ax1.plot(r.history['accuracy'],label='accuracy')
ax1.plot(r.history['val_accuracy'],label='val_accuracy')
ax1.legend()
plt.show()
166/4: plot_confusion_matrix(model,x_test,y_test)
166/5:
model = tf.keras.models.Sequential()

model.add(tf.keras.layers.Input(shape=(D,))
model.add(tf.keras.layers.Dense(1, activation='sigmoid'))

opt = keras.optimizers.Adam(learning_rate=.01)
model.compile(optimizer = opt, loss = 'binary_crossentropy', metrics=['mae','accuracy']) # 'binary_accuracy','acc','accuracy' are the same!

r= model.fit(x_train,y_train,batch_size=16,epochs=100,validation_data=(x_test,y_test))
166/6:
model = tf.keras.models.Sequential()

model.add(tf.keras.layers.Input(shape=(D,)))
model.add(tf.keras.layers.Dense(1, activation='sigmoid'))

opt = keras.optimizers.Adam(learning_rate=.01)
model.compile(optimizer = opt, loss = 'binary_crossentropy', metrics=['mae','accuracy']) # 'binary_accuracy','acc','accuracy' are the same!

r= model.fit(x_train,y_train,batch_size=16,epochs=100,validation_data=(x_test,y_test))
166/7:
model = tf.keras.Sequential()

model.add(tf.keras.layers.Input(shape=(D,)))
model.add(tf.keras.layers.Dense(1, activation='sigmoid'))

opt = keras.optimizers.Adam(learning_rate=.01)
model.compile(optimizer = opt, loss = 'binary_crossentropy', metrics=['mae','accuracy']) # 'binary_accuracy','acc','accuracy' are the same!

r= model.fit(x_train,y_train,batch_size=16,epochs=100,validation_data=(x_test,y_test))
166/8:
model = tf.keras.models.Sequential()

model.add(tf.keras.layers.Input(shape=(D,)))
model.add(tf.keras.layers.Dense(1, activation='sigmoid'))

opt = keras.optimizers.Adam(learning_rate=.01)
model.compile(optimizer = opt, loss = 'binary_crossentropy', metrics=['mae','accuracy']) # 'binary_accuracy','acc','accuracy' are the same!

r= model.fit(x_train,y_train,batch_size=16,epochs=100,validation_data=(x_test,y_test))
166/9:
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
from sklearn.metrics import confusion_matrix, plot_confusion_matrix, plot_roc_curve
print(tf.__version__)
166/10:
# import tensorflow as tf
# with tf.device('/gpu:0'):
#     a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')
#     b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2,3], name='b')
#     c = a+b
166/11: df = load_breast_cancer()
166/12: df.keys()
166/13: df.feature_names
166/14: df.target_names
166/15: df.data.shape
166/16: df.target.shape
166/17:
x_train, x_test, y_train, y_test = train_test_split(df.data,df.target,test_size=0.33)
N,D = x_train.shape
166/18:
scaler = StandardScaler()
x_train = scaler.fit_transform(x_train)
x_test = scaler.transform(x_test)
166/19: print(D)
166/20:
model = tf.keras.models.Sequential()

model.add(tf.keras.layers.Input(shape=(D,)))
model.add(tf.keras.layers.Dense(1, activation='sigmoid'))

opt = keras.optimizers.Adam(learning_rate=.01)
model.compile(optimizer = opt, loss = 'binary_crossentropy', metrics=['mae','accuracy']) # 'binary_accuracy','acc','accuracy' are the same!

r= model.fit(x_train,y_train,batch_size=16,epochs=100,validation_data=(x_test,y_test))
167/1:
print("Train score:", model.evaluate(x_train, y_train))
print("Test score:", model.evaluate(x_test, y_test))
167/2:
model = tf.keras.Sequential()

model.add(tf.keras.layers.Input(shape=(D,)))
model.add(tf.keras.layers.Dense(1, activation='sigmoid'))

opt = keras.optimizers.Adam(learning_rate=.01)
model.compile(optimizer = opt, loss = 'binary_crossentropy', metrics=['mae','accuracy']) # 'binary_accuracy','acc','accuracy' are the same!

r= model.fit(x_train,y_train,batch_size=32,epochs=100,validation_data=(x_test,y_test))
167/3:
model = keras.models.Sequential()

model.add(tf.keras.layers.Input(shape=(D,)))
model.add(tf.keras.layers.Dense(1, activation='sigmoid'))

opt = keras.optimizers.Adam(learning_rate=.01)
model.compile(optimizer = opt, loss = 'binary_crossentropy', metrics=['mae','accuracy']) # 'binary_accuracy','acc','accuracy' are the same!

r= model.fit(x_train,y_train,batch_size=32,epochs=100,validation_data=(x_test,y_test))
167/4:
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
from sklearn.metrics import confusion_matrix, plot_confusion_matrix, plot_roc_curve
print(tf.__version__)
167/5:
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import tensorflow as tf
import tensorflow.keras
from sklearn.metrics import confusion_matrix, plot_confusion_matrix, plot_roc_curve
print(tf.__version__)
167/6:
model = keras.models.Sequential()

model.add(tf.keras.layers.Input(shape=(D,)))
model.add(tf.keras.layers.Dense(1, activation='sigmoid'))

opt = keras.optimizers.Adam(learning_rate=.01)
model.compile(optimizer = opt, loss = 'binary_crossentropy', metrics=['mae','accuracy']) # 'binary_accuracy','acc','accuracy' are the same!

r= model.fit(x_train,y_train,batch_size=32,epochs=100,validation_data=(x_test,y_test))
170/1:
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import tensorflow as tf
import tensorflow.keras
from sklearn.metrics import confusion_matrix, plot_confusion_matrix, plot_roc_curve
print(tf.__version__)
170/2:
import tensorflow as tf
with tf.device('/gpu:0'):
    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')
    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2,3], name='b')
    c = a+b
170/3:
import tensorflow as tf
with tf.device('/gpu:0'):
    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')
    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2,3], name='b')
    c = a+b
    print(c)
170/4: df = load_breast_cancer()
170/5: df = load_breast_cancer()
170/6: df = load_breast_cancer()
170/7: df = load_breast_cancer()
170/8: df.keys()
170/9: df.keys()
170/10: df.keys()
170/11: df.keys()
170/12: df.keys()
170/13: df.feature_names
170/14: df.target_names
170/15: df.data.shape
170/16: df.target.shape
170/17:
x_train, x_test, y_train, y_test = train_test_split(df.data,df.target,test_size=0.33)
N,D = x_train.shape
170/18:
scaler = StandardScaler()
x_train = scaler.fit_transform(x_train)
x_test = scaler.transform(x_test)
170/19: print(D)
170/20: print(D)
170/21:
model = keras.models.Sequential()

model.add(tf.keras.layers.Input(shape=(D,)))
model.add(tf.keras.layers.Dense(1, activation='sigmoid'))

opt = keras.optimizers.Adam(learning_rate=.01)
model.compile(optimizer = opt, loss = 'binary_crossentropy', metrics=['mae','accuracy']) # 'binary_accuracy','acc','accuracy' are the same!

r= model.fit(x_train,y_train,batch_size=32,epochs=100,validation_data=(x_test,y_test))
170/22:
model = keras.models.Sequential()

model.add(tf.keras.layers.Input(shape=(D,)))
model.add(tf.keras.layers.Dense(1, activation='sigmoid'))

opt = keras.optimizers.Adam(learning_rate=.01)
model.compile(optimizer = opt, loss = 'binary_crossentropy', metrics=['mae','accuracy']) # 'binary_accuracy','acc','accuracy' are the same!

r= model.fit(x_train,y_train,batch_size=32,epochs=100,validation_data=(x_test,y_test))
170/23:
model = keras.models.Sequential()

model.add(tf.keras.layers.Input(shape=(D,)))
model.add(tf.keras.layers.Dense(1, activation='sigmoid'))

opt = keras.optimizers.Adam(learning_rate=.01)
model.compile(optimizer = opt, loss = 'binary_crossentropy', metrics=['mae','accuracy']) # 'binary_accuracy','acc','accuracy' are the same!

r= model.fit(x_train,y_train,batch_size=32,epochs=100,validation_data=(x_test,y_test))
170/24:
model = keras.models.Sequential()

model.add(tf.keras.layers.Input(shape=(D,)))
model.add(tf.keras.layers.Dense(1, activation='sigmoid'))

opt = keras.optimizers.Adam(learning_rate=.01)
model.compile(optimizer = opt, loss = 'binary_crossentropy', metrics=['mae','accuracy']) # 'binary_accuracy','acc','accuracy' are the same!

r= model.fit(x_train,y_train,batch_size=32,epochs=100,validation_data=(x_test,y_test))
170/25:
model = tf.keras.models.Sequential()

model.add(tf.keras.layers.Input(shape=(D,)))
model.add(tf.keras.layers.Dense(1, activation='sigmoid'))

opt = keras.optimizers.Adam(learning_rate=.01)
model.compile(optimizer = opt, loss = 'binary_crossentropy', metrics=['mae','accuracy']) # 'binary_accuracy','acc','accuracy' are the same!

r= model.fit(x_train,y_train,batch_size=32,epochs=100,validation_data=(x_test,y_test))
170/26:
print(D)
tf.keras
170/27:
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
from sklearn.metrics import confusion_matrix, plot_confusion_matrix, plot_roc_curve
print(tf.__version__)
170/28:
df = load_breast_cancer()
keras
170/29:
df = load_breast_cancer()
print(keras)
170/30:
model = keras.models.Sequential()

model.add(keras.layers.Input(shape=(D,)))
model.add(keras.layers.Dense(1, activation='sigmoid'))

opt = keras.optimizers.Adam(learning_rate=.01)
model.compile(optimizer = opt, loss = 'binary_crossentropy', metrics=['mae','accuracy']) # 'binary_accuracy','acc','accuracy' are the same!

r= model.fit(x_train,y_train,batch_size=32,epochs=100,validation_data=(x_test,y_test))
170/31:
s = time.time()
model = keras.models.Sequential()

model.add(keras.layers.Input(shape=(D,)))
model.add(keras.layers.Dense(1, activation='sigmoid'))

opt = keras.optimizers.Adam(learning_rate=.01)
model.compile(optimizer = opt, loss = 'binary_crossentropy', metrics=['mae','accuracy']) # 'binary_accuracy','acc','accuracy' are the same!

r= model.fit(x_train,y_train,batch_size=32,epochs=100,validation_data=(x_test,y_test))
print(time.time() - s)
170/32:
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
from sklearn.metrics import confusion_matrix, plot_confusion_matrix, plot_roc_curve
from time import time
print(tf.__version__)
170/33:
s = time.time()
model = keras.models.Sequential()

model.add(keras.layers.Input(shape=(D,)))
model.add(keras.layers.Dense(1, activation='sigmoid'))

opt = keras.optimizers.Adam(learning_rate=.01)
model.compile(optimizer = opt, loss = 'binary_crossentropy', metrics=['mae','accuracy']) # 'binary_accuracy','acc','accuracy' are the same!

r= model.fit(x_train,y_train,batch_size=32,epochs=100,validation_data=(x_test,y_test))
print(time.time() - s)
170/34:
s = time()
model = keras.models.Sequential()

model.add(keras.layers.Input(shape=(D,)))
model.add(keras.layers.Dense(1, activation='sigmoid'))

opt = keras.optimizers.Adam(learning_rate=.01)
model.compile(optimizer = opt, loss = 'binary_crossentropy', metrics=['mae','accuracy']) # 'binary_accuracy','acc','accuracy' are the same!

r= model.fit(x_train,y_train,batch_size=32,epochs=100,validation_data=(x_test,y_test))
print(time() - s)
170/35:
s = time()
model = keras.models.Sequential()

model.add(keras.layers.Input(shape=(D,)))
model.add(keras.layers.Dense(1, activation='sigmoid'))

opt = keras.optimizers.Adam(learning_rate=.01)
model.compile(optimizer = opt, loss = 'binary_crossentropy', metrics=['mae','accuracy']) # 'binary_accuracy','acc','accuracy' are the same!

with tf.device('/gpu:0'):
    r= model.fit(x_train,y_train,batch_size=32,epochs=100,validation_data=(x_test,y_test))
print(time() - s)
170/36:
s = time()
model = keras.models.Sequential()

model.add(keras.layers.Input(shape=(D,)))
model.add(keras.layers.Dense(1, activation='sigmoid'))

opt = keras.optimizers.Adam(learning_rate=.01)
model.compile(optimizer = opt, loss = 'binary_crossentropy', metrics=['mae','accuracy']) # 'binary_accuracy','acc','accuracy' are the same!

with tf.device('/gpu:0'):
    r= model.fit(x_train,y_train,batch_size=32,epochs=100,validation_data=(x_test,y_test))
print(time() - s)
170/37:
s = time()
model = keras.models.Sequential()

model.add(keras.layers.Input(shape=(D,)))
model.add(keras.layers.Dense(1, activation='sigmoid'))

opt = keras.optimizers.Adam(learning_rate=.01)
model.compile(optimizer = opt, loss = 'binary_crossentropy', metrics=['mae','accuracy']) # 'binary_accuracy','acc','accuracy' are the same!

with tf.device('/gpu:0'):
    r= model.fit(x_train,y_train,batch_size=128,epochs=100,validation_data=(x_test,y_test))
print(time() - s)
170/38:
s = time()
model = keras.models.Sequential()

model.add(keras.layers.Input(shape=(D,)))
model.add(keras.layers.Dense(1, activation='sigmoid'))

opt = keras.optimizers.Adam(learning_rate=.01)
model.compile(optimizer = opt, loss = 'binary_crossentropy', metrics=['mae','accuracy']) # 'binary_accuracy','acc','accuracy' are the same!

with tf.device('/gpu:0'):
    r= model.fit(x_train,y_train,batch_size=256,epochs=1000,validation_data=(x_test,y_test))
print(time() - s)
170/39:
s = time()
model = keras.models.Sequential()

model.add(keras.layers.Input(shape=(D,)))
model.add(keras.layers.Dense(1, activation='sigmoid'))

opt = keras.optimizers.Adam(learning_rate=.01)
model.compile(optimizer = opt, loss = 'binary_crossentropy', metrics=['mae','accuracy']) # 'binary_accuracy','acc','accuracy' are the same!

with tf.device('/gpu:0'):
    r= model.fit(x_train,y_train,batch_size=400,epochs=1000,validation_data=(x_test,y_test))
print(time() - s)
170/40:
print("Train score:", model.evaluate(x_train, y_train))
print("Test score:", model.evaluate(x_test, y_test))
170/41:
print(r.history.keys())
plt.plot(r.history['loss'],label='loss')
plt.plot(r.history['val_loss'],label='val_loss')
plt.legend()
170/42:
fig, (ax0,ax1) = plt.subplots(2,1,sharex=True)

ax0.plot(r.history['loss'],label='loss')
ax0.plot(r.history['val_loss'],label='val_loss')
ax0.legend()

ax1.plot(r.history['accuracy'],label='accuracy')
ax1.plot(r.history['val_accuracy'],label='val_accuracy')
ax1.legend()
plt.show()
170/43: plot_confusion_matrix(model,x_test,y_test)
170/44: type(model)
170/45: confusion_matrix(model,x_test,y_test)
170/46: confusion_matrix(model.predict(x_test),y_test)
170/47: model.predict(x_test)
170/48: model.predict(x_test).astype(np.int)
170/49:
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
from sklearn.metrics import confusion_matrix, plot_confusion_matrix, plot_roc_curve
from time import time
import numpy as np
print(tf.__version__)
170/50: model.predict(x_test).astype(np.int)
170/51: confusion_matrix(model.predict(x_test).astype(np.int),y_test)
170/52: model.predict(x_test)
170/53: np.round(model.predict(x_test))
170/54: np.round(model.predict(x_test)).flatten()
170/55:
print(np.round(model.predict(x_test)).flatten())
np.round(model.predict(x_test))
170/56: confusion_matrix(np.round(model.predict(x_test)),y_test)
170/57: print(np.round(model.predict(x_test)).flatten())
170/58: confusion_matrix(np.round(model.predict(x_test)).flatten(),y_test)
170/59: confusion_matrix(np.round(model.predict(x_test)),y_test)
170/60: confusion_matrix(np.round(model.predict(x_test)),y_test)
170/61: confusion_matrix(np.round(model.predict(x_test)),y_test)
170/62: confusion_matrix(np.round(model.predict(x_test)),y_test)
170/63: confusion_matrix(np.round(model.predict(x_test)),y_test)
170/64: confusion_matrix(np.round(model.predict(x_test)),y_test)
170/65: confusion_matrix(np.round(model.predict(x_test)),y_test)
170/66: print(np.round(model.predict(x_test)).flatten())
170/67: print(np.round(model.predict(x_test)).flatten())
170/68:
P = np.round(model.predict(x_test)).flatten()
confusion_matrix(P,y_test)
170/69:
print("Manually calculated accuracy:", np.mean(P == y_test))
print("Evaluate output:", model.evaluate(X_test, y_test))
170/70:
print("Manually calculated accuracy:", np.mean(P == y_test))
print("Evaluate output:", model.evaluate(x_test, y_test))
170/71:
print("Manually calculated accuracy:", np.round(np.mean(P == y_test)),2)
print("Evaluate output:", model.evaluate(x_test, y_test))
170/72:
print("Manually calculated accuracy:", np.round(np.mean(P == y_test)),3)
print("Evaluate output:", model.evaluate(x_test, y_test))
170/73:
print("Manually calculated accuracy:", np.round(np.mean(P == y_test)),1)
print("Evaluate output:", model.evaluate(x_test, y_test))
170/74:
print("Manually calculated accuracy:", np.round(np.mean(P == y_test),2))
print("Evaluate output:", model.evaluate(x_test, y_test))
170/75:
print("Manually calculated accuracy:", np.round(np.mean(P == y_test),2))
print("Evaluate output:", model.evaluate(x_test, y_test))
170/76:
print("Manually calculated accuracy:", np.round(np.mean(P == y_test),2))
print("Evaluate output:", model.evaluate(x_test, y_test))
170/77:
print("Manually calculated accuracy:", np.round(np.mean(P == y_test),2))
print("Evaluate output:", model.evaluate(x_test, y_test))
170/78:
print("Manually calculated accuracy:", np.round(np.mean(P == y_test),2))
print("Evaluate output:", model.evaluate(x_test, y_test))
170/79:
print("Manually calculated accuracy:", np.round(np.mean(P == y_test),3))
print("Evaluate output:", model.evaluate(x_test, y_test))
170/80:
print("Manually calculated accuracy:", np.round(np.mean(P == y_test),2))
print("Evaluate output:", model.evaluate(x_test, y_test))
170/81:
print("Manually calculated accuracy:", np.round(np.mean(P == y_test),2))
print("Evaluate output:", model.evaluate(x_test, y_test))
170/82: model.save('models\\linear_classifier_mine.h5')
170/83:
dir = 'models'
if not os.path.exists(dir):
    os.makedirs(dir)
model.save(f'{dir}\\linear_classifier_mine.h5')
170/84:
dir = 'models'
if not os.path.exists(dir):
    os.makedirs(dir)
model.save(f'{dir}\\linear_classifier_mine.h5')
del model
170/85:
dir = 'models'
if not os.path.exists(dir):
    os.makedirs(dir)
model.save(f'{dir}\\linear_classifier_mine.h5')
del model
model
170/86:

model = keras.models.load_model(f'{dir}\\linear_classifier_mine.h5')
model.layers
170/87:
model = keras.models.load_model(f'{dir}\\linear_classifier_mine.h5')
print(model.layers)
170/88:
model = keras.models.load_model(f'{dir}\\linear_classifier_mine.h5')
print(model.layers)
model
170/89:
model = keras.models.load_model(f'{dir}\\linear_classifier_mine.h5')
print(model.layers)
model.evaluate(x_test,y_test)
180/1:
# Install TensorFlow
# !pip install -q tensorflow-gpu==2.0.0-beta1

try:
  %tensorflow_version 2.x  # Colab only.
except Exception:
  pass

import tensorflow as tf
print(tf.__version__)
180/2:
# Install TensorFlow
# !pip install -q tensorflow-gpu==2.0.0-beta1

try:
  %tensorflow_version 2.x  # Colab only.
except Exception:
  pass

import tensorflow as tf
print(tf.__version__)
180/3:
# Other imports
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
180/4:
# Get the data
!wget https://raw.githubusercontent.com/lazyprogrammer/machine_learning_examples/master/tf2.0/moore.csv
180/5:
# Load in the data
data = pd.read_csv('moore.csv', header=None).values
X = data[:,0].reshape(-1, 1) # make it a 2-D array of size N x D where D = 1
Y = data[:,1]
180/6:
# Plot the data - it is exponential!
plt.scatter(X, Y)
180/7:
# Since we want a linear model, let's take the log
Y = np.log(Y)
plt.scatter(X, Y)
# that's better
180/8:
# Let's also center the X data so the values are not too large
# We could scale it too but then we'd have to reverse the transformation later
X = X - X.mean()
180/9:
# Now create our Tensorflow model
model = tf.keras.models.Sequential([
  tf.keras.layers.Input(shape=(1,)),
  tf.keras.layers.Dense(1)
])

model.compile(optimizer=tf.keras.optimizers.SGD(0.001, 0.9), loss='mse')
# model.compile(optimizer='adam', loss='mse')


# learning rate scheduler
def schedule(epoch, lr):
  if epoch >= 50:
    return 0.0001
  return 0.001
 

scheduler = tf.keras.callbacks.LearningRateScheduler(schedule)


# Train the model
r = model.fit(X, Y, epochs=200, callbacks=[scheduler])
180/10:
# Plot the loss
plt.plot(r.history['loss'], label='loss')
180/11:
# Get the slope of the line
# The slope of the line is related to the doubling rate of transistor count
print(model.layers) # Note: there is only 1 layer, the "Input" layer doesn't count
print(model.layers[0].get_weights())
180/12:
# The slope of the line is:
a = model.layers[0].get_weights()[0][0,0]
180/13: print("Time to double:", np.log(2) / a)
180/14:
# If you know the analytical solution
X = np.array(X).flatten()
Y = np.array(Y)
denominator = X.dot(X) - X.mean() * X.sum()
a = ( X.dot(Y) - Y.mean()*X.sum() ) / denominator
b = ( Y.mean() * X.dot(X) - X.mean() * X.dot(Y) ) / denominator
print(a, b)
print("Time to double:", np.log(2) / a)
180/15:
# Make sure the line fits our data
Yhat = model.predict(X).flatten()
plt.scatter(X, Y)
plt.plot(X, Yhat)
182/1:
# Manual calculation

# Get the weights
w, b = model.layers[0].get_weights()

# Reshape X because we flattened it again earlier
X = X.reshape(-1, 1)

# (N x 1) x (1 x 1) + (1) --> (N x 1)
Yhat2 = (X.dot(w) + b).flatten()

# Don't use == for floating points
np.allclose(Yhat, Yhat2)
182/2:
# Now create our Tensorflow model
model = tf.keras.models.Sequential([
  tf.keras.layers.Input(shape=(1,)),
  tf.keras.layers.Dense(1)
])

model.compile(optimizer=tf.keras.optimizers.SGD(0.001, 0.9), loss='mse')
# model.compile(optimizer='adam', loss='mse')


# learning rate scheduler
def schedule(epoch, lr):
  if epoch >= 50:
    return 0.0001
  return 0.001
 

scheduler = tf.keras.callbacks.LearningRateScheduler(schedule)


# Train the model
r = model.fit(X, Y, epochs=200, callbacks=[scheduler])
182/3:
# Install TensorFlow
# !pip install -q tensorflow-gpu==2.0.0-beta1

# try:
#   %tensorflow_version 2.x  # Colab only.
# except Exception:
#   pass

import tensorflow as tf
print(tf.__version__)
182/4:
# Other imports
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
182/5:
# Get the data
!wget https://raw.githubusercontent.com/lazyprogrammer/machine_learning_examples/master/tf2.0/moore.csv
182/6:
# Load in the data
data = pd.read_csv('moore.csv', header=None).values
X = data[:,0].reshape(-1, 1) # make it a 2-D array of size N x D where D = 1
Y = data[:,1]
182/7:
# Plot the data - it is exponential!
plt.scatter(X, Y)
182/8:
# Since we want a linear model, let's take the log
Y = np.log(Y)
plt.scatter(X, Y)
# that's better
182/9:
# Let's also center the X data so the values are not too large
# We could scale it too but then we'd have to reverse the transformation later
X = X - X.mean()
182/10:
# Now create our Tensorflow model
model = tf.keras.models.Sequential([
  tf.keras.layers.Input(shape=(1,)),
  tf.keras.layers.Dense(1)
])

model.compile(optimizer=tf.keras.optimizers.SGD(0.001, 0.9), loss='mse')
# model.compile(optimizer='adam', loss='mse')


# learning rate scheduler
def schedule(epoch, lr):
  if epoch >= 50:
    return 0.0001
  return 0.001
 

scheduler = tf.keras.callbacks.LearningRateScheduler(schedule)


# Train the model
r = model.fit(X, Y, epochs=200, callbacks=[scheduler])
182/11:
# Now create our Tensorflow model
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Input(shape=(1,)))
model.add(tf.keras.layers.Dense(1))


model.compile(optimizer=tf.keras.optimizers.SGD(0.001, 0.9), loss='mse')
# model.compile(optimizer='adam', loss='mse')


# learning rate scheduler
def schedule(epoch, lr):
  if epoch >= 50:
    return 0.0001
  return 0.001
 

scheduler = tf.keras.callbacks.LearningRateScheduler(schedule)


# Train the model
r = model.fit(X, Y, epochs=200, callbacks=[scheduler])
183/1:
# Plot the loss
plt.plot(r.history['loss'], label='loss')
183/2:
# Install TensorFlow
# !pip install -q tensorflow-gpu==2.0.0-beta1

# try:
#   %tensorflow_version 2.x  # Colab only.
# except Exception:
#   pass

import tensorflow as tf
print(tf.__version__)
183/3:
# Other imports
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
183/4:
# Get the data
!wget https://raw.githubusercontent.com/lazyprogrammer/machine_learning_examples/master/tf2.0/moore.csv
183/5:
# Load in the data
data = pd.read_csv('moore.csv', header=None).values
X = data[:,0].reshape(-1, 1) # make it a 2-D array of size N x D where D = 1
Y = data[:,1]
183/6:
# Plot the data - it is exponential!
plt.scatter(X, Y)
183/7:
# Since we want a linear model, let's take the log
Y = np.log(Y)
plt.scatter(X, Y)
# that's better
183/8:
# Let's also center the X data so the values are not too large
# We could scale it too but then we'd have to reverse the transformation later
X = X - X.mean()
183/9:
# Now create our Tensorflow model
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Input(shape=(1,)))
model.add(tf.keras.layers.Dense(1))


model.compile(optimizer=tf.keras.optimizers.SGD(0.001, 0.9), loss='mse')
# model.compile(optimizer='adam', loss='mse')


# learning rate scheduler
def schedule(epoch, lr):
  if epoch >= 50:
    return 0.0001
  return 0.001
 

scheduler = tf.keras.callbacks.LearningRateScheduler(schedule)


# Train the model
r = model.fit(X, Y, epochs=200, callbacks=[scheduler])
183/10:
# Plot the loss
plt.plot(r.history['loss'], label='loss')
183/11:
# Get the slope of the line
# The slope of the line is related to the doubling rate of transistor count
print(model.layers) # Note: there is only 1 layer, the "Input" layer doesn't count
print(model.layers[0].get_weights())
183/12:
# The slope of the line is:
a = model.layers[0].get_weights()[0][0,0]
183/13: print("Time to double:", np.log(2) / a)
183/14:
# If you know the analytical solution
X = np.array(X).flatten()
Y = np.array(Y)
denominator = X.dot(X) - X.mean() * X.sum()
a = ( X.dot(Y) - Y.mean()*X.sum() ) / denominator
b = ( Y.mean() * X.dot(X) - X.mean() * X.dot(Y) ) / denominator
print(a, b)
print("Time to double:", np.log(2) / a)
183/15:
# Make sure the line fits our data
Yhat = model.predict(X).flatten()
plt.scatter(X, Y)
plt.plot(X, Yhat)
184/1:
# Manual calculation

# Get the weights
w, b = model.layers[0].get_weights()

# Reshape X because we flattened it again earlier
X = X.reshape(-1, 1)

# (N x 1) x (1 x 1) + (1) --> (N x 1)
Yhat2 = (X.dot(w) + b).flatten()

# Don't use == for floating points
np.allclose(Yhat, Yhat2)
184/2:
# Now create our Tensorflow model
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Input(shape=(1,)))
model.add(tf.keras.layers.Dense(1))


model.compile(optimizer=tf.keras.optimizers.SGD(0.001, 0.9), loss='mse')
# model.compile(optimizer='adam', loss='mse')


# learning rate scheduler
def schedule(epoch, lr):
  if epoch >= 50:
    return 0.0001
  return 0.001
 

scheduler = tf.keras.callbacks.LearningRateScheduler(schedule)


# Train the model
with tf.device('/gpu:0'):
  r = model.fit(X, Y, epochs=200, callbacks=[scheduler])
184/3:
# Now create our Tensorflow model
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Input(shape=(1,)))
model.add(tf.keras.layers.Dense(1))


model.compile(optimizer=tf.keras.optimizers.SGD(0.001, 0.9), loss='mse')
# model.compile(optimizer='adam', loss='mse')


# learning rate scheduler
def schedule(epoch, lr):
  if epoch >= 50:
    return 0.0001
  return 0.001
 

scheduler = tf.keras.callbacks.LearningRateScheduler(schedule)


# Train the model
with tf.device('/gpu:0'):
  r = model.fit(X, Y, epochs=200, callbacks=[scheduler])
184/4:
# Now create our Tensorflow model
import tensorflow as tf
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Input(shape=(1,)))
model.add(tf.keras.layers.Dense(1))


model.compile(optimizer=tf.keras.optimizers.SGD(0.001, 0.9), loss='mse')
# model.compile(optimizer='adam', loss='mse')


# learning rate scheduler
def schedule(epoch, lr):
  if epoch >= 50:
    return 0.0001
  return 0.001
 

scheduler = tf.keras.callbacks.LearningRateScheduler(schedule)


# Train the model
with tf.device('/gpu:0'):
  r = model.fit(X, Y, epochs=200, callbacks=[scheduler])
184/5:
# Install TensorFlow
# !pip install -q tensorflow-gpu==2.0.0-beta1

# try:
#   %tensorflow_version 2.x  # Colab only.
# except Exception:
#   pass

import tensorflow as tf
print(tf.__version__)
184/6:
# Other imports
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
184/7:
# Get the data
!wget https://raw.githubusercontent.com/lazyprogrammer/machine_learning_examples/master/tf2.0/moore.csv
184/8:
# Load in the data
data = pd.read_csv('moore.csv', header=None).values
X = data[:,0].reshape(-1, 1) # make it a 2-D array of size N x D where D = 1
Y = data[:,1]
184/9:
# Plot the data - it is exponential!
plt.scatter(X, Y)
184/10:
# Since we want a linear model, let's take the log
Y = np.log(Y)
plt.scatter(X, Y)
# that's better
184/11:
# Let's also center the X data so the values are not too large
# We could scale it too but then we'd have to reverse the transformation later
X = X - X.mean()
184/12:
# Now create our Tensorflow model
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Input(shape=(1,)))
model.add(tf.keras.layers.Dense(1))


model.compile(optimizer=tf.keras.optimizers.SGD(0.001, 0.9), loss='mse')
# model.compile(optimizer='adam', loss='mse')


# learning rate scheduler
def schedule(epoch, lr):
  if epoch >= 50:
    return 0.0001
  return 0.001
 

scheduler = tf.keras.callbacks.LearningRateScheduler(schedule)


# Train the model
with tf.device('/gpu:0'):
  r = model.fit(X, Y, epochs=200, callbacks=[scheduler])
184/13:
# Plot the loss
plt.plot(r.history['loss'], label='loss')
184/14:
# Get the slope of the line
# The slope of the line is related to the doubling rate of transistor count
print(model.layers) # Note: there is only 1 layer, the "Input" layer doesn't count
print(model.layers[0].get_weights())
184/15:
# The slope of the line is:
a = model.layers[0].get_weights()[0][0,0]
184/16: print("Time to double:", np.log(2) / a)
184/17:
# If you know the analytical solution
X = np.array(X).flatten()
Y = np.array(Y)
denominator = X.dot(X) - X.mean() * X.sum()
a = ( X.dot(Y) - Y.mean()*X.sum() ) / denominator
b = ( Y.mean() * X.dot(X) - X.mean() * X.dot(Y) ) / denominator
print(a, b)
print("Time to double:", np.log(2) / a)
184/18:
# Make sure the line fits our data
Yhat = model.predict(X).flatten()
plt.scatter(X, Y)
plt.plot(X, Yhat)
185/1:
# Manual calculation

# Get the weights
w, b = model.layers[0].get_weights()

# Reshape X because we flattened it again earlier
X = X.reshape(-1, 1)

# (N x 1) x (1 x 1) + (1) --> (N x 1)
Yhat2 = (X.dot(w) + b).flatten()

# Don't use == for floating points
np.allclose(Yhat, Yhat2)
186/1:
# Install TensorFlow
# !pip install -q tensorflow-gpu==2.0.0-beta1

# try:
#   %tensorflow_version 2.x  # Colab only.
# except Exception:
#   pass

import tensorflow as tf
print(tf.__version__)
186/2:
# Other imports
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
186/3:
# Get the data
!wget https://raw.githubusercontent.com/lazyprogrammer/machine_learning_examples/master/tf2.0/moore.csv
186/4:
# Load in the data
data = pd.read_csv('moore.csv', header=None).values
X = data[:,0].reshape(-1, 1) # make it a 2-D array of size N x D where D = 1
Y = data[:,1]
186/5:
# Plot the data - it is exponential!
plt.scatter(X, Y)
186/6:
# Since we want a linear model, let's take the log
Y = np.log(Y)
plt.scatter(X, Y)
# that's better
186/7:
# Let's also center the X data so the values are not too large
# We could scale it too but then we'd have to reverse the transformation later
X = X - X.mean()
186/8:
# Now create our Tensorflow model
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Input(shape=(1,)))
model.add(tf.keras.layers.Dense(1))


model.compile(optimizer=tf.keras.optimizers.SGD(0.001, 0.9), loss='mse')
# model.compile(optimizer='adam', loss='mse')


# learning rate scheduler
def schedule(epoch, lr):
  if epoch >= 50:
    return 0.0001
  return 0.001
 

scheduler = tf.keras.callbacks.LearningRateScheduler(schedule)


# Train the model
with tf.device('/gpu:0'):
  r = model.fit(X, Y, epochs=200, callbacks=[scheduler])
186/9:
# Plot the loss
plt.plot(r.history['loss'], label='loss')
186/10:
# Get the slope of the line
# The slope of the line is related to the doubling rate of transistor count
print(model.layers) # Note: there is only 1 layer, the "Input" layer doesn't count
print(model.layers[0].get_weights())
186/11:
# The slope of the line is:
a = model.layers[0].get_weights()[0][0,0]
186/12: print("Time to double:", np.log(2) / a)
186/13:
# If you know the analytical solution
X = np.array(X).flatten()
Y = np.array(Y)
denominator = X.dot(X) - X.mean() * X.sum()
a = ( X.dot(Y) - Y.mean()*X.sum() ) / denominator
b = ( Y.mean() * X.dot(X) - X.mean() * X.dot(Y) ) / denominator
print(a, b)
print("Time to double:", np.log(2) / a)
186/14:
# Make sure the line fits our data
Yhat = model.predict(X).flatten()
plt.scatter(X, Y)
plt.plot(X, Yhat)
190/1:
# Manual calculation

# Get the weights
w, b = model.layers[0].get_weights()

# Reshape X because we flattened it again earlier
X = X.reshape(-1, 1)

# (N x 1) x (1 x 1) + (1) --> (N x 1)
Yhat2 = (X.dot(w) + b).flatten()

# Don't use == for floating points
np.allclose(Yhat, Yhat2)
190/2:
# Install TensorFlow
# !pip install -q tensorflow-gpu==2.0.0-beta1

# try:
#   %tensorflow_version 2.x  # Colab only.
# except Exception:
#   pass

import tensorflow as tf
print(tf.__version__)
190/3:
# Other imports
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
190/4:
# Get the data
!wget https://raw.githubusercontent.com/lazyprogrammer/machine_learning_examples/master/tf2.0/moore.csv
190/5:
# Load in the data
data = pd.read_csv('moore.csv', header=None).values
X = data[:,0].reshape(-1, 1) # make it a 2-D array of size N x D where D = 1
Y = data[:,1]
190/6:
# Plot the data - it is exponential!
plt.scatter(X, Y)
190/7:
# Since we want a linear model, let's take the log
Y = np.log(Y)
plt.scatter(X, Y)
# that's better
190/8:
# Let's also center the X data so the values are not too large
# We could scale it too but then we'd have to reverse the transformation later
X = X - X.mean()
190/9:
# Now create our Tensorflow model
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Input(shape=(1,)))
model.add(tf.keras.layers.Dense(1))


model.compile(optimizer=tf.keras.optimizers.SGD(0.001, 0.9), loss='mse')
# model.compile(optimizer='adam', loss='mse')


# learning rate scheduler
def schedule(epoch, lr):
  if epoch >= 50:
    return 0.0001
  return 0.001
 

scheduler = tf.keras.callbacks.LearningRateScheduler(schedule)


# Train the model
r = model.fit(X, Y, epochs=200, batch_size= 163, callbacks=[scheduler])
190/10:
# Now create our Tensorflow model
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Input(shape=(1,)))
model.add(tf.keras.layers.Dense(1))


model.compile(optimizer=tf.keras.optimizers.SGD(0.001, 0.9), loss='mse')
# model.compile(optimizer='adam', loss='mse')


# learning rate scheduler
def schedule(epoch, lr):
  if epoch >= 50:
    return 0.0001
  return 0.001
 

scheduler = tf.keras.callbacks.LearningRateScheduler(schedule)


# Train the model
with tf.device("/cpu:0"):
  r = model.fit(X, Y, epochs=200, batch_size= 163, callbacks=[scheduler])
191/1:
# Now create our Tensorflow model
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Input(shape=(1,)))
model.add(tf.keras.layers.Dense(1))


model.compile(optimizer=tf.keras.optimizers.SGD(0.001, 0.9), loss='mse')
# model.compile(optimizer='adam', loss='mse')


# learning rate scheduler
def schedule(epoch, lr):
  if epoch >= 50:
    return 0.0001
  return 0.001
 

scheduler = tf.keras.callbacks.LearningRateScheduler(schedule)


# Train the model
with tf.device("/cpu:0"):
  r = model.fit(X, Y, epochs=200, batch_size= 163, callbacks=[scheduler])
191/2:
# Install TensorFlow
# !pip install -q tensorflow-gpu==2.0.0-beta1

# try:
#   %tensorflow_version 2.x  # Colab only.
# except Exception:
#   pass

import tensorflow as tf
print(tf.__version__)
191/3:
# Other imports
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
191/4:
# Get the data
!wget https://raw.githubusercontent.com/lazyprogrammer/machine_learning_examples/master/tf2.0/moore.csv
191/5:
# Load in the data
data = pd.read_csv('moore.csv', header=None).values
X = data[:,0].reshape(-1, 1) # make it a 2-D array of size N x D where D = 1
Y = data[:,1]
191/6:
# Plot the data - it is exponential!
plt.scatter(X, Y)
191/7:
# Since we want a linear model, let's take the log
Y = np.log(Y)
plt.scatter(X, Y)
# that's better
191/8:
# Let's also center the X data so the values are not too large
# We could scale it too but then we'd have to reverse the transformation later
X = X - X.mean()
191/9:
# Now create our Tensorflow model
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Input(shape=(1,)))
model.add(tf.keras.layers.Dense(1))


model.compile(optimizer=tf.keras.optimizers.SGD(0.001, 0.9), loss='mse')
# model.compile(optimizer='adam', loss='mse')


# learning rate scheduler
def schedule(epoch, lr):
  if epoch >= 50:
    return 0.0001
  return 0.001
 

scheduler = tf.keras.callbacks.LearningRateScheduler(schedule)


# Train the model
with tf.device("/cpu:0"):
  r = model.fit(X, Y, epochs=200, batch_size= 163, callbacks=[scheduler])
191/10:
# Plot the loss
plt.plot(r.history['loss'], label='loss')
191/11:
# Get the slope of the line
# The slope of the line is related to the doubling rate of transistor count
print(model.layers) # Note: there is only 1 layer, the "Input" layer doesn't count
print(model.layers[0].get_weights())
191/12:
# The slope of the line is:
a = model.layers[0].get_weights()[0][0,0]
191/13: print("Time to double:", np.log(2) / a)
191/14:
# If you know the analytical solution
X = np.array(X).flatten()
Y = np.array(Y)
denominator = X.dot(X) - X.mean() * X.sum()
a = ( X.dot(Y) - Y.mean()*X.sum() ) / denominator
b = ( Y.mean() * X.dot(X) - X.mean() * X.dot(Y) ) / denominator
print(a, b)
print("Time to double:", np.log(2) / a)
191/15:
# Make sure the line fits our data
Yhat = model.predict(X).flatten()
plt.scatter(X, Y)
plt.plot(X, Yhat)
191/16:
# Manual calculation

# Get the weights
w, b = model.layers[0].get_weights()

# Reshape X because we flattened it again earlier
X = X.reshape(-1, 1)

# (N x 1) x (1 x 1) + (1) --> (N x 1)
Yhat2 = (X.dot(w) + b).flatten()

# Don't use == for floating points
np.allclose(Yhat, Yhat2)
192/1:
# Now create our Tensorflow model
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Input(shape=(1,)))
model.add(tf.keras.layers.Dense(1))


model.compile(optimizer=tf.keras.optimizers.SGD(0.001, 0.9), loss='mse')
# model.compile(optimizer='adam', loss='mse')


# learning rate scheduler
def schedule(epoch, lr):
  if epoch >= 50:
    return 0.0001
  return 0.001
 

scheduler = tf.keras.callbacks.LearningRateScheduler(schedule)


# Train the model
with tf.device("/cpu:0"):
  r = model.fit(X, Y, epochs=200, batch_size= 163, callbacks=[scheduler])
192/2:
# Install TensorFlow
# !pip install -q tensorflow-gpu==2.0.0-beta1

# try:
#   %tensorflow_version 2.x  # Colab only.
# except Exception:
#   pass

import tensorflow as tf
print(tf.__version__)
192/3:
# Other imports
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
192/4:
# Get the data
!wget https://raw.githubusercontent.com/lazyprogrammer/machine_learning_examples/master/tf2.0/moore.csv
192/5:
# Load in the data
data = pd.read_csv('moore.csv', header=None).values
X = data[:,0].reshape(-1, 1) # make it a 2-D array of size N x D where D = 1
Y = data[:,1]
192/6:
# Plot the data - it is exponential!
plt.scatter(X, Y)
192/7:
# Since we want a linear model, let's take the log
Y = np.log(Y)
plt.scatter(X, Y)
# that's better
192/8:
# Let's also center the X data so the values are not too large
# We could scale it too but then we'd have to reverse the transformation later
X = X - X.mean()
192/9:
# Now create our Tensorflow model
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Input(shape=(1,)))
model.add(tf.keras.layers.Dense(1))


model.compile(optimizer=tf.keras.optimizers.SGD(0.001, 0.9), loss='mse')
# model.compile(optimizer='adam', loss='mse')


# learning rate scheduler
def schedule(epoch, lr):
  if epoch >= 50:
    return 0.0001
  return 0.001
 

scheduler = tf.keras.callbacks.LearningRateScheduler(schedule)


# Train the model
with tf.device("/cpu:0"):
  r = model.fit(X, Y, epochs=200, batch_size= 163, callbacks=[scheduler])
192/10:
# Now create our Tensorflow model
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Input(shape=(1,)))
model.add(tf.keras.layers.Dense(1))


model.compile(optimizer=tf.keras.optimizers.SGD(0.001, 0.9), loss='mse')
# model.compile(optimizer='adam', loss='mse')


# learning rate scheduler
def schedule(epoch, lr):
  if epoch >= 50:
    return 0.0001
  return 0.001
 

scheduler = tf.keras.callbacks.LearningRateScheduler(schedule)


# Train the model
with tf.device("/cpu:0"):
  r = model.fit(X, Y, epochs=400, batch_size= 163, callbacks=[scheduler])
192/11:
# Plot the loss
plt.plot(r.history['loss'], label='loss')
192/12:
# Now create our Tensorflow model
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Input(shape=(1,)))
model.add(tf.keras.layers.Dense(1))


model.compile(optimizer=tf.keras.optimizers.SGD(0.001, 0.9), loss='mse')
# model.compile(optimizer='adam', loss='mse')


# learning rate scheduler
def schedule(epoch, lr):
  if epoch >= 50:
    return 0.0001
  return 0.001
 

scheduler = tf.keras.callbacks.LearningRateScheduler(schedule)


# Train the model
with tf.device("/cpu:0"):
  r = model.fit(X, Y, epochs=200, batch_size= 163, callbacks=[scheduler])
192/13:
# Plot the loss
plt.plot(r.history['loss'], label='loss')
192/14:
# Now create our Tensorflow model
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Input(shape=(1,)))
model.add(tf.keras.layers.Dense(1))


model.compile(optimizer=tf.keras.optimizers.SGD(0.001, 0.9), loss='mse')
# model.compile(optimizer='adam', loss='mse')


# learning rate scheduler
def schedule(epoch, lr):
  if epoch >= 50:
    return 0.0001
  return 0.001
 

scheduler = tf.keras.callbacks.LearningRateScheduler(schedule)


# Train the model

r = model.fit(X, Y, epochs=200, batch_size= 163, callbacks=[scheduler])
193/1:
# Plot the loss
plt.plot(r.history['loss'], label='loss')
193/2:
# Now create our Tensorflow model
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Input(shape=(1,)))
model.add(tf.keras.layers.Dense(1))


model.compile(optimizer=tf.keras.optimizers.SGD(0.001, 0.9), loss='mse')
# model.compile(optimizer='adam', loss='mse')


# learning rate scheduler
def schedule(epoch, lr):
  if epoch >= 50:
    return 0.0001
  return 0.001
 

scheduler = tf.keras.callbacks.LearningRateScheduler(schedule)


# Train the model
with tf.device("/gpu:0"):
  r = model.fit(X, Y, epochs=200, batch_size= 163, callbacks=[scheduler])
193/3:
# Now create our Tensorflow model
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Input(shape=(1,)))
model.add(tf.keras.layers.Dense(1))


model.compile(optimizer=tf.keras.optimizers.SGD(0.001, 0.9), loss='mse')
# model.compile(optimizer='adam', loss='mse')


# learning rate scheduler
def schedule(epoch, lr):
  if epoch >= 50:
    return 0.0001
  return 0.001
 

scheduler = tf.keras.callbacks.LearningRateScheduler(schedule)


# Train the model
with tf.device("/gpu:0"):
  r = model.fit(X, Y, epochs=200, batch_size= 163, callbacks=[scheduler])
193/4:
# Now create our Tensorflow model
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Input(shape=(1,)))
model.add(tf.keras.layers.Dense(1))


model.compile(optimizer=tf.keras.optimizers.SGD(0.001, 0.9), loss='mse')
# model.compile(optimizer='adam', loss='mse')


# learning rate scheduler
def schedule(epoch, lr):
  if epoch >= 50:
    return 0.0001
  return 0.001
 

scheduler = tf.keras.callbacks.LearningRateScheduler(schedule)


# Train the model
with tf.device("/gpu:0"):
  r = model.fit(X, Y, epochs=200, batch_size= 163, callbacks=[scheduler])
193/5:
# Now create our Tensorflow model
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Input(shape=(1,)))
model.add(tf.keras.layers.Dense(1))


model.compile(optimizer=tf.keras.optimizers.SGD(0.001, 0.9), loss='mse')
# model.compile(optimizer='adam', loss='mse')


# learning rate scheduler
def schedule(epoch, lr):
  if epoch >= 50:
    return 0.0001
  return 0.001
 

scheduler = tf.keras.callbacks.LearningRateScheduler(schedule)


# Train the model
with tf.device("/gpu:0"):
  r = model.fit(X, Y, epochs=200, batch_size= 163, callbacks=[scheduler])
193/6:
# Now create our Tensorflow model
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Input(shape=(1,)))
model.add(tf.keras.layers.Dense(1))


model.compile(optimizer=tf.keras.optimizers.SGD(0.001, 0.9), loss='mse')
# model.compile(optimizer='adam', loss='mse')


# learning rate scheduler
def schedule(epoch, lr):
  if epoch >= 50:
    return 0.0001
  return 0.001
 

scheduler = tf.keras.callbacks.LearningRateScheduler(schedule)


# Train the model
with tf.device("/gpu:0"):
  r = model.fit(X, Y, epochs=200, batch_size= 163, callbacks=[scheduler])
193/7:
# Now create our Tensorflow model
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Input(shape=(1,)))
model.add(tf.keras.layers.Dense(1))


model.compile(optimizer=tf.keras.optimizers.SGD(0.001, 0.9), loss='mse')
# model.compile(optimizer='adam', loss='mse')


# learning rate scheduler
def schedule(epoch, lr):
  if epoch >= 50:
    return 0.0001
  return 0.001
 

scheduler = tf.keras.callbacks.LearningRateScheduler(schedule)


# Train the model
with tf.device("/gpu:0"):
  r = model.fit(X, Y, epochs=200, batch_size= 163, callbacks=[scheduler])
193/8:
# Now create our Tensorflow model
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Input(shape=(1,)))
model.add(tf.keras.layers.Dense(1))


model.compile(optimizer=tf.keras.optimizers.SGD(0.001, 0.9), loss='mse')
# model.compile(optimizer='adam', loss='mse')


# learning rate scheduler
def schedule(epoch, lr):
  if epoch >= 50:
    return 0.0001
  return 0.001
 

scheduler = tf.keras.callbacks.LearningRateScheduler(schedule)


# Train the model
with tf.device("/gpu:0"):
  r = model.fit(X, Y, epochs=200, batch_size= 163, callbacks=[scheduler])
193/9:
# Now create our Tensorflow model
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Input(shape=(1,)))
model.add(tf.keras.layers.Dense(1))


model.compile(optimizer=tf.keras.optimizers.SGD(0.001, 0.9), loss='mse')
# model.compile(optimizer='adam', loss='mse')


# learning rate scheduler
def schedule(epoch, lr):
  if epoch >= 50:
    return 0.0001
  return 0.001
 

scheduler = tf.keras.callbacks.LearningRateScheduler(schedule)


# Train the model
with tf.device("/gpu:0"):
  r = model.fit(X, Y, epochs=200, batch_size= 163, callbacks=[scheduler])
193/10:
# Now create our Tensorflow model
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Input(shape=(1,)))
model.add(tf.keras.layers.Dense(1))


model.compile(optimizer=tf.keras.optimizers.SGD(0.001, 0.9), loss='mse')
# model.compile(optimizer='adam', loss='mse')


# learning rate scheduler
def schedule(epoch, lr):
  if epoch >= 50:
    return 0.0001
  return 0.001
 

scheduler = tf.keras.callbacks.LearningRateScheduler(schedule)


# Train the model
with tf.device("/gpu:0"):
  r = model.fit(X, Y, epochs=200, batch_size= 163, callbacks=[scheduler])
193/11:
# Now create our Tensorflow model
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Input(shape=(1,)))
model.add(tf.keras.layers.Dense(1))


model.compile(optimizer=tf.keras.optimizers.SGD(0.001, 0.9), loss='mse')
# model.compile(optimizer='adam', loss='mse')


# learning rate scheduler
def schedule(epoch, lr):
  if epoch >= 50:
    return 0.0001
  return 0.001
 

scheduler = tf.keras.callbacks.LearningRateScheduler(schedule)


# Train the model
with tf.device("/gpu:0"):
  r = model.fit(X, Y, epochs=200, batch_size= 163, callbacks=[scheduler])
193/12:
# Now create our Tensorflow model
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Input(shape=(1,)))
model.add(tf.keras.layers.Dense(1))


model.compile(optimizer=tf.keras.optimizers.SGD(0.001, 0.9), loss='mse')
# model.compile(optimizer='adam', loss='mse')


# learning rate scheduler
def schedule(epoch, lr):
  if epoch >= 50:
    return 0.0001
  return 0.001
 

scheduler = tf.keras.callbacks.LearningRateScheduler(schedule)


# Train the model
with tf.device("/gpu:0"):
  r = model.fit(X, Y, epochs=200, batch_size= 163, callbacks=[scheduler])
193/13:
# Now create our Tensorflow model
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Input(shape=(1,)))
model.add(tf.keras.layers.Dense(1))


model.compile(optimizer=tf.keras.optimizers.SGD(0.001, 0.9), loss='mse')
# model.compile(optimizer='adam', loss='mse')


# learning rate scheduler
def schedule(epoch, lr):
  if epoch >= 50:
    return 0.0001
  return 0.001
 

scheduler = tf.keras.callbacks.LearningRateScheduler(schedule)


# Train the model
with tf.device("/gpu:0"):
  r = model.fit(X, Y, epochs=200, batch_size= 163, callbacks=[scheduler])
193/14:
# Now create our Tensorflow model
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Input(shape=(1,)))
model.add(tf.keras.layers.Dense(1))


model.compile(optimizer=tf.keras.optimizers.SGD(0.001, 0.9), loss='mse')
# model.compile(optimizer='adam', loss='mse')


# learning rate scheduler
def schedule(epoch, lr):
  if epoch >= 50:
    return 0.0001
  return 0.001
 

scheduler = tf.keras.callbacks.LearningRateScheduler(schedule)


# Train the model
with tf.device("/gpu:0"):
  r = model.fit(X, Y, epochs=200, batch_size= 163, callbacks=[scheduler])
193/15:
# Now create our Tensorflow model
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Input(shape=(1,)))
model.add(tf.keras.layers.Dense(1))


model.compile(optimizer=tf.keras.optimizers.SGD(0.001, 0.9), loss='mse')
# model.compile(optimizer='adam', loss='mse')


# learning rate scheduler
def schedule(epoch, lr):
  if epoch >= 50:
    return 0.0001
  return 0.001
 

scheduler = tf.keras.callbacks.LearningRateScheduler(schedule)


# Train the model
with tf.device("/gpu:0"):
  r = model.fit(X, Y, epochs=200, batch_size= 163, callbacks=[scheduler])
193/16:
# Now create our Tensorflow model
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Input(shape=(1,)))
model.add(tf.keras.layers.Dense(1))


model.compile(optimizer=tf.keras.optimizers.SGD(0.001, 0.9), loss='mse')
# model.compile(optimizer='adam', loss='mse')


# learning rate scheduler
def schedule(epoch, lr):
  if epoch >= 50:
    return 0.0001
  return 0.001
 

scheduler = tf.keras.callbacks.LearningRateScheduler(schedule)


# Train the model
with tf.device("/gpu:0"):
  r = model.fit(X, Y, epochs=200, batch_size= 163, callbacks=[scheduler])
193/17:
# Now create our Tensorflow model
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Input(shape=(1,)))
model.add(tf.keras.layers.Dense(1))


model.compile(optimizer=tf.keras.optimizers.SGD(0.001, 0.9), loss='mse')
# model.compile(optimizer='adam', loss='mse')


# learning rate scheduler
def schedule(epoch, lr):
  if epoch >= 50:
    return 0.0001
  return 0.001
 

scheduler = tf.keras.callbacks.LearningRateScheduler(schedule)


# Train the model
with tf.device("/gpu:0"):
  r = model.fit(X, Y, epochs=200, batch_size= 163, callbacks=[scheduler])
193/18:
# Install TensorFlow
# !pip install -q tensorflow-gpu==2.0.0-beta1

# try:
#   %tensorflow_version 2.x  # Colab only.
# except Exception:
#   pass

import tensorflow as tf
print(tf.__version__)
193/19:
# Install TensorFlow
# !pip install -q tensorflow-gpu==2.0.0-beta1

# try:
#   %tensorflow_version 2.x  # Colab only.
# except Exception:
#   pass

import tensorflow as tf
print(tf.__version__)
193/20:
# Install TensorFlow
# !pip install -q tensorflow-gpu==2.0.0-beta1

# try:
#   %tensorflow_version 2.x  # Colab only.
# except Exception:
#   pass

import tensorflow as tf
print(tf.__version__)
193/21:
# Install TensorFlow
# !pip install -q tensorflow-gpu==2.0.0-beta1

# try:
#   %tensorflow_version 2.x  # Colab only.
# except Exception:
#   pass

import tensorflow as tf
print(tf.__version__)
193/22:
# Install TensorFlow
# !pip install -q tensorflow-gpu==2.0.0-beta1

# try:
#   %tensorflow_version 2.x  # Colab only.
# except Exception:
#   pass

import tensorflow as tf
print(tf.__version__)
193/23:
# Other imports
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
193/24:
# Get the data
!wget https://raw.githubusercontent.com/lazyprogrammer/machine_learning_examples/master/tf2.0/moore.csv
193/25:
# Load in the data
data = pd.read_csv('moore.csv', header=None).values
X = data[:,0].reshape(-1, 1) # make it a 2-D array of size N x D where D = 1
Y = data[:,1]
193/26:
# Plot the data - it is exponential!
plt.scatter(X, Y)
193/27:
# Since we want a linear model, let's take the log
Y = np.log(Y)
plt.scatter(X, Y)
# that's better
193/28:
# Let's also center the X data so the values are not too large
# We could scale it too but then we'd have to reverse the transformation later
X = X - X.mean()
193/29:
# Now create our Tensorflow model
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Input(shape=(1,)))
model.add(tf.keras.layers.Dense(1))


model.compile(optimizer=tf.keras.optimizers.SGD(0.001, 0.9), loss='mse')
# model.compile(optimizer='adam', loss='mse')


# learning rate scheduler
def schedule(epoch, lr):
  if epoch >= 50:
    return 0.0001
  return 0.001
 

scheduler = tf.keras.callbacks.LearningRateScheduler(schedule)


# Train the model
with tf.device("/gpu:0"):
  r = model.fit(X, Y, epochs=200, batch_size= 163, callbacks=[scheduler])
193/30:
# Now create our Tensorflow model
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Input(shape=(1,)))
model.add(tf.keras.layers.Dense(1))


model.compile(optimizer=tf.keras.optimizers.SGD(0.001, 0.9), loss='mse')
# model.compile(optimizer='adam', loss='mse')


# learning rate scheduler
def schedule(epoch, lr):
  if epoch >= 50:
    return 0.0001
  return 0.001
 

scheduler = tf.keras.callbacks.LearningRateScheduler(schedule)


# Train the model
with tf.device("/gpu:0"):
  r = model.fit(X, Y, epochs=200, batch_size= 32, callbacks=[scheduler])
194/1:
# Now create our Tensorflow model
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Input(shape=(1,)))
model.add(tf.keras.layers.Dense(1))


model.compile(optimizer=tf.keras.optimizers.SGD(0.001, 0.9), loss='mse')
# model.compile(optimizer='adam', loss='mse')


# learning rate scheduler
def schedule(epoch, lr):
  if epoch >= 50:
    return 0.0001
  return 0.001
 

scheduler = tf.keras.callbacks.LearningRateScheduler(schedule)


# Train the model
with tf.device("/gpu:0"):
  r = model.fit(X, Y, epochs=200, batch_size= 32, callbacks=[scheduler])
194/2:
# Now create our Tensorflow model
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Input(shape=(1,)))
model.add(tf.keras.layers.Dense(1))


model.compile(optimizer=tf.keras.optimizers.SGD(0.001, 0.9), loss='mse')
# model.compile(optimizer='adam', loss='mse')


# learning rate scheduler
def schedule(epoch, lr):
  if epoch >= 50:
    return 0.0001
  return 0.001
 

scheduler = tf.keras.callbacks.LearningRateScheduler(schedule)


# Train the model
with tf.device("/gpu:0"):
  r = model.fit(X, Y, epochs=200, batch_size= 32, callbacks=[scheduler])
194/3:
# Now create our Tensorflow model
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Input(shape=(1,)))
model.add(tf.keras.layers.Dense(1))


model.compile(optimizer=tf.keras.optimizers.SGD(0.001, 0.9), loss='mse')
# model.compile(optimizer='adam', loss='mse')


# learning rate scheduler
def schedule(epoch, lr):
  if epoch >= 50:
    return 0.0001
  return 0.001
 

scheduler = tf.keras.callbacks.LearningRateScheduler(schedule)


# Train the model
with tf.device("/gpu:0"):
  r = model.fit(X, Y, epochs=200, batch_size= 32, callbacks=[scheduler])
194/4:
# Now create our Tensorflow model
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Input(shape=(1,)))
model.add(tf.keras.layers.Dense(1))


model.compile(optimizer=tf.keras.optimizers.SGD(0.001, 0.9), loss='mse')
# model.compile(optimizer='adam', loss='mse')


# learning rate scheduler
def schedule(epoch, lr):
  if epoch >= 50:
    return 0.0001
  return 0.001
 

scheduler = tf.keras.callbacks.LearningRateScheduler(schedule)


# Train the model
with tf.device("/gpu:0"):
  r = model.fit(X, Y, epochs=200, batch_size= 32, callbacks=[scheduler])
194/5:
# Now create our Tensorflow model
import tensorflow as tf
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Input(shape=(1,)))
model.add(tf.keras.layers.Dense(1))


model.compile(optimizer=tf.keras.optimizers.SGD(0.001, 0.9), loss='mse')
# model.compile(optimizer='adam', loss='mse')


# learning rate scheduler
def schedule(epoch, lr):
  if epoch >= 50:
    return 0.0001
  return 0.001
 

scheduler = tf.keras.callbacks.LearningRateScheduler(schedule)


# Train the model
with tf.device("/gpu:0"):
  r = model.fit(X, Y, epochs=200, batch_size= 32, callbacks=[scheduler])
194/6:
# Install TensorFlow
# !pip install -q tensorflow-gpu==2.0.0-beta1

# try:
#   %tensorflow_version 2.x  # Colab only.
# except Exception:
#   pass

import tensorflow as tf
print(tf.__version__)
194/7:
# Other imports
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
194/8:
# Get the data
!wget https://raw.githubusercontent.com/lazyprogrammer/machine_learning_examples/master/tf2.0/moore.csv
194/9:
# Load in the data
data = pd.read_csv('moore.csv', header=None).values
X = data[:,0].reshape(-1, 1) # make it a 2-D array of size N x D where D = 1
Y = data[:,1]
194/10:
# Plot the data - it is exponential!
plt.scatter(X, Y)
194/11:
# Since we want a linear model, let's take the log
Y = np.log(Y)
plt.scatter(X, Y)
# that's better
194/12:
# Let's also center the X data so the values are not too large
# We could scale it too but then we'd have to reverse the transformation later
X = X - X.mean()
194/13:
# Now create our Tensorflow model
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Input(shape=(1,)))
model.add(tf.keras.layers.Dense(1))


model.compile(optimizer=tf.keras.optimizers.SGD(0.001, 0.9), loss='mse')
# model.compile(optimizer='adam', loss='mse')


# learning rate scheduler
def schedule(epoch, lr):
  if epoch >= 50:
    return 0.0001
  return 0.001
 

scheduler = tf.keras.callbacks.LearningRateScheduler(schedule)


# Train the model
with tf.device("/gpu:0"):
  r = model.fit(X, Y, epochs=200, batch_size= 32, callbacks=[scheduler])
194/14:
# Plot the loss
plt.plot(r.history['loss'], label='loss')
194/15:
# Get the slope of the line
# The slope of the line is related to the doubling rate of transistor count
print(model.layers) # Note: there is only 1 layer, the "Input" layer doesn't count
print(model.layers[0].get_weights())
194/16:
# The slope of the line is:
a = model.layers[0].get_weights()[0][0,0]
194/17: print("Time to double:", np.log(2) / a)
194/18:
# If you know the analytical solution
X = np.array(X).flatten()
Y = np.array(Y)
denominator = X.dot(X) - X.mean() * X.sum()
a = ( X.dot(Y) - Y.mean()*X.sum() ) / denominator
b = ( Y.mean() * X.dot(X) - X.mean() * X.dot(Y) ) / denominator
print(a, b)
print("Time to double:", np.log(2) / a)
194/19:
# Make sure the line fits our data
Yhat = model.predict(X).flatten()
plt.scatter(X, Y)
plt.plot(X, Yhat)
195/1:
# Manual calculation

# Get the weights
w, b = model.layers[0].get_weights()

# Reshape X because we flattened it again earlier
X = X.reshape(-1, 1)

# (N x 1) x (1 x 1) + (1) --> (N x 1)
Yhat2 = (X.dot(w) + b).flatten()

# Don't use == for floating points
np.allclose(Yhat, Yhat2)
195/2:
# Install TensorFlow
# !pip install -q tensorflow-gpu==2.0.0-beta1

# try:
#   %tensorflow_version 2.x  # Colab only.
# except Exception:
#   pass

import tensorflow as tf
print(tf.__version__)
195/3:
# Install TensorFlow
# !pip install -q tensorflow-gpu==2.0.0-beta1

# try:
#   %tensorflow_version 2.x  # Colab only.
# except Exception:
#   pass

import tensorflow as tf
print(tf.__version__)
195/4:
# Install TensorFlow
# !pip install -q tensorflow-gpu==2.0.0-beta1

# try:
#   %tensorflow_version 2.x  # Colab only.
# except Exception:
#   pass

import tensorflow as tf
print(tf.__version__)
195/5:
# Install TensorFlow
# !pip install -q tensorflow-gpu==2.0.0-beta1

# try:
#   %tensorflow_version 2.x  # Colab only.
# except Exception:
#   pass

import tensorflow as tf
print(tf.__version__)
195/6:
# Other imports
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
195/7:
# Get the data
!wget https://raw.githubusercontent.com/lazyprogrammer/machine_learning_examples/master/tf2.0/moore.csv
195/8:
# Load in the data
data = pd.read_csv('moore.csv', header=None).values
X = data[:,0].reshape(-1, 1) # make it a 2-D array of size N x D where D = 1
Y = data[:,1]
195/9:
# Plot the data - it is exponential!
plt.scatter(X, Y)
195/10: type(X)
195/11: X.ctypes
195/12: X
195/13:
# Let's also center the X data so the values are not too large
# We could scale it too but then we'd have to reverse the transformation later
X = X - X.mean()
195/14: X
188/1:
# Install TensorFlow
# !pip install -q tensorflow-gpu==2.0.0-beta1

try:
  %tensorflow_version 2.x  # Colab only.
except Exception:
  pass

import tensorflow as tf
print(tf.__version__)
188/2:
# Install TensorFlow
# !pip install -q tensorflow-gpu==2.0.0-beta1

try:
  %tensorflow_version 2.x  # Colab only.
except Exception:
  pass

import tensorflow as tf
print(tf.__version__)
188/3:
# Load in the data
mnist = tf.keras.datasets.mnist

(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0
print("x_train.shape:", x_train.shape)
188/4:
# Build the model
model = tf.keras.models.Sequential([
  tf.keras.layers.Flatten(input_shape=(28, 28)),
  tf.keras.layers.Dense(128, activation='relu'),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(10, activation='softmax')
])
188/5:
# Build the model
model = tf.keras.models.Sequential([
  tf.keras.layers.Flatten(input_shape=(28, 28)),
  tf.keras.layers.Dense(128, activation='relu'),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(10, activation='softmax')
])
model.layers
188/6:
# Compile the model
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
188/7:
# Compile the model
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
188/8:
# Train the model
r = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=10)
196/1:
# Train the model
with tf.device('/gpu:0'):
    r = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=10)
196/2:
# Install TensorFlow
# !pip install -q tensorflow-gpu==2.0.0-beta1

try:
  %tensorflow_version 2.x  # Colab only.
except Exception:
  pass

import tensorflow as tf
print(tf.__version__)
196/3:
# Load in the data
mnist = tf.keras.datasets.mnist

(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0
print("x_train.shape:", x_train.shape)
196/4:
# Build the model
model = tf.keras.models.Sequential([
  tf.keras.layers.Flatten(input_shape=(28, 28)),
  tf.keras.layers.Dense(128, activation='relu'),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(10, activation='softmax')
])
model.layers
196/5:
# Compile the model
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
196/6:
# Train the model
with tf.device('/gpu:0'):
    r = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=10)
196/7:
# Plot loss per iteration
import matplotlib.pyplot as plt
plt.plot(r.history['loss'], label='loss')
plt.plot(r.history['val_loss'], label='val_loss')
plt.legend()
196/8:
# Plot accuracy per iteration
plt.plot(r.history['accuracy'], label='acc')
plt.plot(r.history['val_accuracy'], label='val_acc')
plt.legend()
196/9:
# Evaluate the model
print(model.evaluate(x_test, y_test))
197/1:
# Plot confusion matrix
from sklearn.metrics import confusion_matrix
import numpy as np
import itertools

def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
  """
  This function prints and plots the confusion matrix.
  Normalization can be applied by setting `normalize=True`.
  """
  if normalize:
      cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
      print("Normalized confusion matrix")
  else:
      print('Confusion matrix, without normalization')

  print(cm)

  plt.imshow(cm, interpolation='nearest', cmap=cmap)
  plt.title(title)
  plt.colorbar()
  tick_marks = np.arange(len(classes))
  plt.xticks(tick_marks, classes, rotation=45)
  plt.yticks(tick_marks, classes)

  fmt = '.2f' if normalize else 'd'
  thresh = cm.max() / 2.
  for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
      plt.text(j, i, format(cm[i, j], fmt),
               horizontalalignment="center",
               color="white" if cm[i, j] > thresh else "black")

  plt.tight_layout()
  plt.ylabel('True label')
  plt.xlabel('Predicted label')
  plt.show()


p_test = model.predict(x_test).argmax(axis=1)
cm = confusion_matrix(y_test, p_test)
plot_confusion_matrix(cm, list(range(10)))

# Do these results make sense?
# It's easy to confuse 9 <--> 4, 9 <--> 7, 2 <--> 7, etc.
197/2:
# Show some misclassified examples
misclassified_idx = np.where(p_test != y_test)[0]
i = np.random.choice(misclassified_idx)
plt.imshow(x_test[i], cmap='gray')
plt.title("True label: %s Predicted: %s" % (y_test[i], p_test[i]));
170/90:
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
from sklearn.metrics import confusion_matrix, plot_confusion_matrix, plot_roc_curve
from time import time
import numpy as np
print(tf.__version__)
170/91:
# import tensorflow as tf
# with tf.device('/gpu:0'):
#     a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')
#     b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2,3], name='b')
#     c = a+b
#     print(c)
170/92: df = load_breast_cancer()
170/93: df.keys()
170/94: df.feature_names
170/95: df.target_names
170/96: df.data.shape
170/97: df.target.shape
170/98:
x_train, x_test, y_train, y_test = train_test_split(df.data,df.target,test_size=0.33)
N,D = x_train.shape
170/99:
scaler = StandardScaler()
x_train = scaler.fit_transform(x_train)
x_test = scaler.transform(x_test)
170/100: print(D)
170/101:
s = time()
model = keras.models.Sequential()

model.add(keras.layers.Input(shape=(D,)))
model.add(keras.layers.Dense(1, activation='sigmoid'))

opt = keras.optimizers.Adam(learning_rate=.01)
model.compile(optimizer = opt, loss = 'binary_crossentropy', metrics=['mae','accuracy']) # 'binary_accuracy','acc','accuracy' are the same!

with tf.device('/gpu:0'):
    r= model.fit(x_train,y_train,batch_size=400,epochs=1000,validation_data=(x_test,y_test))
print(time() - s)
170/102:
print("Train score:", model.evaluate(x_train, y_train))
print("Test score:", model.evaluate(x_test, y_test))
170/103:
print(r.history.keys())
plt.plot(r.history['loss'],label='loss')
plt.plot(r.history['val_loss'],label='val_loss')
plt.legend()
170/104:
fig, (ax0,ax1) = plt.subplots(2,1,sharex=True)

ax0.plot(r.history['loss'],label='loss')
ax0.plot(r.history['val_loss'],label='val_loss')
ax0.legend()

ax1.plot(r.history['accuracy'],label='accuracy')
ax1.plot(r.history['val_accuracy'],label='val_accuracy')
ax1.legend()
plt.show()
170/105:
P = np.round(model.predict(x_test)).flatten()
confusion_matrix(P,y_test)
170/106: print(np.round(model.predict(x_test)).flatten())
170/107:
print("Manually calculated accuracy:", np.round(np.mean(P == y_test),2))
print("Evaluate output:", model.evaluate(x_test, y_test))
170/108:
dir = 'models'
if not os.path.exists(dir):
    os.makedirs(dir)
model.save(f'{dir}\\linear_classifier_mine.h5')
del model
170/109:
model = keras.models.load_model(f'{dir}\\linear_classifier_mine.h5')
print(model.layers)
model.evaluate(x_test,y_test)
170/110: type(x_test)
170/111: type(x_test.base)
170/112: x_test.base
170/113: x_test
170/114: x_test.dtype
170/115: print(x_test.dtype, y_test.dtype)
197/3:
# Install TensorFlow
# !pip install -q tensorflow-gpu==2.0.0-beta1

# try:
#   %tensorflow_version 2.x  # Colab only.
# except Exception:
#   pass

import tensorflow as tf
print(tf.__version__)
170/116:
s = time()
model = keras.models.Sequential()

model.add(keras.layers.Input(shape=(D,)))
model.add(keras.layers.Dense(1, activation='sigmoid'))

opt = keras.optimizers.Adam(learning_rate=.01)
model.compile(optimizer = opt, loss = 'binary_crossentropy', metrics=['mae','accuracy']) # 'binary_accuracy','acc','accuracy' are the same!

# with tf.device('/gpu:0'):
r= model.fit(x_train,y_train,batch_size=400,epochs=200,validation_data=(x_test,y_test))
print(time() - s)
197/4:
# Install TensorFlow
# !pip install -q tensorflow-gpu==2.0.0-beta1

# try:
#   %tensorflow_version 2.x  # Colab only.
# except Exception:
#   pass

import tensorflow as tf
print(tf.__version__)
197/5:
# Install TensorFlow
# !pip install -q tensorflow-gpu==2.0.0-beta1

# try:
#   %tensorflow_version 2.x  # Colab only.
# except Exception:
#   pass

import tensorflow as tf
print(tf.__version__)
197/6:
# Load in the data
mnist = tf.keras.datasets.mnist

(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0
print("x_train.shape:", x_train.shape)
197/7:
# Build the model
model = tf.keras.models.Sequential([
  tf.keras.layers.Flatten(input_shape=(28, 28)),
  tf.keras.layers.Dense(128, activation='relu'),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(10, activation='softmax')
])
model.layers
197/8:
# Compile the model
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
197/9: print(x_train.dtype, y_train.dtype)
197/10:
# Train the model
# with tf.device('/gpu:0'):
r = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=10)
197/11:
# Compile the model
opt = keras.optimizers.Adam(learning_rate=.01)
model.compile(optimizer=opt,
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
197/12:
# Install TensorFlow
# !pip install -q tensorflow-gpu==2.0.0-beta1

# try:
#   %tensorflow_version 2.x  # Colab only.
# except Exception:
#   pass

import tensorflow as tf
print(tf.__version__)
tf.keras.models
197/13:
# Install TensorFlow
# !pip install -q tensorflow-gpu==2.0.0-beta1

# try:
#   %tensorflow_version 2.x  # Colab only.
# except Exception:
#   pass

import tensorflow as tf
print(tf.__version__)
197/14:
# Install TensorFlow
# !pip install -q tensorflow-gpu==2.0.0-beta1

# try:
#   %tensorflow_version 2.x  # Colab only.
# except Exception:
#   pass

import tensorflow as tf
print(tf.__version__)
197/15:
# Load in the data
mnist = tf.keras.datasets.mnist

(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0
print("x_train.shape:", x_train.shape)
198/1:
# Build the model
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Flatten(input_shape=(28, 28)))
model.add(tf.keras.layers.Dense(128, activation='relu'))
model.add(tf.keras.layers.Dropout(0.2))
model.add(tf.keras.layers.Dense(10, activation='softmax'))

model.layers
198/2:
# Install TensorFlow
# !pip install -q tensorflow-gpu==2.0.0-beta1

# try:
#   %tensorflow_version 2.x  # Colab only.
# except Exception:
#   pass

import tensorflow as tf
print(tf.__version__)
198/3:
# Load in the data
mnist = tf.keras.datasets.mnist

(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0
print("x_train.shape:", x_train.shape)
198/4:
# Build the model
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Flatten(input_shape=(28, 28)))
model.add(tf.keras.layers.Dense(128, activation='relu'))
model.add(tf.keras.layers.Dropout(0.2))
model.add(tf.keras.layers.Dense(10, activation='softmax'))

model.layers
198/5:
# Compile the model
opt = keras.optimizers.Adam(learning_rate=.01)
model.compile(optimizer=opt,
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
198/6:
print(x_train.dtype, y_train.dtype)
print(x_train,y_tr)
198/7:
# Train the model
# with tf.device('/gpu:0'):
r = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=10)
198/8:
# Plot loss per iteration
import matplotlib.pyplot as plt
plt.plot(r.history['loss'], label='loss')
plt.plot(r.history['val_loss'], label='val_loss')
plt.legend()
198/9:
# Plot accuracy per iteration
plt.plot(r.history['accuracy'], label='acc')
plt.plot(r.history['val_accuracy'], label='val_acc')
plt.legend()
198/10:
# Evaluate the model
print(model.evaluate(x_test, y_test))
198/11:
# Plot confusion matrix
from sklearn.metrics import confusion_matrix
import numpy as np
import itertools

def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
  """
  This function prints and plots the confusion matrix.
  Normalization can be applied by setting `normalize=True`.
  """
  if normalize:
      cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
      print("Normalized confusion matrix")
  else:
      print('Confusion matrix, without normalization')

  print(cm)

  plt.imshow(cm, interpolation='nearest', cmap=cmap)
  plt.title(title)
  plt.colorbar()
  tick_marks = np.arange(len(classes))
  plt.xticks(tick_marks, classes, rotation=45)
  plt.yticks(tick_marks, classes)

  fmt = '.2f' if normalize else 'd'
  thresh = cm.max() / 2.
  for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
      plt.text(j, i, format(cm[i, j], fmt),
               horizontalalignment="center",
               color="white" if cm[i, j] > thresh else "black")

  plt.tight_layout()
  plt.ylabel('True label')
  plt.xlabel('Predicted label')
  plt.show()


p_test = model.predict(x_test).argmax(axis=1)
cm = confusion_matrix(y_test, p_test)
plot_confusion_matrix(cm, list(range(10)))

# Do these results make sense?
# It's easy to confuse 9 <--> 4, 9 <--> 7, 2 <--> 7, etc.
198/12:
# Show some misclassified examples
misclassified_idx = np.where(p_test != y_test)[0]
i = np.random.choice(misclassified_idx)
plt.imshow(x_test[i], cmap='gray')
plt.title("True label: %s Predicted: %s" % (y_test[i], p_test[i]));
199/1:
print(x_train.dtype, y_train.dtype)
print(x_train,y_train)
199/2:
# Install TensorFlow
# !pip install -q tensorflow-gpu==2.0.0-beta1

# try:
#   %tensorflow_version 2.x  # Colab only.
# except Exception:
#   pass

import tensorflow as tf
print(tf.__version__)
199/3:
# Load in the data
mnist = tf.keras.datasets.mnist

(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0
print("x_train.shape:", x_train.shape)
189/1:
# Install TensorFlow
# !pip install -q tensorflow-gpu==2.0.0-beta1

# try:
#   %tensorflow_version 2.x  # Colab only.
# except Exception:
#   pass

import tensorflow as tf
print(tf.__version__)
189/2:
# Load in the data
mnist = tf.keras.datasets.mnist

(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0
print("x_train.shape:", x_train.shape)
189/3:
# Build the model
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Flatten(input_shape=(28, 28)))
model.add(tf.keras.layers.Dense(128, activation='relu'))
model.add(tf.keras.layers.Dropout(0.2))
model.add(tf.keras.layers.Dense(10, activation='softmax'))

model.layers
189/4:
# Compile the model
opt = tf.keras.optimizers.Adam(learning_rate=.01)
model.compile(optimizer=opt,
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
189/5:
# Install TensorFlow
# !pip install -q tensorflow-gpu==2.0.0-beta1

# try:
#   %tensorflow_version 2.x  # Colab only.
# except Exception:
#   pass

import tensorflow as tf
print(tf.__version__)
189/6:
# Load in the data
mnist = tf.keras.datasets.mnist

(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0
print("x_train.shape:", x_train.shape)
189/7:
# Build the model
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Flatten(input_shape=(28, 28)))
model.add(tf.keras.layers.Dense(128, activation='relu'))
model.add(tf.keras.layers.Dropout(0.2))
model.add(tf.keras.layers.Dense(10, activation='softmax'))

model.layers
189/8:
# Compile the model
opt = tf.keras.optimizers.Adam(learning_rate=.01)
model.compile(optimizer=opt,
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
189/9:
print(x_train.dtype, y_train.dtype)
print(x_train,y_train)
189/10:
print(x_train.dtype, y_train.dtype)
print(max(x_train))
189/11:
print(x_train.dtype, y_train.dtype)
x_train.max()
189/12:
print(x_train.dtype, y_train.dtype)
x_train.mean()
189/13:
# Compile the model
opt = tf.keras.optimizers.Adam(learning_rate=.01)
model.compile(optimizer=opt,
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
189/14:
# Train the model
# with tf.device('/gpu:0'):
r = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=10)
189/15:
# Train the model
# with tf.device('/gpu:0'):
r = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=10)
201/1:
# Install TensorFlow
# !pip install -q tensorflow-gpu==2.0.0-beta1

# try:
#   %tensorflow_version 2.x  # Colab only.
# except Exception:
#   pass

import tensorflow as tf
print(tf.__version__)
201/2:
# Load in the data
mnist = tf.keras.datasets.mnist

(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0
print("x_train.shape:", x_train.shape)
201/3:
# Build the model
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Flatten(input_shape=(28, 28)))
model.add(tf.keras.layers.Dense(128, activation='relu'))
model.add(tf.keras.layers.Dropout(0.2))
model.add(tf.keras.layers.Dense(10, activation='softmax'))

model.layers
201/4:
# Compile the model
opt = tf.keras.optimizers.Adam(learning_rate=.01)
model.compile(optimizer=opt,
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
201/5:
print(x_train.dtype, y_train.dtype)
x_train.mean()
201/6:
# Train the model
# with tf.device('/gpu:0'):
r = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=10)
201/7:
# Train the model
# with tf.device('/gpu:0'):
r = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=10)
202/1:
# Install TensorFlow
# !pip install -q tensorflow-gpu==2.0.0-beta1

# try:
#   %tensorflow_version 2.x  # Colab only.
# except Exception:
#   pass

import tensorflow as tf
print(tf.__version__)
202/2:
# Train the model
# with tf.device('/gpu:0'):
r = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=10)
202/3:
# Load in the data
mnist = tf.keras.datasets.mnist

(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0
print("x_train.shape:", x_train.shape)
202/4:
# Build the model
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Flatten(input_shape=(28, 28)))
model.add(tf.keras.layers.Dense(128, activation='relu'))
model.add(tf.keras.layers.Dropout(0.2))
model.add(tf.keras.layers.Dense(10, activation='softmax'))

model.layers
202/5:
# Compile the model
opt = tf.keras.optimizers.Adam(learning_rate=.01)
model.compile(optimizer=opt,
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
202/6:
print(x_train.dtype, y_train.dtype)
x_train.mean()
202/7:
# Train the model
# with tf.device('/gpu:0'):
r = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=10)
202/8:
# Train the model
# with tf.device('/gpu:0'):
r = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=10)
203/1:
# Train the model
# with tf.device('/gpu:0'):
r = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=10)
203/2:
# Train the model
# with tf.device('/gpu:0'):
r = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=10)
203/3:
# Install TensorFlow
# !pip install -q tensorflow-gpu==2.0.0-beta1

# try:
#   %tensorflow_version 2.x  # Colab only.
# except Exception:
#   pass

import tensorflow as tf
print(tf.__version__)
203/4:
# Load in the data
mnist = tf.keras.datasets.mnist

(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0
print("x_train.shape:", x_train.shape)
203/5:
# Build the model
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Flatten(input_shape=(28, 28)))
model.add(tf.keras.layers.Dense(128, activation='relu'))
model.add(tf.keras.layers.Dropout(0.2))
model.add(tf.keras.layers.Dense(10, activation='softmax'))

model.layers
203/6:
# Compile the model
opt = tf.keras.optimizers.Adam(learning_rate=.01)
model.compile(optimizer=opt,
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
203/7:
print(x_train.dtype, y_train.dtype)
x_train.mean()
203/8:
# Train the model
with tf.device('/gpu:0'):
    r = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=10)
204/1:
# Install TensorFlow
# !pip install -q tensorflow-gpu==2.0.0-beta1

# try:
#   %tensorflow_version 2.x  # Colab only.
# except Exception:
#   pass

import tensorflow as tf
print(tf.__version__)
204/2:
# To add a new cell, type '# %%'
# To add a new markdown cell, type '# %% [markdown]'
# %%
# Install TensorFlow
# !pip install -q tensorflow-gpu==2.0.0-beta1

# try:
#   %tensorflow_version 2.x  # Colab only.
# except Exception:
#   pass

import tensorflow as tf
print(tf.__version__)
204/3:
# Load in the data
mnist = tf.keras.datasets.mnist

(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0
print("x_train.shape:", x_train.shape)
204/4:
# Build the model
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Flatten(input_shape=(28, 28)))
model.add(tf.keras.layers.Dense(128, activation='relu'))
model.add(tf.keras.layers.Dropout(0.2))
model.add(tf.keras.layers.Dense(10, activation='softmax'))

model.layers
204/5:
# Compile the model
opt = tf.keras.optimizers.Adam(learning_rate=.01)
model.compile(optimizer=opt,
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
204/6:
print(x_train.dtype, y_train.dtype)
x_train.mean()
204/7:
# Compile the model
opt = tf.keras.optimizers.Adam(learning_rate=.01)
model.compile(optimizer=opt,
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
204/8:
print(x_train.dtype, y_train.dtype)
x_train.mean()
204/9:
# Train the model
# with tf.device('/gpu:0'):
r = model.fit(x_train, y_train, batch_size=170,validation_data=(x_test, y_test), epochs=10)
204/10:
# Train the model
# with tf.device('/gpu:0'):
r = model.fit(x_train, y_train, batch_size=32,validation_data=(x_test, y_test), epochs=10)
206/1:
# Train the model
# with tf.device('/gpu:0'):
r = model.fit(x_train, y_train, batch_size=32,validation_data=(x_test, y_test), epochs=10)
207/1:
# Install TensorFlow
# !pip install -q tensorflow-gpu==2.0.0-beta1

# try:
#   %tensorflow_version 2.x  # Colab only.
# except Exception:
#   pass

import tensorflow as tf
print(tf.__version__)
207/2:
# Load in the data
mnist = tf.keras.datasets.mnist

(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0
print("x_train.shape:", x_train.shape)
207/3:
# Build the model
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Flatten(input_shape=(28, 28)))
model.add(tf.keras.layers.Dense(128, activation='relu'))
model.add(tf.keras.layers.Dropout(0.2))
model.add(tf.keras.layers.Dense(10, activation='softmax'))

model.layers
207/4:
# Compile the model
opt = tf.keras.optimizers.Adam(learning_rate=.01)
model.compile(optimizer=opt,
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
207/5:
print(x_train.dtype, y_train.dtype)
x_train.mean()
207/6:
# Train the model
with tf.device('/gpu:0'):
    r = model.fit(x_train, y_train,validation_data=(x_test, y_test), epochs=10)
207/7:
# Plot loss per iteration
import matplotlib.pyplot as plt
plt.plot(r.history['loss'], label='loss')
plt.plot(r.history['val_loss'], label='val_loss')
plt.legend()
207/8:
# Plot accuracy per iteration
plt.plot(r.history['accuracy'], label='acc')
plt.plot(r.history['val_accuracy'], label='val_acc')
plt.legend()
207/9:
# Evaluate the model
print(model.evaluate(x_test, y_test))
207/10:
# Plot confusion matrix
from sklearn.metrics import confusion_matrix
import numpy as np
import itertools

def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
  """
  This function prints and plots the confusion matrix.
  Normalization can be applied by setting `normalize=True`.
  """
  if normalize:
      cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
      print("Normalized confusion matrix")
  else:
      print('Confusion matrix, without normalization')

  print(cm)

  plt.imshow(cm, interpolation='nearest', cmap=cmap)
  plt.title(title)
  plt.colorbar()
  tick_marks = np.arange(len(classes))
  plt.xticks(tick_marks, classes, rotation=45)
  plt.yticks(tick_marks, classes)

  fmt = '.2f' if normalize else 'd'
  thresh = cm.max() / 2.
  for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
      plt.text(j, i, format(cm[i, j], fmt),
               horizontalalignment="center",
               color="white" if cm[i, j] > thresh else "black")

  plt.tight_layout()
  plt.ylabel('True label')
  plt.xlabel('Predicted label')
  plt.show()


p_test = model.predict(x_test).argmax(axis=1)
cm = confusion_matrix(y_test, p_test)
plot_confusion_matrix(cm, list(range(10)))

# Do these results make sense?
# It's easy to confuse 9 <--> 4, 9 <--> 7, 2 <--> 7, etc.
207/11:
# Show some misclassified examples
misclassified_idx = np.where(p_test != y_test)[0]
i = np.random.choice(misclassified_idx)
plt.imshow(x_test[i], cmap='gray')
plt.title("True label: %s Predicted: %s" % (y_test[i], p_test[i]));
207/12:
# Show some misclassified examples
misclassified_idx = np.where(p_test != y_test)[0]
i = np.random.choice(misclassified_idx)
plt.imshow(x_test[i], cmap='gray')
plt.title("True label: %s Predicted: %s" % (y_test[i], p_test[i]));
207/13:
# Show some misclassified examples
misclassified_idx = np.where(p_test != y_test)[0]
i = np.random.choice(misclassified_idx)
plt.imshow(x_test[i], cmap='gray')
plt.title("True label: %s Predicted: %s" % (y_test[i], p_test[i]));
207/14:
# Show some misclassified examples
misclassified_idx = np.where(p_test != y_test)[0]
i = np.random.choice(misclassified_idx)
plt.imshow(x_test[i], cmap='gray')
plt.title("True label: %s Predicted: %s" % (y_test[i], p_test[i]));
207/15: x_train
207/16: x_train.mean
207/17:
x_train.mean(
)
207/18: type(x_train)
207/19: x_train[1000:2000]
207/20: x_train.tail()
207/21: x_train
207/22: x_train.shape
207/23: x_train
207/24: x_train[0]
207/25: # x_train
207/26: y_train
207/27:
# Train the model
# with tf.device('/gpu:0'):
r = model.fit(x_train, y_train,validation_data=(x_test, y_test), epochs=10)
207/28:
# Train the model
# with tf.device('/gpu:0'):
r = model.fit(x_train, y_train,validation_data=(x_test, y_test), epochs=10)
207/29:
# Plot loss per iteration
import matplotlib.pyplot as plt
plt.plot(r.history['loss'], label='loss')
plt.plot(r.history['val_loss'], label='val_loss')
plt.legend()
207/30:
# Plot accuracy per iteration
plt.plot(r.history['accuracy'], label='acc')
plt.plot(r.history['val_accuracy'], label='val_acc')
plt.legend()
207/31:
# Evaluate the model
print(model.evaluate(x_test, y_test))
207/32:
# Plot confusion matrix
from sklearn.metrics import confusion_matrix
import numpy as np
import itertools

def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
  """
  This function prints and plots the confusion matrix.
  Normalization can be applied by setting `normalize=True`.
  """
  if normalize:
      cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
      print("Normalized confusion matrix")
  else:
      print('Confusion matrix, without normalization')

  print(cm)

  plt.imshow(cm, interpolation='nearest', cmap=cmap)
  plt.title(title)
  plt.colorbar()
  tick_marks = np.arange(len(classes))
  plt.xticks(tick_marks, classes, rotation=45)
  plt.yticks(tick_marks, classes)

  fmt = '.2f' if normalize else 'd'
  thresh = cm.max() / 2.
  for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
      plt.text(j, i, format(cm[i, j], fmt),
               horizontalalignment="center",
               color="white" if cm[i, j] > thresh else "black")

  plt.tight_layout()
  plt.ylabel('True label')
  plt.xlabel('Predicted label')
  plt.show()


p_test = model.predict(x_test).argmax(axis=1)
cm = confusion_matrix(y_test, p_test)
plot_confusion_matrix(cm, list(range(10)))

# Do these results make sense?
# It's easy to confuse 9 <--> 4, 9 <--> 7, 2 <--> 7, etc.
207/33:
# Show some misclassified examples
misclassified_idx = np.where(p_test != y_test)[0]
i = np.random.choice(misclassified_idx)
plt.imshow(x_test[i], cmap='gray')
plt.title("True label: %s Predicted: %s" % (y_test[i], p_test[i]));
207/34:
# Show some misclassified examples
misclassified_idx = np.where(p_test != y_test)[0]
i = np.random.choice(misclassified_idx)
plt.imshow(x_test[i], cmap='gray')
plt.title("True label: %s Predicted: %s" % (y_test[i], p_test[i]));
207/35:
# Show some misclassified examples
misclassified_idx = np.where(p_test != y_test)[0]
i = np.random.choice(misclassified_idx)
plt.imshow(x_test[i], cmap='gray')
plt.title("True label: %s Predicted: %s" % (y_test[i], p_test[i]));
207/36:
# Install TensorFlow
# !pip install -q tensorflow-gpu==2.0.0-beta1

# try:
#   %tensorflow_version 2.x  # Colab only.
# except Exception:
#   pass

import tensorflow as tf
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
import numpy as np
import itertools
print(tf.__version__)
207/37:
# Install TensorFlow
# !pip install -q tensorflow-gpu==2.0.0-beta1

# try:
#   %tensorflow_version 2.x  # Colab only.
# except Exception:
#   pass

import tensorflow as tf
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
import numpy as np
import itertools
import wandb
from wandb.keras import WandbCallback
from sklearn.preprocessing import StandardScaler
print(tf.__version__)
207/38:
scaler = StandardScaler()
wandb.init(project="mnist")
wandb.config.
207/39:
scaler = StandardScaler()
wandb.init(project="mnist")
wandb.config.dropout = 0.2
207/40:
scaler = StandardScaler()
wandb.init(project="mnist")
wandb.config.dropout = 0.2
207/41:
scaler = StandardScaler()
wandb.init(project="mnist")
wandb.config.dropout = 0.2
207/42:
# Load in the data
mnist = tf.keras.datasets.mnist

(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0

x_train = scaler.fit_transform(x_train)
x_test = scaler.fit_transform(x_test)

print("x_train.shape:", x_train.shape)
207/43:
# Load in the data
mnist = tf.keras.datasets.mnist

(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0

# x_train = scaler.fit_transform(x_train)
# x_test = scaler.fit_transform(x_test)

print("x_train.shape:", x_train.shape)
207/44:
# scaler = StandardScaler()
wandb.init(project="mnist")
wandb.config.dropout = 0.2
207/45:
# Load in the data
mnist = tf.keras.datasets.mnist

(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0

# x_train = scaler.fit_transform(x_train)
# x_test = scaler.fit_transform(x_test)
N,D = x_train.shape
print("x_train.shape:", x_train.shape)
207/46:
# Load in the data
mnist = tf.keras.datasets.mnist

(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0

# x_train = scaler.fit_transform(x_train)
# x_test = scaler.fit_transform(x_test)
N,D,D = x_train.shape
print("x_train.shape:", x_train.shape)
207/47:
# Load in the data
mnist = tf.keras.datasets.mnist

(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0

# x_train = scaler.fit_transform(x_train)
# x_test = scaler.fit_transform(x_test)
N,D0,D1 = x_train.shape
print("x_train.shape:", x_train.shape)
207/48:
# scaler = StandardScaler()
wandb.init(project="mnist")
wandb.config.dropout = 0.2
wandb.config.
207/49:
# scaler = StandardScaler()
wandb.init(project="mnist")
wandb.config.dropout = 0.2
wandb.config.dense=128
207/50:
# Build the model
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Flatten(input_shape=(D0, D1)))
model.add(tf.keras.layers.Dense(wandb.config.dense, activation='relu'))
model.add(tf.keras.layers.Dropout(wandb.config.dropout))
model.add(tf.keras.layers.Dense(10, activation='softmax'))

model.layers
207/51:
# scaler = StandardScaler()
wandb.init(project="mnist")
wandb.config.dropout = 0.2
wandb.config.dense=128
wandb.config.learning_rate = 0.01
wandb.config.batch_size = 64
wandb.config.epochs = 200
207/52:
# Compile the model
opt = tf.keras.optimizers.Adam(learning_rate=wandb.config.learning_rate)
model.compile(optimizer=opt,
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy',WandbCallback()])
207/53:
# scaler = StandardScaler()
wandb.init(project="mnist", config={
    'dropout':0.2,
    'dense0':128,
    'learning_rate':0.01,
    'batch_size':64,
    'epochs':100})
# wandb.config.dropout = 0.2
# wandb.config.dense=128
# wandb.config.learning_rate = 0.01
# wandb.config.batch_size = 64
# wandb.config.epochs = 200
207/54:
# scaler = StandardScaler()
hyperparameters = dict(
    dropout:0.2,
    dense0:128,
    learning_rate:0.01,
    batch_size:64,
    epochs:100}
)
wandb.init(project="mnist", config = hyperparameters, entity = 'seank')
config = wandb.config
207/55:
# scaler = StandardScaler()
hyperparameters = dict(
    dropout=0.2,
    dense0=128,
    learning_rate=0.01,
    batch_size=64,
    epochs=100}
)
wandb.init(project="mnist", config = hyperparameters, entity = 'seank')
config = wandb.config
207/56:
# scaler = StandardScaler()
hyperparameters = dict(
    dropout=0.2,
    dense0=128,
    learning_rate=0.01,
    batch_size=64,
    epochs=100
)
wandb.init(project="mnist", config = hyperparameters, entity = 'seank')
config = wandb.config
207/57:
# Build the model
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Flatten(input_shape=(D0, D1)))
model.add(tf.keras.layers.Dense(config.dense0, activation='relu'))
model.add(tf.keras.layers.Dropout(config.dropout))
model.add(tf.keras.layers.Dense(10, activation='softmax'))

model.layers
207/58:
# Compile the model
opt = tf.keras.optimizers.Adam(learning_rate=config.learning_rate)
model.compile(optimizer=opt,
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy',WandbCallback()])
207/59:
# Compile the model
opt = tf.keras.optimizers.Adam(learning_rate=config.learning_rate)
model.compile(optimizer=opt,
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
207/60:
print(x_train.dtype, y_train.dtype)
x_train.mean()
207/61:
# Train the model
# with tf.device('/gpu:0'):
r = model.fit(x_train, y_train,validation_data=(x_test, y_test), batch_size = config.batch_size,epochs=config.epochs,callbacks=[WandbCallback()])
207/62:
# scaler = StandardScaler()
hyperparameters = dict(
    dropout=0.2,
    dense0=128,
    learning_rate=0.01,
    batch_size=128,
    epochs=10
)
wandb.init(project="mnist", config = hyperparameters, entity = 'seank')
config = wandb.config
207/63:
# Build the model
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Flatten(input_shape=(D0, D1)))
model.add(tf.keras.layers.Dense(config.dense0, activation='relu'))
model.add(tf.keras.layers.Dropout(config.dropout))
model.add(tf.keras.layers.Dense(10, activation='softmax'))

model.layers
207/64:
# Compile the model
opt = tf.keras.optimizers.Adam(learning_rate=config.learning_rate)
model.compile(optimizer=opt,
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
207/65:
print(x_train.dtype, y_train.dtype)
x_train.mean()
207/66:
# Train the model
# with tf.device('/gpu:0'):
r = model.fit(x_train, y_train,validation_data=(x_test, y_test), batch_size = config.batch_size,epochs=config.epochs,callbacks=[WandbCallback()])
207/67:
# Plot loss per iteration
plt.plot(r.history['loss'], label='loss')
plt.plot(r.history['val_loss'], label='val_loss')
plt.legend()
207/68:
# Plot accuracy per iteration
plt.plot(r.history['accuracy'], label='acc')
plt.plot(r.history['val_accuracy'], label='val_acc')
plt.legend()
207/69:
# Evaluate the model
print(model.evaluate(x_test, y_test))
207/70:
# Plot confusion matrix
def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
  """
  This function prints and plots the confusion matrix.
  Normalization can be applied by setting `normalize=True`.
  """
  if normalize:
      cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
      print("Normalized confusion matrix")
  else:
      print('Confusion matrix, without normalization')

  print(cm)

  plt.imshow(cm, interpolation='nearest', cmap=cmap)
  plt.title(title)
  plt.colorbar()
  tick_marks = np.arange(len(classes))
  plt.xticks(tick_marks, classes, rotation=45)
  plt.yticks(tick_marks, classes)

  fmt = '.2f' if normalize else 'd'
  thresh = cm.max() / 2.
  for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
      plt.text(j, i, format(cm[i, j], fmt),
               horizontalalignment="center",
               color="white" if cm[i, j] > thresh else "black")

  plt.tight_layout()
  plt.ylabel('True label')
  plt.xlabel('Predicted label')
  plt.show()


p_test = model.predict(x_test).argmax(axis=1)
cm = confusion_matrix(y_test, p_test)
plot_confusion_matrix(cm, list(range(10)))

# Do these results make sense?
# It's easy to confuse 9 <--> 4, 9 <--> 7, 2 <--> 7, etc.
207/71:
# Show some misclassified examples
misclassified_idx = np.where(p_test != y_test)[0]
i = np.random.choice(misclassified_idx)
plt.imshow(x_test[i], cmap='gray')
plt.title("True label: %s Predicted: %s" % (y_test[i], p_test[i]));
207/72:
# scaler = StandardScaler()
hyperparameters = dict(
    dropout=0.2,
    dense0=256,
    learning_rate=0.01,
    batch_size=128,
    epochs=10
)
wandb.init(project="mnist", config = hyperparameters, entity = 'seank')
config = wandb.config
207/73:
# Build the model
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Flatten(input_shape=(D0, D1)))
model.add(tf.keras.layers.Dense(config.dense0, activation='relu'))
model.add(tf.keras.layers.Dropout(config.dropout))
model.add(tf.keras.layers.Dense(10, activation='softmax'))

model.layers
207/74:
# Compile the model
opt = tf.keras.optimizers.Adam(learning_rate=config.learning_rate)
model.compile(optimizer=opt,
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
207/75:
print(x_train.dtype, y_train.dtype)
x_train.mean()
207/76:
# Train the model
# with tf.device('/gpu:0'):
r = model.fit(x_train, y_train,validation_data=(x_test, y_test), batch_size = config.batch_size,epochs=config.epochs,callbacks=[WandbCallback()])
207/77:
# Plot loss per iteration
plt.plot(r.history['loss'], label='loss')
plt.plot(r.history['val_loss'], label='val_loss')
plt.legend()
207/78:
# Plot accuracy per iteration
plt.plot(r.history['accuracy'], label='acc')
plt.plot(r.history['val_accuracy'], label='val_acc')
plt.legend()
207/79:
# Evaluate the model
print(model.evaluate(x_test, y_test))
207/80:
# Plot confusion matrix
def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
  """
  This function prints and plots the confusion matrix.
  Normalization can be applied by setting `normalize=True`.
  """
  if normalize:
      cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
      print("Normalized confusion matrix")
  else:
      print('Confusion matrix, without normalization')

  print(cm)

  plt.imshow(cm, interpolation='nearest', cmap=cmap)
  plt.title(title)
  plt.colorbar()
  tick_marks = np.arange(len(classes))
  plt.xticks(tick_marks, classes, rotation=45)
  plt.yticks(tick_marks, classes)

  fmt = '.2f' if normalize else 'd'
  thresh = cm.max() / 2.
  for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
      plt.text(j, i, format(cm[i, j], fmt),
               horizontalalignment="center",
               color="white" if cm[i, j] > thresh else "black")

  plt.tight_layout()
  plt.ylabel('True label')
  plt.xlabel('Predicted label')
  plt.show()


p_test = model.predict(x_test).argmax(axis=1)
cm = confusion_matrix(y_test, p_test)
plot_confusion_matrix(cm, list(range(10)))

# Do these results make sense?
# It's easy to confuse 9 <--> 4, 9 <--> 7, 2 <--> 7, etc.
207/81:
# Show some misclassified examples
misclassified_idx = np.where(p_test != y_test)[0]
i = np.random.choice(misclassified_idx)
plt.imshow(x_test[i], cmap='gray')
plt.title("True label: %s Predicted: %s" % (y_test[i], p_test[i]));
207/82:
# scaler = StandardScaler()
hyperparameters = dict(
    dropout=0.2,
    dense0=256,
    learning_rate=0.1,
    batch_size=128,
    epochs=10
)
wandb.init(project="mnist", config = hyperparameters, entity = 'seank')
config = wandb.config
207/83:
# Build the model
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Flatten(input_shape=(D0, D1)))
model.add(tf.keras.layers.Dense(config.dense0, activation='relu'))
model.add(tf.keras.layers.Dropout(config.dropout))
model.add(tf.keras.layers.Dense(10, activation='softmax'))

model.layers
207/84:
# Compile the model
opt = tf.keras.optimizers.Adam(learning_rate=config.learning_rate)
model.compile(optimizer=opt,
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
207/85:
print(x_train.dtype, y_train.dtype)
x_train.mean()
207/86:
# Train the model
# with tf.device('/gpu:0'):
r = model.fit(x_train, y_train,validation_data=(x_test, y_test), batch_size = config.batch_size,epochs=config.epochs,callbacks=[WandbCallback()])
207/87:
# Plot loss per iteration
plt.plot(r.history['loss'], label='loss')
plt.plot(r.history['val_loss'], label='val_loss')
plt.legend()
207/88:
# scaler = StandardScaler()
hyperparameters = dict(
    dropout=0.2,
    dense0=256,
    learning_rate=0.03,
    batch_size=128,
    epochs=10
)
wandb.init(project="mnist", config = hyperparameters, entity = 'seank')
config = wandb.config
207/89:
# Plot accuracy per iteration
plt.plot(r.history['accuracy'], label='acc')
plt.plot(r.history['val_accuracy'], label='val_acc')
plt.legend()
207/90:
# Build the model
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Flatten(input_shape=(D0, D1)))
model.add(tf.keras.layers.Dense(config.dense0, activation='relu'))
model.add(tf.keras.layers.Dropout(config.dropout))
model.add(tf.keras.layers.Dense(10, activation='softmax'))

model.layers
207/91:
# Evaluate the model
print(model.evaluate(x_test, y_test))
207/92:
# Compile the model
opt = tf.keras.optimizers.Adam(learning_rate=config.learning_rate)
model.compile(optimizer=opt,
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
207/93:
print(x_train.dtype, y_train.dtype)
x_train.mean()
207/94:
# Plot confusion matrix
def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
  """
  This function prints and plots the confusion matrix.
  Normalization can be applied by setting `normalize=True`.
  """
  if normalize:
      cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
      print("Normalized confusion matrix")
  else:
      print('Confusion matrix, without normalization')

  print(cm)

  plt.imshow(cm, interpolation='nearest', cmap=cmap)
  plt.title(title)
  plt.colorbar()
  tick_marks = np.arange(len(classes))
  plt.xticks(tick_marks, classes, rotation=45)
  plt.yticks(tick_marks, classes)

  fmt = '.2f' if normalize else 'd'
  thresh = cm.max() / 2.
  for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
      plt.text(j, i, format(cm[i, j], fmt),
               horizontalalignment="center",
               color="white" if cm[i, j] > thresh else "black")

  plt.tight_layout()
  plt.ylabel('True label')
  plt.xlabel('Predicted label')
  plt.show()


p_test = model.predict(x_test).argmax(axis=1)
cm = confusion_matrix(y_test, p_test)
plot_confusion_matrix(cm, list(range(10)))

# Do these results make sense?
# It's easy to confuse 9 <--> 4, 9 <--> 7, 2 <--> 7, etc.
207/95:
# Train the model
# with tf.device('/gpu:0'):
r = model.fit(x_train, y_train,validation_data=(x_test, y_test), batch_size = config.batch_size,epochs=config.epochs,callbacks=[WandbCallback()])
207/96:
# Show some misclassified examples
misclassified_idx = np.where(p_test != y_test)[0]
i = np.random.choice(misclassified_idx)
plt.imshow(x_test[i], cmap='gray')
plt.title("True label: %s Predicted: %s" % (y_test[i], p_test[i]));
207/97:
# Plot loss per iteration
plt.plot(r.history['loss'], label='loss')
plt.plot(r.history['val_loss'], label='val_loss')
plt.legend()
207/98:
# Plot accuracy per iteration
plt.plot(r.history['accuracy'], label='acc')
plt.plot(r.history['val_accuracy'], label='val_acc')
plt.legend()
207/99:
# Evaluate the model
print(model.evaluate(x_test, y_test))
207/100:
# Plot confusion matrix
def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
  """
  This function prints and plots the confusion matrix.
  Normalization can be applied by setting `normalize=True`.
  """
  if normalize:
      cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
      print("Normalized confusion matrix")
  else:
      print('Confusion matrix, without normalization')

  print(cm)

  plt.imshow(cm, interpolation='nearest', cmap=cmap)
  plt.title(title)
  plt.colorbar()
  tick_marks = np.arange(len(classes))
  plt.xticks(tick_marks, classes, rotation=45)
  plt.yticks(tick_marks, classes)

  fmt = '.2f' if normalize else 'd'
  thresh = cm.max() / 2.
  for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
      plt.text(j, i, format(cm[i, j], fmt),
               horizontalalignment="center",
               color="white" if cm[i, j] > thresh else "black")

  plt.tight_layout()
  plt.ylabel('True label')
  plt.xlabel('Predicted label')
  plt.show()


p_test = model.predict(x_test).argmax(axis=1)
cm = confusion_matrix(y_test, p_test)
plot_confusion_matrix(cm, list(range(10)))

# Do these results make sense?
# It's easy to confuse 9 <--> 4, 9 <--> 7, 2 <--> 7, etc.
207/101:
# Show some misclassified examples
misclassified_idx = np.where(p_test != y_test)[0]
i = np.random.choice(misclassified_idx)
plt.imshow(x_test[i], cmap='gray')
plt.title("True label: %s Predicted: %s" % (y_test[i], p_test[i]));
207/102:
# scaler = StandardScaler()
hyperparameters = dict(
    dropout=0.2,
    dense0=512,
    learning_rate=0.01,
    batch_size=128,
    epochs=10
)
wandb.init(project="mnist", config = hyperparameters, entity = 'seank')
config = wandb.config
207/103:
# scaler = StandardScaler()
hyperparameters = dict(
    dropout=0.2,
    dense0=512,
    learning_rate=0.01,
    batch_size=128,
    epochs=10
)
wandb.init(project="mnist", config = hyperparameters, entity = 'seank')
config = wandb.config
207/104:
# Build the model
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Flatten(input_shape=(D0, D1)))
model.add(tf.keras.layers.Dense(config.dense0, activation='relu'))
model.add(tf.keras.layers.Dropout(config.dropout))
model.add(tf.keras.layers.Dense(10, activation='softmax'))

model.layers
207/105:
# Build the model
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Flatten(input_shape=(D0, D1)))
model.add(tf.keras.layers.Dense(config.dense0, activation='relu'))
model.add(tf.keras.layers.Dropout(config.dropout))
model.add(tf.keras.layers.Dense(10, activation='softmax'))

model.layers
207/106:
# Compile the model
opt = tf.keras.optimizers.Adam(learning_rate=config.learning_rate)
model.compile(optimizer=opt,
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
207/107:
# Compile the model
opt = tf.keras.optimizers.Adam(learning_rate=config.learning_rate)
model.compile(optimizer=opt,
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
207/108:
print(x_train.dtype, y_train.dtype)
x_train.mean()
207/109:
print(x_train.dtype, y_train.dtype)
x_train.mean()
207/110:
# Train the model
# with tf.device('/gpu:0'):
r = model.fit(x_train, y_train,validation_data=(x_test, y_test), batch_size = config.batch_size,epochs=config.epochs,callbacks=[WandbCallback()])
207/111:
# Train the model
# with tf.device('/gpu:0'):
r = model.fit(x_train, y_train,validation_data=(x_test, y_test), batch_size = config.batch_size,epochs=config.epochs,callbacks=[WandbCallback()])
207/112:
# Plot loss per iteration
plt.plot(r.history['loss'], label='loss')
plt.plot(r.history['val_loss'], label='val_loss')
plt.legend()
207/113:
# Plot loss per iteration
plt.plot(r.history['loss'], label='loss')
plt.plot(r.history['val_loss'], label='val_loss')
plt.legend()
207/114:
# Plot accuracy per iteration
plt.plot(r.history['accuracy'], label='acc')
plt.plot(r.history['val_accuracy'], label='val_acc')
plt.legend()
207/115:
# Plot accuracy per iteration
plt.plot(r.history['accuracy'], label='acc')
plt.plot(r.history['val_accuracy'], label='val_acc')
plt.legend()
207/116:
# Evaluate the model
print(model.evaluate(x_test, y_test))
207/117:
# Evaluate the model
print(model.evaluate(x_test, y_test))
207/118:
# Plot confusion matrix
def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
  """
  This function prints and plots the confusion matrix.
  Normalization can be applied by setting `normalize=True`.
  """
  if normalize:
      cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
      print("Normalized confusion matrix")
  else:
      print('Confusion matrix, without normalization')

  print(cm)

  plt.imshow(cm, interpolation='nearest', cmap=cmap)
  plt.title(title)
  plt.colorbar()
  tick_marks = np.arange(len(classes))
  plt.xticks(tick_marks, classes, rotation=45)
  plt.yticks(tick_marks, classes)

  fmt = '.2f' if normalize else 'd'
  thresh = cm.max() / 2.
  for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
      plt.text(j, i, format(cm[i, j], fmt),
               horizontalalignment="center",
               color="white" if cm[i, j] > thresh else "black")

  plt.tight_layout()
  plt.ylabel('True label')
  plt.xlabel('Predicted label')
  plt.show()


p_test = model.predict(x_test).argmax(axis=1)
cm = confusion_matrix(y_test, p_test)
plot_confusion_matrix(cm, list(range(10)))

# Do these results make sense?
# It's easy to confuse 9 <--> 4, 9 <--> 7, 2 <--> 7, etc.
207/119:
# Plot confusion matrix
def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
  """
  This function prints and plots the confusion matrix.
  Normalization can be applied by setting `normalize=True`.
  """
  if normalize:
      cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
      print("Normalized confusion matrix")
  else:
      print('Confusion matrix, without normalization')

  print(cm)

  plt.imshow(cm, interpolation='nearest', cmap=cmap)
  plt.title(title)
  plt.colorbar()
  tick_marks = np.arange(len(classes))
  plt.xticks(tick_marks, classes, rotation=45)
  plt.yticks(tick_marks, classes)

  fmt = '.2f' if normalize else 'd'
  thresh = cm.max() / 2.
  for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
      plt.text(j, i, format(cm[i, j], fmt),
               horizontalalignment="center",
               color="white" if cm[i, j] > thresh else "black")

  plt.tight_layout()
  plt.ylabel('True label')
  plt.xlabel('Predicted label')
  plt.show()


p_test = model.predict(x_test).argmax(axis=1)
cm = confusion_matrix(y_test, p_test)
plot_confusion_matrix(cm, list(range(10)))

# Do these results make sense?
# It's easy to confuse 9 <--> 4, 9 <--> 7, 2 <--> 7, etc.
207/120:
# Show some misclassified examples
misclassified_idx = np.where(p_test != y_test)[0]
i = np.random.choice(misclassified_idx)
plt.imshow(x_test[i], cmap='gray')
plt.title("True label: %s Predicted: %s" % (y_test[i], p_test[i]));
207/121:
# Show some misclassified examples
misclassified_idx = np.where(p_test != y_test)[0]
i = np.random.choice(misclassified_idx)
plt.imshow(x_test[i], cmap='gray')
plt.title("True label: %s Predicted: %s" % (y_test[i], p_test[i]));
207/122:
# Install TensorFlow
# !pip install -q tensorflow-gpu==2.0.0-beta1

# try:
#   %tensorflow_version 2.x  # Colab only.
# except Exception:
#   pass

import tensorflow as tf
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
import numpy as np
import itertools
import wandb
from wandb.keras import WandbCallback
from sklearn.preprocessing import StandardScaler
print(tf.__version__)
207/123:
# Install TensorFlow
# !pip install -q tensorflow-gpu==2.0.0-beta1

# try:
#   %tensorflow_version 2.x  # Colab only.
# except Exception:
#   pass

import tensorflow as tf
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
import numpy as np
import itertools
import wandb
from wandb.keras import WandbCallback
from sklearn.preprocessing import StandardScaler
import shap
print(tf.__version__)
207/124: r
207/125: background = x_train[np.random.choice(x_train.shape[0], 100, replace=False)]
207/126:
background = x_train[np.random.choice(x_train.shape[0], 100, replace=False)]
background
207/127:
background = x_train[np.random.choice(x_train.shape[0], 100, replace=False)]
x_train.shape[0]
207/128:
background = x_train[np.random.choice(x_train.shape[0], 100, replace=False)]
x_train.shape
207/129:
background = x_train[np.random.choice(N, 100, replace=False)]
background
207/130:
background = x_train[np.random.choice(N, 100, replace=False)]
e = shap.DeepExplainer(model, background)
207/131:
background = x_train[np.random.choice(N, 100, replace=False)]
e = shap.DeepExplainer(model, background)

shap_values = e.shap_values(x_test[1:5])
shap.image_plot(shap_values, -x_test[1:5])
207/132:
background = x_train[np.random.choice(N, 100, replace=False)]
e = shap.DeepExplainer(model, background)

rand = x_test[np.random.choice(n,4)]
shap_values = e.shap_values(rand)
shap.image_plot(shap_values, -rand)
207/133:
background = x_train[np.random.choice(N, 100, replace=False)]
e = shap.DeepExplainer(model, background)

rand = x_test[np.random.choice(N,4)]
shap_values = e.shap_values(rand)
shap.image_plot(shap_values, -rand)
207/134: N
207/135: np.random.choice(N,4)
207/136: random.choice(N,4)
207/137: x_test[np.random.choice(N,4)]
207/138: x_test.shape
207/139:
# Load in the data
mnist = tf.keras.datasets.mnist

(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0

# x_train = scaler.fit_transform(x_train)
# x_test = scaler.fit_transform(x_test)
N,D0,D1 = x_train.shape
print("x_train.shape:", x_train.shape)
207/140:
# Install TensorFlow
# !pip install -q tensorflow-gpu==2.0.0-beta1

# try:
#   %tensorflow_version 2.x  # Colab only.
# except Exception:
#   pass

import tensorflow as tf
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
import numpy as np
import itertools
import wandb
from wandb.keras import WandbCallback
from sklearn.preprocessing import StandardScaler
import shap
print(tf.__version__)
207/141:
# Load in the data
mnist = tf.keras.datasets.mnist

(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0

# x_train = scaler.fit_transform(x_train)
# x_test = scaler.fit_transform(x_test)
N,D0,D1 = x_train.shape
print("x_train.shape:", x_train.shape)
207/142:
# scaler = StandardScaler()
hyperparameters = dict(
    dropout=0.2,
    dense0=512,
    learning_rate=0.01,
    batch_size=128,
    epochs=10
)
wandb.init(project="mnist", config = hyperparameters, entity = 'seank')
config = wandb.config
207/143:
# scaler = StandardScaler()
hyperparameters = dict(
    dropout=0.2,
    dense0=512,
    learning_rate=0.01,
    batch_size=128,
    epochs=10
)
wandb.init(project="mnist", config = hyperparameters, entity = 'seank')
config = wandb.config
207/144:
# Build the model
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Flatten(input_shape=(D0, D1)))
model.add(tf.keras.layers.Dense(config.dense0, activation='relu'))
model.add(tf.keras.layers.Dropout(config.dropout))
model.add(tf.keras.layers.Dense(10, activation='softmax'))

model.layers
207/145:
# Compile the model
opt = tf.keras.optimizers.Adam(learning_rate=config.learning_rate)
model.compile(optimizer=opt,
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
207/146:
# Compile the model
opt = tf.keras.optimizers.Adam(learning_rate=config.learning_rate)
model.compile(optimizer=opt,
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
207/147:
print(x_train.dtype, y_train.dtype)
x_train.mean()
207/148:
background = x_train[np.random.choice(N, 100, replace=False)]
e = shap.DeepExplainer(model, background)

rand = x_test[np.random.choice(N,4)]
shap_values = e.shap_values(rand)
shap.image_plot(shap_values, -rand)
207/149:
background = x_train[np.random.choice(N, 100, replace=False)]
e = shap.DeepExplainer(model, background)


rand = x_test[np.random.choice(x_test.shape[0],4)]
shap_values = e.shap_values(rand)
shap.image_plot(shap_values, -rand)
207/150:
background = x_train[np.random.choice(N, 100, replace=False)]
e = shap.DeepExplainer(model, background)


rand = x_test[np.random.choice(x_test.shape[0],4)]
shap_values = e.shap_values(rand)
shap.image_plot(shap_values, -rand)
207/151:
background = x_train[np.random.choice(N, 100, replace=False)]
# e = shap.DeepExplainer(model, background)


# rand = x_test[np.random.choice(x_test.shape[0],4)]
# shap_values = e.shap_values(rand)
# shap.image_plot(shap_values, -rand)
207/152:
background = x_train[np.random.choice(N, 100, replace=False)]
e = shap.DeepExplainer(model, background)


# rand = x_test[np.random.choice(x_test.shape[0],4)]
# shap_values = e.shap_values(rand)
# shap.image_plot(shap_values, -rand)
207/153:
background = x_train[np.random.choice(N, 100, replace=False)]
e = shap.DeepExplainer(model, background)


rand = x_test[np.random.choice(x_test.shape[0],4)]
shap_values = e.shap_values(rand)
shap.image_plot(shap_values, -rand)
207/154:
background = x_train[np.random.choice(N, 100, replace=False)]
e = shap.DeepExplainer(model, background)


rand = x_test[np.random.choice(x_test.shape[0],4)]
# shap_values = e.shap_values(rand)
# shap.image_plot(shap_values, -rand)
207/155:
background = x_train[np.random.choice(N, 100, replace=False)]
e = shap.DeepExplainer(model, background)


rand = x_test[np.random.choice(x_test.shape[0],4)]
rand
# shap_values = e.shap_values(rand)
# shap.image_plot(shap_values, -rand)
207/156:
background = x_train[np.random.choice(N, 100, replace=False)]
e = shap.DeepExplainer(model, background)


rand = x_test[np.random.choice(x_test.shape[0],4)]
shap_values = e.shap_values(rand)
# shap.image_plot(shap_values, -rand)
207/157:
background = x_train[np.random.choice(N, 100, replace=False)]
e = shap.DeepExplainer(model, background)


rand = x_test[np.random.choice(x_test.shape[0],4)]
shap_values = e.shap_values(rand)
shap.image_plot(shap_values, -rand)
207/158:
background = x_train[np.random.choice(N, 100, replace=False)]
e = shap.DeepExplainer(model, background)


rand = x_test[np.random.choice(x_test.shape[0],4)]
shap_values = e.shap_values(rand)
shap.image_plot(shap_values, -rand)
207/159:
background = x_train[np.random.choice(x_train.shape[0], 100, replace=False)]
explainer = shap.DeepExplainer(model,background)

shap_values = explainer.shap_values(X)
shap.summary_plot(shap_values, X, plot_type="bar")
207/160:
background = x_train[np.random.choice(x_train.shape[0], 100, replace=False)]
explainer = shap.DeepExplainer(model,background)

shap_values = explainer.shap_values(x_train)
shap.summary_plot(shap_values, x_train, plot_type="bar")
209/1:
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
from sklearn.metrics import confusion_matrix, plot_confusion_matrix, plot_roc_curve
from time import time
import numpy as np
print(tf.__version__)
209/2: print(x_test.dtype, y_test.dtype)
209/3:
# import tensorflow as tf
# with tf.device('/gpu:0'):
#     a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')
#     b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2,3], name='b')
#     c = a+b
#     print(c)
209/4: df = load_breast_cancer()
209/5: df.keys()
209/6: df.feature_names
209/7: df.target_names
209/8: df.data.shape
209/9: df.target.shape
209/10:
x_train, x_test, y_train, y_test = train_test_split(df.data,df.target,test_size=0.33)
N,D = x_train.shape
209/11:
scaler = StandardScaler()
x_train = scaler.fit_transform(x_train)
x_test = scaler.transform(x_test)
209/12: print(x_test.dtype, y_test.dtype)
209/13:
print(x_test.dtype, y_test.dtype)
x_train.shape
209/14:
s = time()
model = keras.models.Sequential()

model.add(keras.layers.Input(shape=(D,)))
model.add(keras.layers.Dense(1, activation='sigmoid'))

opt = keras.optimizers.Adam(learning_rate=.01)
model.compile(optimizer = opt, loss = 'binary_crossentropy', metrics=['mae','accuracy']) # 'binary_accuracy','acc','accuracy' are the same!

# with tf.device('/gpu:0'):
r= model.fit(x_train,y_train,batch_size=400,epochs=200,validation_data=(x_test,y_test))
print(time() - s)
209/15:
model = keras.models.load_model(f'{dir}\\linear_classifier_mine.h5')
print(model.layers)
model.evaluate(x_test,y_test)
209/16:
model = keras.models.load_model(f'{dir}\\linear_classifier_mine.h5')
print(model.layers)
model.evaluate(x_test,y_test)
209/17:
model = keras.models.load_model(f'{dir}\\linear_classifier_mine.h5')
print(model.layers)
model.evaluate(x_test,y_test)
209/18:
s = time()
model = keras.models.Sequential()

model.add(keras.layers.Input(shape=(D,)))
model.add(keras.layers.Dense(1, activation='sigmoid'))

opt = keras.optimizers.Adam(learning_rate=.01)
model.compile(optimizer = opt, loss = 'binary_crossentropy', metrics=['mae','accuracy']) # 'binary_accuracy','acc','accuracy' are the same!

# with tf.device('/gpu:0'):
r= model.fit(x_train,y_train,batch_size=400,epochs=200,validation_data=(x_test,y_test))
print(time() - s)
211/1:
s = time()
model = keras.models.Sequential()

model.add(keras.layers.Input(shape=(D,)))
model.add(keras.layers.Dense(1, activation='sigmoid'))

opt = keras.optimizers.Adam(learning_rate=.01)
model.compile(optimizer = opt, loss = 'binary_crossentropy', metrics=['mae','accuracy']) # 'binary_accuracy','acc','accuracy' are the same!

# with tf.device('/gpu:0'):
r= model.fit(x_train,y_train,batch_size=400,epochs=200,validation_data=(x_test,y_test))
print(time() - s)
211/2:
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
from sklearn.metrics import confusion_matrix, plot_confusion_matrix, plot_roc_curve
from time import time
import numpy as np
print(tf.__version__)
211/3:
s = time()
model = keras.models.Sequential()

model.add(keras.layers.Input(shape=(D,)))
model.add(keras.layers.Dense(1, activation='sigmoid'))

opt = keras.optimizers.Adam(learning_rate=.01)
model.compile(optimizer = opt, loss = 'binary_crossentropy', metrics=['mae','accuracy']) # 'binary_accuracy','acc','accuracy' are the same!

# with tf.device('/gpu:0'):
r= model.fit(x_train,y_train,batch_size=400,epochs=200,validation_data=(x_test,y_test))
print(time() - s)
211/4:
# import tensorflow as tf
# with tf.device('/gpu:0'):
#     a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')
#     b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2,3], name='b')
#     c = a+b
#     print(c)
211/5: df = load_breast_cancer()
211/6: df.keys()
211/7: df.feature_names
211/8: df.target_names
211/9: df.data.shape
211/10: df.target.shape
211/11:
x_train, x_test, y_train, y_test = train_test_split(df.data,df.target,test_size=0.33)
N,D = x_train.shape
211/12:
scaler = StandardScaler()
x_train = scaler.fit_transform(x_train)
x_test = scaler.transform(x_test)
211/13:
print(x_test.dtype, y_test.dtype)
x_train.shape
211/14:
s = time()
model = keras.models.Sequential()

model.add(keras.layers.Input(shape=(D,)))
model.add(keras.layers.Dense(1, activation='sigmoid'))

opt = keras.optimizers.Adam(learning_rate=.01)
model.compile(optimizer = opt, loss = 'binary_crossentropy', metrics=['mae','accuracy']) # 'binary_accuracy','acc','accuracy' are the same!

# with tf.device('/gpu:0'):
r= model.fit(x_train,y_train,batch_size=400,epochs=200,validation_data=(x_test,y_test))
print(time() - s)
207/161:
# Train the model
# with tf.device('/gpu:0'):
r = model.fit(x_train, y_train,validation_data=(x_test, y_test), batch_size = config.batch_size,epochs=config.epochs,callbacks=[WandbCallback()])
210/1:
s = time()
model = keras.models.Sequential()

model.add(keras.layers.Input(shape=(D,)))
model.add(keras.layers.Dense(1, activation='sigmoid'))

opt = keras.optimizers.Adam(learning_rate=.01)
model.compile(optimizer = opt, loss = 'binary_crossentropy', metrics=['mae','accuracy']) # 'binary_accuracy','acc','accuracy' are the same!

# with tf.device('/gpu:0'):
r= model.fit(x_train,y_train,batch_size=400,epochs=200,validation_data=(x_test,y_test))
print(time() - s)
210/2:
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
from sklearn.metrics import confusion_matrix, plot_confusion_matrix, plot_roc_curve
from time import time
import numpy as np
print(tf.__version__)
210/3:
# import tensorflow as tf
# with tf.device('/gpu:0'):
#     a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')
#     b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2,3], name='b')
#     c = a+b
#     print(c)
210/4: df = load_breast_cancer()
210/5: df.keys()
210/6: df.feature_names
210/7: df.target_names
210/8: df.data.shape
210/9: df.target.shape
210/10:
x_train, x_test, y_train, y_test = train_test_split(df.data,df.target,test_size=0.33)
N,D = x_train.shape
210/11:
scaler = StandardScaler()
x_train = scaler.fit_transform(x_train)
x_test = scaler.transform(x_test)
210/12:
print(x_test.dtype, y_test.dtype)
x_train.shape
210/13:
s = time()
model = keras.models.Sequential()

model.add(keras.layers.Input(shape=(D,)))
model.add(keras.layers.Dense(1, activation='sigmoid'))

opt = keras.optimizers.Adam(learning_rate=.01)
model.compile(optimizer = opt, loss = 'binary_crossentropy', metrics=['mae','accuracy']) # 'binary_accuracy','acc','accuracy' are the same!

# with tf.device('/gpu:0'):
r= model.fit(x_train,y_train,batch_size=400,epochs=200,validation_data=(x_test,y_test))
print(time() - s)
210/14:
print("Train score:", model.evaluate(x_train, y_train))
print("Test score:", model.evaluate(x_test, y_test))
210/15:
P = np.round(model.predict(x_test)).flatten()
confusion_matrix(P,y_test)
210/16: print(np.round(model.predict(x_test)).flatten())
210/17:
print("Manually calculated accuracy:", np.round(np.mean(P == y_test),2))
print("Evaluate output:", model.evaluate(x_test, y_test))
210/18:
dir = 'models'
if not os.path.exists(dir):
    os.makedirs(dir)
model.save(f'{dir}\\linear_classifier_mine.h5')
del model
210/19: x_test
210/20: x_test.shape
210/21: x_test[0]
210/22: x_test[0].shape
210/23: x_test[0]
210/24: x_test
210/25: x_test.shape
210/26:
background = x_train[np.random.choice(x_train.shape[0], 100, replace=False)]
explainer = shap.DeepExplainer(model,background)

shap_values = explainer.shap_values(x_train)
shap.summary_plot(shap_values, x_train, plot_type="bar")
210/27:
import shap
background = x_train[np.random.choice(x_train.shape[0], 100, replace=False)]
explainer = shap.DeepExplainer(model,background)

shap_values = explainer.shap_values(x_train)
shap.summary_plot(shap_values, x_train, plot_type="bar")
210/28: model
210/29:
print("Manually calculated accuracy:", np.round(np.mean(P == y_test),2))
print("Evaluate output:", model.evaluate(x_test, y_test))
210/30:
model = keras.models.load_model(f'{dir}\\linear_classifier_mine.h5')
print(model.layers)
model.evaluate(x_test,y_test)
210/31:
dir = 'models'
if not os.path.exists(dir):
    os.makedirs(dir)
model.save(f'{dir}\\linear_classifier_mine.h5')
del model
210/32:
model = keras.models.load_model(f'{dir}\\linear_classifier_mine.h5')
print(model.layers)
model.evaluate(x_test,y_test)
210/33:
import shap
background = x_train[np.random.choice(x_train.shape[0], 100, replace=False)]
explainer = shap.DeepExplainer(model,background)

shap_values = explainer.shap_values(x_train)
shap.summary_plot(shap_values, x_train, plot_type="bar")
210/34:
import shap
background = x_train[np.random.choice(x_train.shape[0], 100, replace=False)]
explainer = shap.DeepExplainer(model,background)

shap_values = explainer.shap_values(x_train)
shap.summary_plot(shap_values, x_train, plot_type="histogram")
210/35:
import shap
background = x_train[np.random.choice(x_train.shape[0], 100, replace=False)]
explainer = shap.DeepExplainer(model,background)

shap_values = explainer.shap_values(x_train)
shap.summary_plot(shap_values, x_train)
210/36: df.feature_names
210/37: shap_values
210/38:
# df.feature_names[]
np.argsort(shap_values)
210/39:
# df.feature_names[]
list(np.argsort(shap_values))
210/40:
# df.feature_names[]
# list(np.argsort(shap_values))
shap_values
210/41:
# df.feature_names[]
# list(np.argsort(shap_values))
shap_values[-1]
210/42:
# df.feature_names[]
# list(np.argsort(shap_values))
shap_values.shape
210/43:
# df.feature_names[]
# list(np.argsort(shap_values))
shap_values
210/44:
# df.feature_names[]
# list(np.argsort(shap_values))
len(shap_values)
210/45:
# df.feature_names[]
# list(np.argsort(shap_values))
shap_values
210/46:
import shap
background = x_train[np.random.choice(x_train.shape[0], 100, replace=False)]
explainer = shap.DeepExplainer(model,background)

shap_values = explainer.shap_values(x_train)
shap.summary_plot(shap_values, x_train)
210/47:
# df.feature_names[]
# list(np.argsort(shap_values))
shap_values
210/48:
# df.feature_names[]
# list(np.argsort(shap_values))
shap_values
210/49:
# df.feature_names[]
# list(np.argsort(shap_values))
shap_values
210/50:
# df.feature_names[]
# list(np.argsort(shap_values))
shap_values
210/51:
# df.feature_names[]
# list(np.argsort(shap_values))
unlist(shap_values)
210/52:
# df.feature_names[]
# list(np.argsort(shap_values))
shap_values
210/53:
# df.feature_names[]
# list(np.argsort(shap_values)) # sorted(range(len(vals)), key=vals.__getitem__)
np.array(list_of_arrays).flatten()
210/54:
# df.feature_names[]
# list(np.argsort(shap_values)) # sorted(range(len(vals)), key=vals.__getitem__)
np.array(shap_values).flatten()
210/55:
import shap
background = x_train[np.random.choice(x_train.shape[0], 100, replace=False)]
explainer = shap.DeepExplainer(model,background)

shap_values = explainer.shap_values(x_train)
shap.summary_plot(shap_values, x_train)
210/56:
import shap
background = x_train[np.random.choice(x_train.shape[0], 100, replace=False)]
explainer = shap.DeepExplainer(model,background)

shap_values = explainer.shap_values(x_train)
shap.summary_plot(shap_values, x_train)
210/57:
import shap
background = x_train[np.random.choice(x_train.shape[0], 100, replace=False)]
explainer = shap.DeepExplainer(model,background)

shap_values = explainer.shap_values(x_train)
shap.summary_plot(shap_values, x_train)
210/58:
import shap
background = x_train[np.random.choice(x_train.shape[0], 100, replace=False)]
explainer = shap.DeepExplainer(model,background)

shap_values = explainer.shap_values(x_train)
shap.summary_plot(shap_values, x_train)
210/59:
import shap
background = x_train[np.random.choice(x_train.shape[0], 100, replace=False)]
explainer = shap.DeepExplainer(model,background)

shap_values = explainer.shap_values(x_train)
shap.summary_plot(shap_values, x_train)
210/60:
import shap
# background = x_train[np.random.choice(x_train.shape[0], 100, replace=False)]
explainer = shap.DeepExplainer(model,x_train[-1])

shap_values = explainer.shap_values(x_train)
shap.summary_plot(shap_values, x_train)
210/61:
import shap
# background = x_train[np.random.choice(x_train.shape[0], 100, replace=False)]
explainer = shap.DeepExplainer(model,background)

shap_values = explainer.shap_values(background)
shap.summary_plot(shap_values, x_train)
210/62:
import shap
# background = x_train[np.random.choice(x_train.shape[0], 100, replace=False)]
explainer = shap.DeepExplainer(model,background)

shap_values = explainer.shap_values(background)
shap.summary_plot(shap_values, x_train)
210/63:
import shap
# background = x_train[np.random.choice(x_train.shape[0], 100, replace=False)]
explainer = shap.DeepExplainer(model,background)

shap_values = explainer.shap_values(background)
shap.summary_plot(shap_values, x_train)
210/64:
import shap
# background = x_train[np.random.choice(x_train.shape[0], 100, replace=False)]
explainer = shap.DeepExplainer(model,background)

shap_values = explainer.shap_values(background)
shap.summary_plot(shap_values, x_train)
210/65:
import shap
# background = x_train[np.random.choice(x_train.shape[0], 100, replace=False)]
explainer = shap.DeepExplainer(model,background)

shap_values = explainer.shap_values(background)
shap.summary_plot(shap_values, x_train)
210/66:
import shap
# background = x_train[np.random.choice(x_train.shape[0], 100, replace=False)]
explainer = shap.DeepExplainer(model,background)

shap_values = explainer.shap_values(background)
shap.summary_plot(shap_values, x_train)
210/67:
# df.feature_names[]
# list(np.argsort(shap_values)) # sorted(range(len(vals)), key=vals.__getitem__)
shap_values
210/68:
# df.feature_names[]
# list(np.argsort(shap_values)) # sorted(range(len(vals)), key=vals.__getitem__)
background
210/69:
# df.feature_names[]
# list(np.argsort(shap_values)) # sorted(range(len(vals)), key=vals.__getitem__)
background.shape
210/70:
# df.feature_names[]
# list(np.argsort(shap_values)) # sorted(range(len(vals)), key=vals.__getitem__)
np.array(shap_values).flatten()
len(shap_values)
210/71:
# df.feature_names[]
# list(np.argsort(shap_values)) # sorted(range(len(vals)), key=vals.__getitem__)
np.array(shap_values).flatten()
len(shap_values)
210/72:
# df.feature_names[]
# list(np.argsort(shap_values)) # sorted(range(len(vals)), key=vals.__getitem__)
np.array(shap_values).shape
210/73:
# df.feature_names[]
# list(np.argsort(shap_values)) # sorted(range(len(vals)), key=vals.__getitem__)
np.array(shap_values).flatten()
210/74:
# df.feature_names[]
# list(np.argsort(shap_values)) # sorted(range(len(vals)), key=vals.__getitem__)
np.array(shap_values).flatten().shape()
210/75:
# df.feature_names[]
# list(np.argsort(shap_values)) # sorted(range(len(vals)), key=vals.__getitem__)
np.array(shap_values).flatten().shape()
210/76:
# df.feature_names[]
# list(np.argsort(shap_values)) # sorted(range(len(vals)), key=vals.__getitem__)
np.array(shap_values).flatten().shape()
210/77:
# df.feature_names[]
# list(np.argsort(shap_values)) # sorted(range(len(vals)), key=vals.__getitem__)
np.array(shap_values).flatten()
210/78:
# df.feature_names[]
# list(np.argsort(shap_values)) # sorted(range(len(vals)), key=vals.__getitem__)
np.array(shap_values).flatten()
210/79:
# df.feature_names[]
# list(np.argsort(shap_values)) # sorted(range(len(vals)), key=vals.__getitem__)
np.array(shap_values).flatten()
210/80:
# df.feature_names[]
# list(np.argsort(shap_values)) # sorted(range(len(vals)), key=vals.__getitem__)
np.array(shap_values).flatten()
210/81:
# df.feature_names[]
# list(np.argsort(shap_values)) # sorted(range(len(vals)), key=vals.__getitem__)
np.array(shap_values).flatten()
210/82:
# df.feature_names[]
# list(np.argsort(shap_values)) # sorted(range(len(vals)), key=vals.__getitem__)
np.array(shap_values).flatten()
210/83:
# df.feature_names[]
# list(np.argsort(shap_values)) # sorted(range(len(vals)), key=vals.__getitem__)
np.array(shap_values).flatten().shape
210/84:
# df.feature_names[]
# list(np.argsort(shap_values)) # sorted(range(len(vals)), key=vals.__getitem__)
np.array(shap_values)
210/85:
import shap
# background = x_train[np.random.choice(x_train.shape[0], 100, replace=False)]
explainer = shap.DeepExplainer(model,background)

shap_values = explainer.shap_values(background)
print(shap_values)
shap.summary_plot(shap_values, x_train)
210/86:
import shap
# background = x_train[np.random.choice(x_train.shape[0], 100, replace=False)]
explainer = shap.DeepExplainer(model,background)

shap_values = explainer.shap_values(background)
shap.summary_plot(shap_values, x_train)
210/87:
import shap
background = x_train[np.random.choice(x_train.shape[0], 100, replace=False)]
explainer = shap.DeepExplainer(model,background)

shap_values = explainer.shap_values(background)
shap.summary_plot(shap_values, x_train)
210/88:
import shap
background = x_train[np.random.choice(x_train.shape[0], 100, replace=False)]
explainer = shap.DeepExplainer(model,background)

shap_values = explainer.shap_values(background)
shap.summary_plot(shap_values, x_train)
210/89:
import shap
background = x_train[np.random.choice(x_train.shape[0], 100, replace=False)]
explainer = shap.DeepExplainer(model,background)

shap_values = explainer.shap_values(background)
shap.summary_plot(shap_values, x_train)
210/90:
import shap
# background = x_train[np.random.choice(x_train.shape[0], 100, replace=False)]
background = x_train[-1]
explainer = shap.DeepExplainer(model,background)

shap_values = explainer.shap_values(background)
shap.summary_plot(shap_values, x_train)
210/91:
import shap
# background = x_train[np.random.choice(x_train.shape[0], 100, replace=False)]
background = x_train[-1]
explainer = shap.DeepExplainer(model,background)

shap_values = explainer.shap_values(background)
# shap.summary_plot(shap_values, x_train)
shap_values
210/92:
import shap
# background = x_train[np.random.choice(x_train.shape[0], 100, replace=False)]
background = x_train[-100:]
explainer = shap.DeepExplainer(model,background)

shap_values = explainer.shap_values(background)
shap.summary_plot(shap_values, x_train)
210/93:
import shap
# background = x_train[np.random.choice(x_train.shape[0], 100, replace=False)]
background = x_train[-100:]
explainer = shap.DeepExplainer(model,background)

shap_values = explainer.shap_values(background)
shap.summary_plot(shap_values, x_train)
210/94:
import shap
# background = x_train[np.random.choice(x_train.shape[0], 100, replace=False)]
background = x_train[-200:]
explainer = shap.DeepExplainer(model,background)

shap_values = explainer.shap_values(background)
shap.summary_plot(shap_values, x_train)
210/95:
import shap
# background = x_train[np.random.choice(x_train.shape[0], 100, replace=False)]
background = x_train[-200:]
explainer = shap.DeepExplainer(model,background)

shap_values = explainer.shap_values(background)
shap.summary_plot(shap_values, x_train)
210/96:
import shap
# background = x_train[np.random.choice(x_train.shape[0], 100, replace=False)]
background = x_train[-100:]
explainer = shap.DeepExplainer(model,background)

shap_values = explainer.shap_values(background)
shap.summary_plot(shap_values, x_train)
210/97:
import shap
background = x_train[np.random.choice(x_train.shape[0], 1000, replace=False)]
# background = x_train[-100:]
explainer = shap.DeepExplainer(model,background)

shap_values = explainer.shap_values(background)
shap.summary_plot(shap_values, x_train)
210/98:
import shap
background = x_train[np.random.choice(x_train.shape[0], 100, replace=False)]
# background = x_train[-100:]
explainer = shap.DeepExplainer(model,background)

shap_values = explainer.shap_values(background)
shap.summary_plot(shap_values, x_train)
210/99:
import shap
background = x_train[np.random.choice(x_train.shape[0], 200, replace=False)]
# background = x_train[-100:]
explainer = shap.DeepExplainer(model,background)

shap_values = explainer.shap_values(background)
shap.summary_plot(shap_values, x_train)
210/100:
import shap
background = x_train[np.random.choice(x_train.shape[0], 100, replace=False)]
# background = x_train[-100:]
explainer = shap.DeepExplainer(model,background)

shap_values = explainer.shap_values(background)
shap.summary_plot(shap_values, x_train)
210/101:
import shap
background = x_train[np.random.choice(x_train.shape[0], 300, replace=False)]
# background = x_train[-100:]
explainer = shap.DeepExplainer(model,background)

shap_values = explainer.shap_values(background)
shap.summary_plot(shap_values, x_train)
210/102:
import shap
background = x_train[np.random.choice(x_train.shape[0], N, replace=False)]
# background = x_train[-100:]
explainer = shap.DeepExplainer(model,background)

shap_values = explainer.shap_values(background)
shap.summary_plot(shap_values, x_train)
210/103:
import shap
background = x_train[np.random.choice(x_train.shape[0], N, replace=False)]
# background = x_train[-100:]
explainer = shap.DeepExplainer(model)

shap_values = explainer.shap_values(background)
shap.summary_plot(shap_values, x_train)
210/104:
import shap
background = x_train[np.random.choice(x_train.shape[0], N, replace=False)]
# background = x_train[-100:]
explainer = shap.DeepExplainer(model,background)

shap_values = explainer.shap_values(background)
shap.summary_plot(shap_values, x_train)
210/105:
import shap
background = x_train[np.random.choice(x_train.shape[0], N, replace=False)]
# background = x_train[-100:]
explainer = shap.DeepExplainer(model,background)

shap_values = explainer.shap_values(background)
shap.summary_plot(shap_values, x_train)
210/106:
import shap
background = x_train[np.random.choice(x_train.shape[0], N, replace=False)]
# background = x_train[-100:]
explainer = shap.DeepExplainer(model,background)

shap_values = explainer.shap_values(background)
shap.summary_plot(shap_values, x_train)
213/1:
model = keras.models.load_model(f'{dir}\\linear_classifier_mine.h5')
print(model.layers)
model.evaluate(x_test,y_test)
213/2:
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
from sklearn.metrics import confusion_matrix, plot_confusion_matrix, plot_roc_curve
from time import time
import numpy as np
print(tf.__version__)
213/3: df = load_breast_cancer()
213/4: df = load_breast_cancer()
213/5: df.keys()
213/6: df.feature_names
213/7:
x_train, x_test, y_train, y_test = train_test_split(df.data,df.target,test_size=0.33)
N,D = x_train.shape
213/8:
scaler = StandardScaler()
x_train = scaler.fit_transform(x_train)
x_test = scaler.transform(x_test)
213/9:
print(x_test.dtype, y_test.dtype)
x_train.shape
213/10:
print("Train score:", model.evaluate(x_train, y_train))
print("Test score:", model.evaluate(x_test, y_test))
213/11:
P = np.round(model.predict(x_test)).flatten()
confusion_matrix(P,y_test)
213/12:
dir = 'models'
if not os.path.exists(dir):
    os.makedirs(dir)
model.save(f'{dir}\\linear_classifier_mine.h5')
# del model
213/13:
dir = 'models'
if not os.path.exists(dir):
    os.makedirs(dir)
model.save(f'{dir}\\linear_classifier_mine.h5')
# del model
213/14:
model = keras.models.load_model(f'{dir}\\linear_classifier_mine.h5')
print(model.layers)
model.evaluate(x_test,y_test)
213/15:
print("Manually calculated accuracy:", np.round(np.mean(P == y_test),2))
print("Evaluate output:", model.evaluate(x_test, y_test))
213/16: print(np.round(model.predict(x_test)).flatten())
213/17:
P = np.round(model.predict(x_test)).flatten()
confusion_matrix(P,y_test)
213/18:
fig, (ax0,ax1) = plt.subplots(2,1,sharex=True)

ax0.plot(r.history['loss'],label='loss')
ax0.plot(r.history['val_loss'],label='val_loss')
ax0.legend()

ax1.plot(r.history['accuracy'],label='accuracy')
ax1.plot(r.history['val_accuracy'],label='val_accuracy')
ax1.legend()
plt.show()
213/19:
print(r.history.keys())
plt.plot(r.history['loss'],label='loss')
plt.plot(r.history['val_loss'],label='val_loss')
plt.legend()
213/20:
print("Train score:", model.evaluate(x_train, y_train))
print("Test score:", model.evaluate(x_test, y_test))
213/21:
print(r.history.keys())
plt.plot(r.history['loss'],label='loss')
plt.plot(r.history['val_loss'],label='val_loss')
plt.legend()
213/22:
model = keras.models.load_model(f'{dir}\\linear_classifier_mine.h5')
print(model.layers)
model.evaluate(x_test,y_test)
type(model)
216/1:
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
from sklearn.metrics import confusion_matrix, plot_confusion_matrix, plot_roc_curve
from time import time
import numpy as np
print(tf.__version__)
216/2:
# model = keras.models.load_model(f'{dir}\\linear_classifier_mine.h5')
# print(model.layers)
# model.evaluate(x_test,y_test)
# model = keras.models.Sequential()
# r = model.history()
216/3:
import shap
background = x_train[np.random.choice(x_train.shape[0], N, replace=False)]
# background = x_train[-100:]
explainer = shap.DeepExplainer(model,background)

shap_values = explainer.shap_values(background)
shap.summary_plot(shap_values, x_train)
216/4:
# df.feature_names[]
# list(np.argsort(shap_values)) # sorted(range(len(vals)), key=vals.__getitem__)
np.array(shap_values)[]
216/5:
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
from sklearn.metrics import confusion_matrix, plot_confusion_matrix, plot_roc_curve
from time import time
import numpy as np
print(tf.__version__)
216/6:
# import tensorflow as tf
# with tf.device('/gpu:0'):
#     a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')
#     b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2,3], name='b')
#     c = a+b
#     print(c)
216/7: df = load_breast_cancer()
216/8: df.keys()
216/9: df.feature_names
216/10: df.target_names
216/11: df.data.shape
216/12: df.target.shape
216/13:
x_train, x_test, y_train, y_test = train_test_split(df.data,df.target,test_size=0.33)
N,D = x_train.shape
216/14:
scaler = StandardScaler()
x_train = scaler.fit_transform(x_train)
x_test = scaler.transform(x_test)
216/15:
print(x_test.dtype, y_test.dtype)
x_train.shape
216/16:
s = time()
model = keras.models.Sequential()

model.add(keras.layers.Input(shape=(D,)))
model.add(keras.layers.Dense(1, activation='sigmoid'))

opt = keras.optimizers.Adam(learning_rate=.01)
model.compile(optimizer = opt, loss = 'binary_crossentropy', metrics=['mae','accuracy']) # 'binary_accuracy','acc','accuracy' are the same!

# with tf.device('/gpu:0'):
r= model.fit(x_train,y_train,batch_size=400,epochs=200,validation_data=(x_test,y_test))
print(time() - s)
216/17:
print("Train score:", model.evaluate(x_train, y_train))
print("Test score:", model.evaluate(x_test, y_test))
216/18:
print(r.history.keys())
plt.plot(r.history['loss'],label='loss')
plt.plot(r.history['val_loss'],label='val_loss')
plt.legend()
216/19:
fig, (ax0,ax1) = plt.subplots(2,1,sharex=True)

ax0.plot(r.history['loss'],label='loss')
ax0.plot(r.history['val_loss'],label='val_loss')
ax0.legend()

ax1.plot(r.history['accuracy'],label='accuracy')
ax1.plot(r.history['val_accuracy'],label='val_accuracy')
ax1.legend()
plt.show()
216/20:
P = np.round(model.predict(x_test)).flatten()
confusion_matrix(P,y_test)
216/21: print(np.round(model.predict(x_test)).flatten())
216/22:
print("Manually calculated accuracy:", np.round(np.mean(P == y_test),2))
print("Evaluate output:", model.evaluate(x_test, y_test))
216/23:
dir = 'models'
if not os.path.exists(dir):
    os.makedirs(dir)
model.save(f'{dir}\\linear_classifier_mine.h5')
# del model
216/24: r
216/25:
import shap
background = x_train[np.random.choice(x_train.shape[0], N, replace=False)]
# background = x_train[-100:]
explainer = shap.DeepExplainer(model,background)

shap_values = explainer.shap_values(background)
shap.summary_plot(shap_values, x_train)
216/26: np.corrcoef(x_train)
216/27: np.corrcoef(x_train).shape
216/28:
# import seaborn as sns; sns.set()
x_train.shape
# np.corrcoef(x_train)
216/29:
x = np.random.randint(0, 50, 1000)
x
216/30:
x = np.random.randint(0, 50, 1000)
x
216/31:
x = np.random.randint(0, 50, 1000)
y = x + np.random.normal(0, 10, 1000)
y
216/32:
x = np.random.randint(0, 50, 1000)
y = x + np.random.normal(0, 10, 1000)
plt.scatter(x, y)
216/33:
x = np.random.randint(0, 50, 1000)
y = x + np.random.normal(0, 10, 1000)
plt.scatter(x, y)
216/34:
x = np.random.randint(0, 50, 1000)
y = x + np.random.normal(0, 10, 1000)
plt.scatter(x, y)
216/35:
x = np.random.randint(0, 50, 1000)
y = x + np.random.normal(0, 10, 1000)
plt.scatter(x, y)
216/36:
x = np.random.randint(0, 50, 1000)
y = x + np.random.normal(0, 10, 1000)
plt.scatter(x, y)
216/37:
x = np.random.randint(0, 50, 1000)
y = x + np.random.normal(0, 10, 1000)
plt.scatter(x, y)

np.corrcoef(x,y)
216/38:
x = np.random.randint(0, 50, 1000)
y = x + np.random.normal(0, 10, 1000)
plt.scatter(x, y)

np.corrcoef(x,y)
216/39:
import seaborn as sns; sns.set()
np.corrcoef(x_train,y_train)
216/40:
import seaborn as sns; sns.set()
np.corrcoef(x_train)
216/41:
import seaborn as sns; sns.set()
corr = np.corrcoef(x_train)
sns.heatmap(corr)
216/42:
import seaborn as sns; sns.set()
corr = np.corrcoef(x_train.T)
sns.heatmap(corr)
216/43:
import seaborn as sns; sns.set()
corr = np.corrcoef(x_train.T)
sns.heatmap(corr,fisize = (4,4))
216/44:
import seaborn as sns; sns.set()
corr = np.corrcoef(x_train.T)
sns.heatmap(corr,fisize = 4)
216/45:
import seaborn as sns; sns.set(rc={'figure.figsize':(11.7,8.27)})
corr = np.corrcoef(x_train.T)
sns.heatmap(corr)
216/46:
import seaborn as sns; sns.set(rc={'figure.figsize':(11,8.5)})
corr = np.corrcoef(x_train.T)
sns.heatmap(corr)
216/47:
import seaborn as sns; sns.set(rc={'figure.figsize':(11,8.5)})
corr = np.corrcoef(x_train.T)
sns.heatmap(corr,cmap='viridis')
216/48:
import seaborn as sns; sns.set(rc={'figure.figsize':(11,8.5)})
corr = np.corrcoef(x_train.T)
sns.heatmap(corr,cmap='hot') # cmap='viridis'
216/49:
import seaborn as sns; sns.set(rc={'figure.figsize':(11,8.5)})
corr = np.corrcoef(x_train.T)
print(corr)
# corr = corr[]
sns.heatmap(corr,cmap='hot') # cmap='viridis', 'hot'
216/50:
import seaborn as sns; sns.set(rc={'figure.figsize':(11,8.5)})
corr = np.corrcoef(x_train.T)
print(corr)
sns.heatmap(corr,cmap='hot') # cmap='viridis', 'hot'
216/51:
import seaborn as sns; sns.set(rc={'figure.figsize':(11,8.5)})
corr = np.corrcoef(x_train.T)
corr = corr[abs(corr)>=.5]
sns.heatmap(corr,cmap='hot') # cmap='viridis', 'hot'
216/52:
import seaborn as sns; sns.set(rc={'figure.figsize':(11,8.5)})
corr = np.corrcoef(x_train.T)
corr = corr[abs(corr)>=.5]
corr
# sns.heatmap(corr,cmap='hot') # cmap='viridis', 'hot'
216/53:
import seaborn as sns; sns.set(rc={'figure.figsize':(11,8.5)})
corr = np.corrcoef(x_train.T)
corr = corr[abs(corr)>=.5]
corr.shape
# sns.heatmap(corr,cmap='hot') # cmap='viridis', 'hot'
216/54:
import seaborn as sns; sns.set(rc={'figure.figsize':(11,8.5)})
corr = np.corrcoef(x_train.T)
corr.shape
corr = corr[abs(corr)>=.5]
# sns.heatmap(corr,cmap='hot') # cmap='viridis', 'hot'
216/55:
import seaborn as sns; sns.set(rc={'figure.figsize':(11,8.5)})
corr = np.corrcoef(x_train.T)
print(corr.shape)
corr = corr[abs(corr)>=.5]
# sns.heatmap(corr,cmap='hot') # cmap='viridis', 'hot'
216/56:
import seaborn as sns; sns.set(rc={'figure.figsize':(11,8.5)})
corr = np.corrcoef(x_train.T)
# corr = corr.
sns.heatmap(corr,cmap='hot',interpolation='nearest') # cmap='viridis', 'hot'
216/57:
import seaborn as sns; sns.set(rc={'figure.figsize':(11,8.5)})
corr = np.corrcoef(x_train.T)
# corr = corr.
sns.heatmap(corr,cmap='hot') # cmap='viridis', 'hot'
216/58:
import seaborn as sns; sns.set(rc={'figure.figsize':(11,8.5)})
corr = np.corrcoef(x_train.T)
# corr = corr.
sns.heatmap(corr,cmap='hot',linewidth=.5) # cmap='viridis', 'hot'
216/59:
import seaborn as sns; sns.set(rc={'figure.figsize':(11,8.5)})
corr = np.corrcoef(x_train.T)
# corr = corr.
sns.heatmap(corr,cmap='hot',linewidth=.1) # cmap='viridis', 'hot'
216/60:
import seaborn as sns; sns.set(rc={'figure.figsize':(11,8.5)})
corr = np.corrcoef(x_train.T)
weak_corr = corr[corr<.5]
corr[weak_corr] = 0
sns.heatmap(corr,cmap='hot',linewidth=.1) # cmap='viridis', 'hot', 'RdYlGn_r'
216/61:
import seaborn as sns; sns.set(rc={'figure.figsize':(11,8.5)})
corr = np.corrcoef(x_train.T)
weak_corr = corr[corr<.5]
corr[weak_corr] 
# sns.heatmap(corr,cmap='hot',linewidth=.1) # cmap='viridis', 'hot', 'RdYlGn_r'
216/62:
import seaborn as sns; sns.set(rc={'figure.figsize':(11,8.5)})
corr = np.corrcoef(x_train.T)
weak_corr = corr[corr<.5]
corr[weak_corr] = 0
# sns.heatmap(corr,cmap='hot',linewidth=.1) # cmap='viridis', 'hot', 'RdYlGn_r'
216/63:
import seaborn as sns; sns.set(rc={'figure.figsize':(11,8.5)})
corr = np.corrcoef(x_train.T)
weak_corr = corr[corr<.5]
# corr[weak_corr] = 0
# sns.heatmap(corr,cmap='hot',linewidth=.1) # cmap='viridis', 'hot', 'RdYlGn_r'
216/64:
import seaborn as sns; sns.set(rc={'figure.figsize':(11,8.5)})
corr = np.corrcoef(x_train.T)
weak_corr = corr[corr<.5]
weak_corr
# corr[weak_corr] = 0
# sns.heatmap(corr,cmap='hot',linewidth=.1) # cmap='viridis', 'hot', 'RdYlGn_r'
216/65:
import seaborn as sns; sns.set(rc={'figure.figsize':(11,8.5)})
corr = np.corrcoef(x_train.T)
weak_corr = corr[corr<.5]
corr
# corr[weak_corr] = 0
# sns.heatmap(corr,cmap='hot',linewidth=.1) # cmap='viridis', 'hot', 'RdYlGn_r'
216/66:
import seaborn as sns; sns.set(rc={'figure.figsize':(11,8.5)})
corr = np.corrcoef(x_train.T)
weak_corr = corr[corr<.5]
weak_corr.shape
# corr[weak_corr] = 0
# sns.heatmap(corr,cmap='hot',linewidth=.1) # cmap='viridis', 'hot', 'RdYlGn_r'
216/67:
import seaborn as sns; sns.set(rc={'figure.figsize':(11,8.5)})
corr = np.corrcoef(x_train.T)
weak_corr = corr * corr<0.5

# corr[weak_corr] = 0
# sns.heatmap(corr,cmap='hot',linewidth=.1) # cmap='viridis', 'hot', 'RdYlGn_r'
216/68:
import seaborn as sns; sns.set(rc={'figure.figsize':(11,8.5)})
corr = np.corrcoef(x_train.T)
weak_corr = corr * corr<0.5
weak_corr.shape

# corr[weak_corr] = 0
# sns.heatmap(corr,cmap='hot',linewidth=.1) # cmap='viridis', 'hot', 'RdYlGn_r'
216/69:
import seaborn as sns; sns.set(rc={'figure.figsize':(11,8.5)})
corr = np.corrcoef(x_train.T)
weak_corr = corr * corr<0.5
weak_corr

# corr[weak_corr] = 0
# sns.heatmap(corr,cmap='hot',linewidth=.1) # cmap='viridis', 'hot', 'RdYlGn_r'
216/70:
import seaborn as sns; sns.set(rc={'figure.figsize':(11,8.5)})
corr = np.corrcoef(x_train.T)
weak_corr =  corr<0.5
weak_corr

# corr[weak_corr] = 0
# sns.heatmap(corr,cmap='hot',linewidth=.1) # cmap='viridis', 'hot', 'RdYlGn_r'
216/71:
import seaborn as sns; sns.set(rc={'figure.figsize':(11,8.5)})
corr = np.corrcoef(x_train.T)
weak_corr =  corr<0.5
weak_corr.shape

# corr[weak_corr] = 0
# sns.heatmap(corr,cmap='hot',linewidth=.1) # cmap='viridis', 'hot', 'RdYlGn_r'
216/72:
import seaborn as sns; sns.set(rc={'figure.figsize':(11,8.5)})
corr = np.corrcoef(x_train.T)
weak_corr =  corr<0.5
corr[weak_corr]

# corr[weak_corr] = 0
# sns.heatmap(corr,cmap='hot',linewidth=.1) # cmap='viridis', 'hot', 'RdYlGn_r'
216/73:
import seaborn as sns; sns.set(rc={'figure.figsize':(11,8.5)})
corr = np.corrcoef(x_train.T)
weak_corr =  corr<0.5
corr[weak_corr].shape

# corr[weak_corr] = 0
# sns.heatmap(corr,cmap='hot',linewidth=.1) # cmap='viridis', 'hot', 'RdYlGn_r'
216/74:
import seaborn as sns; sns.set(rc={'figure.figsize':(11,8.5)})
corr = np.corrcoef(x_train.T)
weak_corr =  corr<0.5
corr[weak_corr] = 0
corr.shape

# corr[weak_corr] = 0
# sns.heatmap(corr,cmap='hot',linewidth=.1) # cmap='viridis', 'hot', 'RdYlGn_r'
216/75:
import seaborn as sns; sns.set(rc={'figure.figsize':(11,8.5)})
corr = np.corrcoef(x_train.T)
weak_corr =  corr<0.5
corr[weak_corr] = 0
corr

# corr[weak_corr] = 0
# sns.heatmap(corr,cmap='hot',linewidth=.1) # cmap='viridis', 'hot', 'RdYlGn_r'
216/76:
import seaborn as sns; sns.set(rc={'figure.figsize':(11,8.5)})
corr = np.corrcoef(x_train.T)
weak_corr =  corr<0.5
corr[corr<0.5] = 0
corr

# corr[weak_corr] = 0
# sns.heatmap(corr,cmap='hot',linewidth=.1) # cmap='viridis', 'hot', 'RdYlGn_r'
216/77:
import seaborn as sns; sns.set(rc={'figure.figsize':(11,8.5)})
corr = np.corrcoef(x_train.T)
weak_corr =  corr<0.5
corr[corr<0.5] = 0
corr.shape

# corr[weak_corr] = 0
# sns.heatmap(corr,cmap='hot',linewidth=.1) # cmap='viridis', 'hot', 'RdYlGn_r'
216/78:
import seaborn as sns; sns.set(rc={'figure.figsize':(11,8.5)})
corr = np.corrcoef(x_train.T)
weak_corr =  corr<0.5
corr[corr<0.5] = 0

sns.heatmap(corr,cmap='hot',linewidth=.1) # cmap='viridis', 'hot', 'RdYlGn_r'
216/79:
import seaborn as sns; sns.set(rc={'figure.figsize':(11,8.5)})
corr = np.corrcoef(x_train.T)
# weak_corr =  corr<0.5
# corr[corr<0.5] = 0

sns.heatmap(corr,cmap='hot',linewidth=.1) # cmap='viridis', 'hot', 'RdYlGn_r'
216/80:
import seaborn as sns; sns.set(rc={'figure.figsize':(11,8.5)})
corr = np.corrcoef(x_train.T)
# weak_corr =  corr<0.5
# corr[corr<0.5] = 0

sns.heatmap(corr,cmap='hot',linewidth=.1,vmax = 1.0,vmin=-1.0) # cmap='viridis', 'hot', 'RdYlGn_r'
216/81:
import seaborn as sns; sns.set(rc={'figure.figsize':(11,8.5)})
corr = np.corrcoef(x_train.T)
weak_corr =  corr<0.5
corr[corr<0.5] = 0

sns.heatmap(corr,cmap='hot',linewidth=.1,vmax = 1.0,vmin=-1.0) # cmap='viridis', 'hot', 'RdYlGn_r'
216/82:
import seaborn as sns; sns.set(rc={'figure.figsize':(11,8.5)})
corr = np.corrcoef(x_train.T)
weak_corr =  corr<0.5
corr[corr<0.5] = 0
palette = sns.diverging_palette(20, 220, n=256)
sns.heatmap(corr,cmap=palette,linewidth=.1,vmax = 1.0,vmin=-1.0) # cmap='viridis', 'hot', 'RdYlGn_r'
216/83:
import seaborn as sns; sns.set(rc={'figure.figsize':(11,8.5)})
corr = np.corrcoef(x_train.T)
weak_corr =  corr<0.5
corr[corr<0.5] = 0

palette = sns.diverging_palette(20, 220, n=256)
sns.heatmap(corr,cmap=palette,linewidth=.1,vmax = 1.0,vmin=-1.0,center = 0) # cmap='viridis', 'hot', 'RdYlGn_r'
216/84:
import seaborn as sns; sns.set(rc={'figure.figsize':(11,8.5)})
corr = np.corrcoef(x_train.T)
# weak_corr =  corr<0.5
# corr[corr<0.5] = 0

palette = sns.diverging_palette(20, 220, n=256)
sns.heatmap(corr,cmap=palette,linewidth=.1,vmax = 1.0,vmin=-1.0,center = 0) # cmap='viridis', 'hot', 'RdYlGn_r'
216/85:
import seaborn as sns; sns.set(rc={'figure.figsize':(11,8.5)})
corr = np.corrcoef(x_train.T)
# weak_corr =  corr<0.5
# corr[corr<0.5] = 0

palette = sns.diverging_palette(20, 220, n=256)
sns.heatmap(corr,cmap=palette,linewidth=.1) # cmap='viridis', 'hot', 'RdYlGn_r'
216/86:
import seaborn as sns; sns.set(rc={'figure.figsize':(11,8.5)})
corr = np.corrcoef(x_train.T)
# weak_corr =  corr<0.5
# corr[corr<0.5] = 0

palette = sns.diverging_palette(20, 220, n=256)
sns.heatmap(corr,cmap=palette,linewidth=.1,center = 0) # cmap='viridis', 'hot', 'RdYlGn_r'
216/87:
import seaborn as sns; sns.set(rc={'figure.figsize':(11,8.5)})
corr = np.corrcoef(x_train.T)
# weak_corr =  corr<0.5
# corr[corr<0.5] = 0
palette = sns.diverging_palette(20, 220, n=256)
sns.heatmap(corr,cmap=palette,linewidth=.1,vmax = 1.0,vmin=-1.0,center = 0) # cmap='viridis', 'hot', 'RdYlGn_r'
corr.argmin
216/88:
import seaborn as sns; sns.set(rc={'figure.figsize':(11,8.5)})
corr = np.corrcoef(x_train.T)
# weak_corr =  corr<0.5
# corr[corr<0.5] = 0
palette = sns.diverging_palette(20, 220, n=256)
sns.heatmap(corr,cmap=palette,linewidth=.1,vmax = 1.0,vmin=-1.0,center = 0) # cmap='viridis', 'hot', 'RdYlGn_r'
corr.argmin()
216/89:
import seaborn as sns; sns.set(rc={'figure.figsize':(11,8.5)})
corr = np.corrcoef(x_train.T)
# weak_corr =  corr<0.5
# corr[corr<0.5] = 0
palette = sns.diverging_palette(20, 220, n=256)
sns.heatmap(corr,cmap=palette,linewidth=.1,vmax = 1.0,vmin=-1.0,center = 0) # cmap='viridis', 'hot', 'RdYlGn_r'
corr
216/90:
import seaborn as sns; sns.set(rc={'figure.figsize':(11,8.5)})
corr = np.corrcoef(x_train.T)
weak_corr =  abs(corr)<0.5
corr[corr<0.5] = 0
palette = sns.diverging_palette(20, 220, n=256)
sns.heatmap(corr,cmap=palette,linewidth=.1,vmax = 1.0,vmin=-1.0,center = 0) # cmap='viridis', 'hot', 'RdYlGn_r'
216/91:
import seaborn as sns; sns.set(rc={'figure.figsize':(11,8.5)})
corr = np.corrcoef(x_train.T)
weak_corr =  abs(corr)>0.5
corr[corr<0.5] = 0

palette = sns.diverging_palette(20, 220, n=256)
sns.heatmap(corr,cmap=palette,linewidth=.1,vmax = 1.0,vmin=-1.0,center = 0) # cmap='viridis', 'hot', 'RdYlGn_r'
216/92:
import seaborn as sns; sns.set(rc={'figure.figsize':(11,8.5)})
corr = np.corrcoef(x_train.T)
weak_corr =  abs(corr)<0.5
corr[corr<0.5] = 0

palette = sns.diverging_palette(20, 220, n=256)
sns.heatmap(corr,cmap=palette,linewidth=.1,vmax = 1.0,vmin=-1.0,center = 0) # cmap='viridis', 'hot', 'RdYlGn_r'
216/93:
import seaborn as sns; sns.set(rc={'figure.figsize':(11,8.5)})
corr = np.corrcoef(x_train.T)
weak_corr =  corr>0.5
corr[corr<0.5] = 0

palette = sns.diverging_palette(20, 220, n=256)
sns.heatmap(corr,cmap=palette,linewidth=.1,vmax = 1.0,vmin=-1.0,center = 0) # cmap='viridis', 'hot', 'RdYlGn_r'
216/94:
import seaborn as sns; sns.set(rc={'figure.figsize':(11,8.5)})
corr = np.corrcoef(x_train.T)

corr[corr>0.5] = 0

palette = sns.diverging_palette(20, 220, n=256)
sns.heatmap(corr,cmap=palette,linewidth=.1,vmax = 1.0,vmin=-1.0,center = 0) # cmap='viridis', 'hot', 'RdYlGn_r'
216/95:
import seaborn as sns; sns.set(rc={'figure.figsize':(11,8.5)})
corr = np.corrcoef(x_train.T)

corr[abs(corr)<0.5] = 0

palette = sns.diverging_palette(20, 220, n=256)
sns.heatmap(corr,cmap=palette,linewidth=.1,vmax = 1.0,vmin=-1.0,center = 0) # cmap='viridis', 'hot', 'RdYlGn_r'
216/96:
import seaborn as sns; sns.set(rc={'figure.figsize':(11,8.5)})
corr = np.corrcoef(x_train.T)

corr[abs(corr)>0.5] = 0

palette = sns.diverging_palette(20, 220, n=256)
sns.heatmap(corr,cmap=palette,linewidth=.1,vmax = 1.0,vmin=-1.0,center = 0) # cmap='viridis', 'hot', 'RdYlGn_r'
216/97:
import seaborn as sns; sns.set(rc={'figure.figsize':(11,8.5)})
corr = np.corrcoef(x_train.T)

corr[abs(corr)<0.5] = 0

palette = sns.diverging_palette(20, 220, n=256)
sns.heatmap(corr,cmap=palette,linewidth=.1,vmax = 1.0,vmin=-1.0,center = 0) # cmap='viridis', 'hot', 'RdYlGn_r'
216/98:
import seaborn as sns; sns.set(rc={'figure.figsize':(11,8.5)})
corr = np.corrcoef(x_train.T)

corr[abs(corr)<0.5] = 0

palette = sns.diverging_palette(20, 220, n=256)
sns.heatmap(corr,cmap=palette,linewidth=.1) # cmap='viridis', 'hot', 'RdYlGn_r'
216/99:
import seaborn as sns; sns.set(rc={'figure.figsize':(11,8.5)})
corr = np.corrcoef(x_train.T)

corr[abs(corr)<0.5] = 0

palette = sns.diverging_palette(20, 220, n=256)
sns.heatmap(corr,cmap=palette,linewidth=.1,vmax = 1.0,vmin=-1.0,center = 0) # cmap='viridis', 'hot', 'RdYlGn_r'
216/100:
import seaborn as sns; sns.set(rc={'figure.figsize':(11,8.5)})
corr = np.corrcoef(x_train.T)

corr[abs(corr)<0.5] = 0

palette = sns.diverging_palette(20, 220, n=256)
sns.heatmap(corr,cmap=palette,linewidth=.1,vmax = 0.3,vmin=-0.3,center = 0) # cmap='viridis', 'hot', 'RdYlGn_r'
216/101:
import seaborn as sns; sns.set(rc={'figure.figsize':(11,8.5)})
corr = np.corrcoef(x_train.T)

corr[abs(corr)<0.5] = 0

palette = sns.diverging_palette(20, 220, n=256)
sns.heatmap(corr,cmap=palette,linewidth=.1,vmax = 1.0,vmin=-1.0,center = 0) # cmap='viridis', 'hot', 'RdYlGn_r'
216/102:
import seaborn as sns; sns.set(rc={'figure.figsize':(11,8.5)})
corr = np.corrcoef(x_train.T)

# corr[abs(corr)<0.5] = 0

palette = sns.diverging_palette(20, 220, n=256)
sns.heatmap(corr,cmap=palette,linewidth=.1,vmax = 1.0,vmin=-1.0,center = 0) # cmap='viridis', 'hot', 'RdYlGn_r'
215/1:
# Install TensorFlow
# !pip install -q tensorflow-gpu==2.0.0-beta1

# try:
#   %tensorflow_version 2.x  # Colab only.
# except Exception:
#   pass
from tensorflow.keras.callbacks import ModelCheckpoint
import tensorflow as tf
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
import numpy as np
import itertools
import wandb
from wandb.keras import WandbCallback
from sklearn.preprocessing import StandardScaler
import shap
print(tf.__version__)
215/2:
# Install TensorFlow
# !pip install -q tensorflow-gpu==2.0.0-beta1

# try:
#   %tensorflow_version 2.x  # Colab only.
# except Exception:
#   pass
import tensorflow as tf
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
import numpy as np
import itertools
import wandb
from wandb.keras import WandbCallback
from tensorflow.keras.callbacks import ModelCheckpoint
from sklearn.preprocessing import StandardScaler
import shap
print(tf.__version__)
215/3:
# Install TensorFlow
# !pip install -q tensorflow-gpu==2.0.0-beta1

# try:
#   %tensorflow_version 2.x  # Colab only.
# except Exception:
#   pass
import tensorflow as tf
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
import numpy as np
import itertools
import wandb
from wandb.keras import WandbCallback
from tensorflow.keras.callbacks import ModelCheckpoint
from sklearn.preprocessing import StandardScaler
import shap
print(tf.__version__)
215/4:
filepath = "mnist_classifier-{epoch:02d}-{val_accuracy:.2f}.h5"  # unique file name that will include the epoch and the validation acc for that epoch
# checkpoint = ModelCheckpoint(f"models\\{filepath}.ckpt", monitor='val_acc', verbose=1, save_best_only=True, mode='max') # saves only the best ones
checkpoint = ModelCheckpoint(f"models\\{filepath}", monitor='val_accuracy',verbose = 1, save_best_only=True, mode='max') # saves only the best ones
215/5:
# Train the model
# with tf.device('/gpu:0'):
r = model.fit(x_train, y_train,validation_data=(x_test, y_test), batch_size = config.batch_size,epochs=config.epochs,callbacks=[WandbCallback(), chekcpoint])
215/6:
# Install TensorFlow
# !pip install -q tensorflow-gpu==2.0.0-beta1

# try:
#   %tensorflow_version 2.x  # Colab only.
# except Exception:
#   pass
import tensorflow as tf
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
import numpy as np
import itertools
import wandb
from wandb.keras import WandbCallback
from tensorflow.keras.callbacks import ModelCheckpoint
from sklearn.preprocessing import StandardScaler
import shap
print(tf.__version__)
215/7:
# Load in the data
mnist = tf.keras.datasets.mnist

(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0

# x_train = scaler.fit_transform(x_train)
# x_test = scaler.fit_transform(x_test)
N,D0,D1 = x_train.shape
print("x_train.shape:", x_train.shape)
215/8:
# scaler = StandardScaler()
hyperparameters = dict(
    dropout=0.2,
    dense0=512,
    learning_rate=0.01,
    batch_size=128,
    epochs=10
)
wandb.init(project="mnist", config = hyperparameters, entity = 'seank')
config = wandb.config
215/9:
# Build the model
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Flatten(input_shape=(D0, D1)))
model.add(tf.keras.layers.Dense(config.dense0, activation='relu'))
model.add(tf.keras.layers.Dropout(config.dropout))
model.add(tf.keras.layers.Dense(10, activation='softmax'))

model.layers
215/10:
# Compile the model
opt = tf.keras.optimizers.Adam(learning_rate=config.learning_rate)
model.compile(optimizer=opt,
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
215/11:
print(x_train.dtype, y_train.dtype)
x_train.mean()
215/12:
filepath = "mnist_classifier-{epoch:02d}-{val_accuracy:.2f}.h5"  # unique file name that will include the epoch and the validation acc for that epoch
# checkpoint = ModelCheckpoint(f"models\\{filepath}.ckpt", monitor='val_acc', verbose=1, save_best_only=True, mode='max') # saves only the best ones
checkpoint = ModelCheckpoint(f"models\\{filepath}", monitor='val_accuracy',verbose = 1, save_best_only=True, mode='max') # saves only the best ones
215/13:
# Train the model
# with tf.device('/gpu:0'):
r = model.fit(x_train, y_train,validation_data=(x_test, y_test), batch_size = config.batch_size,epochs=config.epochs,callbacks=[WandbCallback(), chekcpoint])
215/14:
filepath = "mnist_classifier-{epoch:02d}-{val_accuracy:.2f}.h5"  # unique file name that will include the epoch and the validation acc for that epoch
# checkpoint = ModelCheckpoint(f"models\\{filepath}.ckpt", monitor='val_acc', verbose=1, save_best_only=True, mode='max') # saves only the best ones
checkpoint = ModelCheckpoint(f"models\\{filepath}", monitor='val_accuracy',verbose = 1, save_best_only=True, mode='max') # saves only the best ones
215/15:
# Train the model
# with tf.device('/gpu:0'):
r = model.fit(x_train, y_train,validation_data=(x_test, y_test), batch_size = config.batch_size,epochs=config.epochs,callbacks=[WandbCallback(), chekcpoint])
219/1:
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
# from sklearn.metrics import confusion_matrix, plot_confusion_matrix, plot_roc_curve
from time import time
import numpy as np
print(tf.__version__)
219/2:
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
# from sklearn.metrics import confusion_matrix, plot_confusion_matrix, plot_roc_curve
from time import time
import numpy as np
print(tf.__version__)
219/3:
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
# from sklearn.metrics import confusion_matrix, plot_confusion_matrix, plot_roc_curve
from time import time
import numpy as np
from util_funcs import *
import random

np.random.seed(1)
tf.random.set_random_seed(1)
random.seed(1)
print(tf.__version__)
219/4:
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
# from sklearn.metrics import confusion_matrix, plot_confusion_matrix, plot_roc_curve
from time import time
import numpy as np
from util_funcs import *
import random

np.random.seed(1)
tf.random.set_random_seed(seed=1)
random.seed(1)
print(tf.__version__)
219/5:
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
# from sklearn.metrics import confusion_matrix, plot_confusion_matrix, plot_roc_curve
from time import time
import numpy as np
from util_funcs import *
import random

np.random.seed(1)
tf.random.set_random_seed(seed=1)
random.seed(1)
print(tf.__version__)
220/1:
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
# from sklearn.metrics import confusion_matrix, plot_confusion_matrix, plot_roc_curve
from time import time
import numpy as np
from util_funcs import *
import random

np.random.seed(1)
tf.random.set_random_seed(seed=1)
random.seed(1)
print(tf.__version__)
220/2:
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
# from sklearn.metrics import confusion_matrix, plot_confusion_matrix, plot_roc_curve
from time import time
import numpy as np
from util_funcs import *
import random

np.random.seed(1)
tf.random.set_random_seed(1)
random.seed(1)
print(tf.__version__)
220/3:
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
# from sklearn.metrics import confusion_matrix, plot_confusion_matrix, plot_roc_curve
from time import time
import numpy as np
from util_funcs import *
import random

np.random.seed(1)
tf.random.seed(1)
random.seed(1)
print(tf.__version__)
220/4:
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
# from sklearn.metrics import confusion_matrix, plot_confusion_matrix, plot_roc_curve
from time import time
import numpy as np
from util_funcs import *
import random

np.random.seed(1)
tf.random(1)
random.seed(1)
print(tf.__version__)
220/5:
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
# from sklearn.metrics import confusion_matrix, plot_confusion_matrix, plot_roc_curve
from time import time
import numpy as np
from util_funcs import *
import random

np.random.seed(1)
random.seed(1)
print(tf.__version__)
tf.random.get_seed()
220/6:
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
# from sklearn.metrics import confusion_matrix, plot_confusion_matrix, plot_roc_curve
from time import time
import numpy as np
from util_funcs import *
import random

np.random.seed(1)
random.seed(1)
print(tf.__version__)
218/1:
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
from sklearn.metrics import confusion_matrix, plot_confusion_matrix, plot_roc_curve
from time import time
import numpy as np
from wandb.keras import WandbCallback

print(tf.__version__)
220/7:
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
# from sklearn.metrics import confusion_matrix, plot_confusion_matrix, plot_roc_curve
from time import time
import numpy as np
import random
from util_funcs import *

np.random.seed(1)
random.seed(1)
print(tf.__version__)
220/8:
SYMBOL = 'BTCUSDT'
INTERVAL = '15m'
MODE = 'atr_multiplier'
PARAM = 1

NAME = f"arnold_{symbol+interval}_{int(time.time())}"

scaler_train = MinMaxScaler(feature_range=(0,1))
scaler_test = MinMaxScaler(feature_range=(0,1))
220/9:
# scaler = StandardScaler()
hyperparameters = dict(
    dropout=0.2,
    dense0=512,
    learning_rate=0.01,
    batch_size=128,
    epochs=100,
    testing_pct = .2
    # symbol = SYMBOL,
    # interval = INTERVAL,
    # mode = mode
)
wandb.init(project="arnold", config = hyperparameters, entity = 'seank')
config = wandb.config
220/10:
SYMBOL = 'BTCUSDT'
INTERVAL = '15m'
MODE = 'atr_multiplier'
PARAM = 1

NAME = f"arnold_{SYMBOL+INTERVAL}_{int(time())}"

scaler_train = MinMaxScaler(feature_range=(0,1))
scaler_test = MinMaxScaler(feature_range=(0,1))
220/11:
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
# from sklearn.metrics import confusion_matrix, plot_confusion_matrix, plot_roc_curve
from time import time
import numpy as np
import random
from util_funcs import *

np.random.seed(1)
random.seed(1)
print(tf.__version__)
220/12:
SYMBOL = 'BTCUSDT'
INTERVAL = '15m'
MODE = 'atr_multiplier'
PARAM = 1

NAME = f"arnold_{SYMBOL+INTERVAL}_{int(time())}"

scaler_train = MinMaxScaler(feature_range=(0,1))
scaler_test = MinMaxScaler(feature_range=(0,1))
220/13:
SYMBOL = 'BTCUSDT'
INTERVAL = '15m'
MODE = 'atr_multiplier'
PARAM = 1

NAME = f"arnold_{SYMBOL+INTERVAL}_{int(time.time())}"

scaler_train = MinMaxScaler(feature_range=(0,1))
scaler_test = MinMaxScaler(feature_range=(0,1))
220/14:
SYMBOL = 'BTCUSDT'
INTERVAL = '15m'
MODE = 'atr_multiplier'
PARAM = 1

NAME = f"arnold_{SYMBOL+INTERVAL}_{int(time.time())}"

scaler_train = StandardScaler(feature_range=(0,1))
scaler_test = StandardScaler(feature_range=(0,1))
220/15:
SYMBOL = 'BTCUSDT'
INTERVAL = '15m'
MODE = 'atr_multiplier'
PARAM = 1

NAME = f"arnold_{SYMBOL+INTERVAL}_{int(time.time())}"

scaler_train = MinMaxScaler(feature_range=(0,1))
scaler_test = MinMaxScaler(feature_range=(0,1))
220/16:
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
# from sklearn.metrics import confusion_matrix, plot_confusion_matrix, plot_roc_curve
from time import time
import numpy as np
import random
from util_funcs import *

np.random.seed(1)
random.seed(1)
print(tf.__version__)
220/17:
SYMBOL = 'BTCUSDT'
INTERVAL = '15m'
MODE = 'atr_multiplier'
PARAM = 1

NAME = f"arnold_{SYMBOL+INTERVAL}_{int(time.time())}"

scaler_train = MinMaxScaler(feature_range=(0,1))
scaler_test = MinMaxScaler(feature_range=(0,1))
220/18:
# scaler = StandardScaler()
hyperparameters = dict(
    dropout=0.2,
    dense0=512,
    learning_rate=0.01,
    batch_size=128,
    epochs=100,
    testing_pct = .2
    # symbol = SYMBOL,
    # interval = INTERVAL,
    # mode = mode
)
wandb.init(project="arnold", config = hyperparameters, entity = 'seank')
config = wandb.config
220/19:
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
# from sklearn.metrics import confusion_matrix, plot_confusion_matrix, plot_roc_curve
from time import time
import numpy as np
import random
from util_funcs import *
import wandb

np.random.seed(1)
random.seed(1)
print(tf.__version__)
220/20:
# scaler = StandardScaler()
hyperparameters = dict(
    dropout=0.2,
    dense0=512,
    learning_rate=0.01,
    batch_size=128,
    epochs=100,
    testing_pct = .2
    # symbol = SYMBOL,
    # interval = INTERVAL,
    # mode = mode
)
wandb.init(project="arnold", config = hyperparameters, entity = 'seank')
config = wandb.config
220/21:
[df,features, targets,signals]=read_dfts_pickle(SYMBOL,INTERVAL,MODE,PARAM)
is_good_data(df,1)
220/22:
[df,features, targets,signals]=read_dfts_pickle(SYMBOL,INTERVAL,MODE,PARAM)
is_good_data(df,1)

df
220/23:
SYMBOL = 'BTCUSDT'
INTERVAL = '15m'
MODE = 'atr_multiplier'
PARAM = 1

NAME = f"arnold_{SYMBOL+INTERVAL}_{int(time.time())}"

scaler = MinMaxScaler(feature_range=(0,1))
220/24:
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
# from sklearn.metrics import confusion_matrix, plot_confusion_matrix, plot_roc_curve
from time import time
import numpy as np
import random
from util_funcs import *
import wandb
from wandb.keras import WandbCallback

np.random.seed(1)
random.seed(1)
print(tf.__version__)
220/25:
SYMBOL = 'BTCUSDT'
INTERVAL = '15m'
MODE = 'atr_multiplier'
PARAM = 1

NAME = f"arnold_{SYMBOL+INTERVAL}_{int(time.time())}"

scaler = MinMaxScaler(feature_range=(0,1))

hyperparameters = dict(
    # dropout=0.2,
    dense0=512,
    learning_rate=0.01,
    batch_size=128,
    epochs=100,
    testing_pct = .2
    # symbol = SYMBOL,
    # interval = INTERVAL,
    # mode = mode
)
hyperparameters.dense0
220/26:
SYMBOL = 'BTCUSDT'
INTERVAL = '15m'
MODE = 'atr_multiplier'
PARAM = 1

NAME = f"arnold_{SYMBOL+INTERVAL}_{int(time.time())}"

scaler = MinMaxScaler(feature_range=(0,1))

hyperparameters = dict(
    # dropout=0.2,
    dense0=512,
    learning_rate=0.01,
    batch_size=128,
    epochs=100,
    testing_pct = .2
    # symbol = SYMBOL,
    # interval = INTERVAL,
    # mode = mode
)
hyperparameters['dense0']
220/27:
print(len(df))
hyperparameters = dict(
    # dropout=0.2,
    dense0=512,
    learning_rate=0.01,
    batch_size=128,
    epochs=100,
    testing_pct = .2
    # symbol = SYMBOL,
    # interval = INTERVAL,
    # mode = mode
)

wandb.init(project="arnold", config = hyperparameters, entity = 'seank')
config = wandb.config
220/28:
SYMBOL = 'BTCUSDT'
INTERVAL = '15m'
MODE = 'atr_multiplier'
PARAM = 1

NAME = f"arnold_{SYMBOL+INTERVAL}_{int(time.time())}"

scaler = MinMaxScaler(feature_range=(0,1))

[df,features, targets,signals]=read_dfts_pickle(SYMBOL,INTERVAL,MODE,PARAM)
is_good_data(df,1)
220/29:
print(len(df))
hyperparameters = dict(
    # dropout=0.2,
    dense0=512,
    learning_rate=0.01,
    batch_size=128,
    epochs=100,
    testing_pct = .2
    # symbol = SYMBOL,
    # interval = INTERVAL,
    # mode = mode
)

wandb.init(project="arnold", config = hyperparameters, entity = 'seank')
config = wandb.config
220/30:
print(len(df))
hyperparameters = dict(
    # dropout=0.2,
    dense0=512,
    learning_rate=0.01,
    batch_size=128,
    epochs=100,
    testing_pct = .2
    # symbol = SYMBOL,
    # interval = INTERVAL,
    # mode = mode
)

wandb.init(project="arnold", config = hyperparameters, entity = 'seank')
config = wandb.config
df
220/31:
print(len(df))
hyperparameters = dict(
    # dropout=0.2,
    dense0=512,
    learning_rate=0.01,
    batch_size=128,
    epochs=100,
    testing_pct = .2
    # symbol = SYMBOL,
    # interval = INTERVAL,
    # mode = mode
)

# wandb.init(project="arnold", config = hyperparameters, entity = 'seank')
config = wandb.config
df
220/32:
SYMBOL = 'BTCUSDT'
INTERVAL = '15m'
MODE = 'atr_multiplier'
PARAM = 1
TESTING_PCT = .2

NAME = f"arnold_{SYMBOL+INTERVAL}_{int(time.time())}"

scaler = MinMaxScaler(feature_range=(0,1))

[df,features, targets,signals]=read_dfts_pickle(SYMBOL,INTERVAL,MODE,PARAM)
is_good_data(df,1)
220/33:
print(len(df)*TESTING_PCT)
hyperparameters = dict(
    # dropout=0.2,
    dense0=512,
    learning_rate=0.01,
    batch_size=128,
    epochs=100,
    testing_pct = TESTING_PCT
    # symbol = SYMBOL,
    # interval = INTERVAL,
    # mode = mode
)

# wandb.init(project="arnold", config = hyperparameters, entity = 'seank')
config = wandb.config
df
220/34:
print(int(len(df)*TESTING_PCT+1))
hyperparameters = dict(
    # dropout=0.2,
    dense0=512,
    learning_rate=0.01,
    batch_size=128,
    epochs=100,
    testing_pct = TESTING_PCT
    # symbol = SYMBOL,
    # interval = INTERVAL,
    # mode = mode
)

# wandb.init(project="arnold", config = hyperparameters, entity = 'seank')
config = wandb.config
df
220/35:
print(int(len(df)*TESTING_PCT+1)//10)
hyperparameters = dict(
    # dropout=0.2,
    dense0=512,
    learning_rate=0.01,
    batch_size=128,
    epochs=100,
    testing_pct = TESTING_PCT
    # symbol = SYMBOL,
    # interval = INTERVAL,
    # mode = mode
)

# wandb.init(project="arnold", config = hyperparameters, entity = 'seank')
config = wandb.config
df
220/36:
print(int(len(df)*TESTING_PCT+1)//12)
hyperparameters = dict(
    # dropout=0.2,
    dense0=512,
    learning_rate=0.01,
    batch_size=128,
    epochs=100,
    testing_pct = TESTING_PCT
    # symbol = SYMBOL,
    # interval = INTERVAL,
    # mode = mode
)

# wandb.init(project="arnold", config = hyperparameters, entity = 'seank')
config = wandb.config
df
220/37:
print(int(len(df)*TESTING_PCT+1)//13)
hyperparameters = dict(
    # dropout=0.2,
    dense0=512,
    learning_rate=0.01,
    batch_size=128,
    epochs=100,
    testing_pct = TESTING_PCT
    # symbol = SYMBOL,
    # interval = INTERVAL,
    # mode = mode
)

# wandb.init(project="arnold", config = hyperparameters, entity = 'seank')
config = wandb.config
df
220/38:
print(int(len(df)*TESTING_PCT+1)//15)
hyperparameters = dict(
    # dropout=0.2,
    dense0=512,
    learning_rate=0.01,
    batch_size=128,
    epochs=100,
    testing_pct = TESTING_PCT
    # symbol = SYMBOL,
    # interval = INTERVAL,
    # mode = mode
)

# wandb.init(project="arnold", config = hyperparameters, entity = 'seank')
config = wandb.config
df
220/39:
print(int(len(df)*TESTING_PCT+1)//25)
hyperparameters = dict(
    # dropout=0.2,
    dense0=512,
    learning_rate=0.01,
    batch_size=128,
    epochs=100,
    testing_pct = TESTING_PCT
    # symbol = SYMBOL,
    # interval = INTERVAL,
    # mode = mode
)

# wandb.init(project="arnold", config = hyperparameters, entity = 'seank')
config = wandb.config
df
220/40:
print(int(len(df)*TESTING_PCT+1)//30)
hyperparameters = dict(
    # dropout=0.2,
    dense0=512,
    learning_rate=0.01,
    batch_size=128,
    epochs=100,
    testing_pct = TESTING_PCT
    # symbol = SYMBOL,
    # interval = INTERVAL,
    # mode = mode
)

# wandb.init(project="arnold", config = hyperparameters, entity = 'seank')
config = wandb.config
df
220/41:
print(int(len(df)*TESTING_PCT+1)//31)
hyperparameters = dict(
    # dropout=0.2,
    dense0=512,
    learning_rate=0.01,
    batch_size=128,
    epochs=100,
    testing_pct = TESTING_PCT
    # symbol = SYMBOL,
    # interval = INTERVAL,
    # mode = mode
)

# wandb.init(project="arnold", config = hyperparameters, entity = 'seank')
config = wandb.config
df
220/42:
print(int(len(df)*TESTING_PCT+1)//30)
hyperparameters = dict(
    # dropout=0.2,
    dense0=512,
    learning_rate=0.01,
    batch_size=128,
    epochs=100,
    testing_pct = TESTING_PCT
    # symbol = SYMBOL,
    # interval = INTERVAL,
    # mode = mode
)

# wandb.init(project="arnold", config = hyperparameters, entity = 'seank')
config = wandb.config
df
220/43:
batch_size = int(len(df)*TESTING_PCT+1)//30 + 1
hyperparameters = dict(
    # dropout=0.2,
    dense0=216,
    dense1=128,
    learning_rate=0.01,
    batch_size=batch_size,
    epochs=100,
    testing_pct = TESTING_PCT
    # symbol = SYMBOL,
    # interval = INTERVAL,
    # mode = mode
)

# wandb.init(project="arnold", config = hyperparameters, entity = 'seank')
config = wandb.config
df
batch_size
220/44:
batch_size = int(len(df)*TESTING_PCT+1)//30 + 1
hyperparameters = dict(
    # dropout=0.2,
    dense0=216,
    dense1=128,
    learning_rate=0.01,
    batch_size=batch_size,
    epochs=100,
    testing_pct = TESTING_PCT
    # symbol = SYMBOL,
    # interval = INTERVAL,
    # mode = mode
)

# wandb.init(project="arnold", config = hyperparameters, entity = 'seank')
config = wandb.config
220/45:
# batch_size = int(len(df)*TESTING_PCT+1)//30 + 1
hyperparameters = dict(
    # dropout=0.2,
    dense0=216,
    dense1=128,
    learning_rate=0.01,
    batch_size=256,
    epochs=100,
    testing_pct = TESTING_PCT
    # symbol = SYMBOL,
    # interval = INTERVAL,
    # mode = mode
)

# wandb.init(project="arnold", config = hyperparameters, entity = 'seank')
config = wandb.config
220/46:
# batch_size = int(len(df)*TESTING_PCT+1)//30 + 1
hyperparameters = dict(
    # dropout=0.2,
    dense0=216,
    dense1=128,
    learning_rate=0.01,
    batch_size=256,
    epochs=100,
    testing_pct = TESTING_PCT
    # symbol = SYMBOL,
    # interval = INTERVAL,
    # mode = mode
)

# wandb.init(project="arnold", config = hyperparameters, entity = 'seank')
config = wandb.config
220/47:
# batch_size = int(len(df)*TESTING_PCT+1)//30 + 1
hyperparameters = dict(
    # dropout=0.2,
    dense0=216,
    dense1=128,
    learning_rate=0.01,
    batch_size=256,
    epochs=100,
    testing_pct = TESTING_PCT
    # symbol = SYMBOL,
    # interval = INTERVAL,
    # mode = mode
)

# wandb.init(project="arnold", config = hyperparameters, entity = 'seank')
config = wandb.config
220/48:
# batch_size = int(len(df)*TESTING_PCT+1)//30 + 1
hyperparameters = dict(
    # dropout=0.2,
    dense0=216,
    dense1=128,
    learning_rate=0.01,
    batch_size=256,
    epochs=100,
    testing_pct = TESTING_PCT
    # symbol = SYMBOL,
    # interval = INTERVAL,
    # mode = mode
)

# wandb.init(project="arnold", config = hyperparameters, entity = 'seank')
config = wandb.config
220/49:
# batch_size = int(len(df)*TESTING_PCT+1)//30 + 1
hyperparameters = dict(
    # dropout=0.2,
    dense0=216,
    dense1=128,
    learning_rate=0.01,
    batch_size=256,
    epochs=100,
    testing_pct = TESTING_PCT
    # symbol = SYMBOL,
    # interval = INTERVAL,
    # mode = mode
)

# wandb.init(project="arnold", config = hyperparameters, entity = 'seank')
config = wandb.config
220/50:
# batch_size = int(len(df)*TESTING_PCT+1)//30 + 1
hyperparameters = dict(
    # dropout=0.2,
    dense0=216,
    dense1=128,
    learning_rate=0.01,
    batch_size=256,
    epochs=100,
    testing_pct = TESTING_PCT
    # symbol = SYMBOL,
    # interval = INTERVAL,
    # mode = mode
)

# wandb.init(project="arnold", config = hyperparameters, entity = 'seank')
config = wandb.config
220/51:
# batch_size = int(len(df)*TESTING_PCT+1)//30 + 1
hp = dict(
    # dropout=0.2,
    dense0=216,
    dense1=128,
    learning_rate=0.01,
    batch_size=256,
    epochs=100,
    testing_pct = .2
    # symbol = SYMBOL,
    # interval = INTERVAL,
    # mode = mode
)

# wandb.init(project="arnold", config = hyperparameters, entity = 'seank')
config = wandb.config
220/52:
a = np.array([[3,5,2],[2,7,3]])
a[a!=2]
220/53:
a = np.array([[3,5,2],[2,7,3]])
a
220/54:
a = np.array([[3,5,2],[2,7,3]])
a!=2
220/55:
a = np.array([[3,5,2],[2,7,3]])
a[a!=2]
220/56:
a = np.array([[3,5,2],[2,7,3]])
a[a!=2]=0
220/57:
a = np.array([[3,5,2],[2,7,3]])
a[a!=2]=0
a
220/58:
a = np.array([[3,5,2],[2,7,3]])
np.count_nonzero(a==2)
220/59:
a = np.array([[3,5,2],[2,7,3]])
np.count_nonzero(a!=2)
220/60: signals
220/61: signals[100]
220/62: signals[100:200]
220/63: signals[100:140]
226/1:
a = np.array([1,0,1,0,1])
np.count_nonzero(a)
226/2:
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
# from sklearn.metrics import confusion_matrix, plot_confusion_matrix, plot_roc_curve
from time import time
import numpy as np
import random
from util_funcs import *
import wandb
from wandb.keras import WandbCallback

np.random.seed(1)
random.seed(1)
print(tf.__version__)
226/3:
SYMBOL = 'BTCUSDT'
INTERVAL = '15m'
MODE = 'atr_multiplier'
PARAM = 1
# TESTING_PCT = .2

NAME = f"arnold_{SYMBOL+INTERVAL}_{int(time.time())}"

scaler = MinMaxScaler(feature_range=(0,1))

[df,features, targets,signals]=read_dfts_pickle(SYMBOL,INTERVAL,MODE,PARAM)
is_good_data(df,1)
226/4:
# batch_size = int(len(df)*TESTING_PCT+1)//30 + 1
hp = dict(
    # dropout=0.2,
    dense0=216,
    dense1=128,
    learning_rate=0.01,
    batch_size=256,
    epochs=100,
    testing_pct = .2
    # symbol = SYMBOL,
    # interval = INTERVAL,
    # mode = mode
)

# wandb.init(project="arnold", config = hp, entity = 'seank')
config = wandb.config
226/5:
a = np.array([1,0,1,0,1])
np.count_nonzero(a)
226/6:
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
# from sklearn.metrics import confusion_matrix, plot_confusion_matrix, plot_roc_curve
from time import time
import numpy as np
import random
from util_funcs import *
import wandb
from wandb.keras import WandbCallback

np.random.seed(1)
random.seed(1)
print(tf.__version__)
226/7: print(1)
226/8: print(1+1)
226/9:
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
# from sklearn.metrics import confusion_matrix, plot_confusion_matrix, plot_roc_curve
from time import time
import numpy as np
import random
import wandb
from wandb.keras import WandbCallback
from util_funcs import *

# np.random.seed(1)
# random.seed(1)
# print(tf.__version__)
226/10:
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
# from sklearn.metrics import confusion_matrix, plot_confusion_matrix, plot_roc_curve
from time import time
import numpy as np
import random
import wandb
from wandb.keras import WandbCallback
from util_funcs import *

np.random.seed(1)
# random.seed(1)
# print(tf.__version__)
226/11:
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
# from sklearn.metrics import confusion_matrix, plot_confusion_matrix, plot_roc_curve
from time import time
import numpy as np
import random
import wandb
from wandb.keras import WandbCallback
from util_funcs import *

np.random.seed(1)
random.seed(1)
# print(tf.__version__)
226/12:
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
# from sklearn.metrics import confusion_matrix, plot_confusion_matrix, plot_roc_curve
from time import time
import numpy as np
import random
import wandb
from wandb.keras import WandbCallback
from util_funcs import *

np.random.seed(1)
random.seed(1)
print(tf.__version__)
226/13:
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
# from sklearn.metrics import confusion_matrix, plot_confusion_matrix, plot_roc_curve
from time import time
import numpy as np
import random
import wandb
from wandb.keras import WandbCallback
from util_funcs import *

np.random.seed(1)
random.seed(1)
print(tf.__version__)
226/14:
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
# from sklearn.metrics import confusion_matrix, plot_confusion_matrix, plot_roc_curve
from time import time
import numpy as np
import random
import wandb
from wandb.keras import WandbCallback
from util_funcs import *

np.random.seed(1)
random.seed(1)
print(tf.__version__)
226/15:
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
# from sklearn.metrics import confusion_matrix, plot_confusion_matrix, plot_roc_curve
from time import time
import numpy as np
import random
import wandb
from wandb.keras import WandbCallback
from util_funcs import *

np.random.seed(1)
random.seed(1)
print(tf.__version__)
226/16:
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
# from sklearn.metrics import confusion_matrix, plot_confusion_matrix, plot_roc_curve
from time import time
import numpy as np
import random
import wandb
from wandb.keras import WandbCallback
from util_funcs import *

np.random.seed(1)
random.seed(1)
print(tf.__version__)
226/17:
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
# from sklearn.metrics import confusion_matrix, plot_confusion_matrix, plot_roc_curve
from time import time
import numpy as np
import random
import wandb
from wandb.keras import WandbCallback
from util_funcs import *

np.random.seed(1)
random.seed(1)
print(tf.__version__)
226/18:
SYMBOL = 'BTCUSDT'
INTERVAL = '15m'
MODE = 'atr_multiplier'
PARAM = 1
# TESTING_PCT = .2

NAME = f"arnold_{SYMBOL+INTERVAL}_{int(time.time())}"

scaler = MinMaxScaler(feature_range=(0,1))

[df,features, targets,signals]=read_dfts_pickle(SYMBOL,INTERVAL,MODE,PARAM)
is_good_data(df,1)
226/19:
# batch_size = int(len(df)*TESTING_PCT+1)//30 + 1
hp = dict(
    # dropout=0.2,
    dense0=216,
    dense1=128,
    learning_rate=0.01,
    batch_size=256,
    epochs=100,
    testing_pct = .2
    # symbol = SYMBOL,
    # interval = INTERVAL,
    # mode = mode
)

# wandb.init(project="arnold", config = hp, entity = 'seank')
config = wandb.config
226/20:
a = np.array([1,0,1,0,1])
np.count_nonzero(a)
226/21:
a = np.array([1,0,1,0,1])
np.count_nonzero(!a)
226/22:
a = np.array([1,0,1,0,1])
np.count_nonzero(-a)
226/23:
a = np.array([1,0,1,0,1])
np.count_nonzero(not a)
226/24:
a = np.array([1,0,1,0,1])
np.count_nonzero(a==0)
226/25:
a = np.array([1,0,1,0,1])
np.count_nonzero(a)
226/26:
a = np.array([1,0,1,0,1])
np.count_nonzero(not a)
226/27:
a = np.array([1,0,1,0,1])
np.count_nonzero(not (a))
226/28:
a = np.array([1,0,1,0,1])
np.count_nonzero(a)
232/1:
def load_words():
    with open('words_alpha.txt') as word_file:
        valid_words = set(word_file.read().split())

    return valid_words
232/2:
def load_words():
    with open('words_alpha.txt') as word_file:
        valid_words = set(word_file.read().split())

    return valid_words

words = load_words()
words
232/3:
def load_words():
    with open('words.txt') as word_file:
        valid_words = set(word_file.read().split())

    return valid_words

words = load_words()
words
232/4:
import json
import pandas as pd
232/5:
def load_words(p='words'):
    assert p in ['words','words_alpha','dic']
    if p == 'dic':
        with open('words_dictionary.json','r') as f:
            words = json.load(f)
    else:
        with open(f'{p}.txt') as word_file:
            words = set(word_file.read().split())

    return words

words = load_words()
alpha = load_words('words_alpha')
dic = load_words('dic')
232/6: dic['f']
232/7: dic.keys
232/8: dic.keys()
232/9: alpha
232/10: type(dic.keys())
232/11: words
232/12: sorted(words)
232/13: sorted(alpha)
232/14: sorted(words)
232/15: sorted(alpha)
232/16: sorted(dic.keys())
232/17: sorted(alpha)
232/18:
allow = ['a','e','i','o','u','w','q','y']
number = 592
'a' in alpha
232/19:
allow = ['a','e','i','o','u','w','q','y']
number = 592
'A' in alpha
232/20:
allow = ['a','e','i','o','u','w','q','y']
number = 592
any(alpha.islower())
232/21:
allow = ['a','e','i','o','u','w','q','y']
number = 592
any([a.islower() for a in alpha])
232/22:
allow = ['a','e','i','o','u','w','q','y']
number = 592
all([a.islower() for a in alpha])
232/23:
def load_words(p='words'):
    assert p in ['words','words_alpha','dic']
    if p == 'dic':
        with open('words_dictionary.json','r') as f:
            words = json.load(f)
    else:
        with open(f'{p}.txt') as word_file:
            words = set(word_file.read().split())

    return words

words = load_words()
alpha = load_words('words_alpha')
dic = load_words('dic')

all([a.islower() for a in words]) # all are lower case!
232/24:
# Checks!
all([a.islower() for a in alpha]) # all are lower case!
all(['\'' in a for a in alpha]) # all are lower case!
232/25:
# Checks!
all([a.islower() for a in alpha]) # all are lower case!
any(['\'' in a for a in alpha]) # all are lower case!
232/26:
# Checks!
all([a.islower() for a in alpha]) # all are lower case!
any(["'" in a for a in alpha]) # all are lower case!
232/27:
# Checks!
all([a.islower() for a in alpha]) # all are lower case!
all(["'" in a for a in alpha]) # all are lower case!
232/28:
# Checks!
all([a.islower() for a in alpha]) # all are lower case!
all(["a" in a for a in alpha]) # all are lower case!
232/29:
# Checks!
all([a.islower() for a in alpha]) # all are lower case!
any(["a" in a for a in alpha]) # all are lower case!
232/30:
# Checks!
all([a.islower() for a in alpha]) # all are lower case!
any(["'" in a for a in alpha]) # all are lower case!
232/31:
# Checks!
all([a.islower() for a in alpha]) # all are lower case!
any(["`" in a for a in alpha]) # all are lower case!
232/32:
# Checks!
all([a.islower() for a in alpha]) # all are lower case!
any(["0" in a for a in alpha]) # all are lower case!
232/33:
# Checks!
all([a.islower() for a in alpha]) # all are lower case!
any(["1" in a for a in alpha]) # all are lower case!
232/34:
# Checks!
all([a.islower() for a in alpha]) # all are lower case!
any(["&" in a for a in alpha]) # all are lower case!
232/35:
# # Checks!
# all([a.islower() for a in alpha]) # all are lower case!
# any(["'" in a for a in alpha]) # all are lower case!
num_dic = dict(
    0 = ['s',z]
)
232/36:
# # Checks!
# all([a.islower() for a in alpha]) # all are lower case!
# any(["'" in a for a in alpha]) # all are lower case!
num_dic = dict(
    0 = ['s','z']
)
232/37:
# # Checks!
# all([a.islower() for a in alpha]) # all are lower case!
# any(["'" in a for a in alpha]) # all are lower case!
num_dic = {}
    0 = ['s','z']
}
232/38:
# # Checks!
# all([a.islower() for a in alpha]) # all are lower case!
# any(["'" in a for a in alpha]) # all are lower case!
num_dic = {
    0 = ['s','z']
}
232/39:
# # Checks!
# all([a.islower() for a in alpha]) # all are lower case!
# any(["'" in a for a in alpha]) # all are lower case!
num_dic = {
    0:['s','z']
}
232/40:
# # Checks!
# all([a.islower() for a in alpha]) # all are lower case!
# any(["'" in a for a in alpha]) # all are lower case!
num_dic = {
    0:['s','z']
}
num_dic
232/41:
# # Checks!
# all([a.islower() for a in alpha]) # all are lower case!
# any(["'" in a for a in alpha]) # all are lower case!
num_dic = {
    0:['s','z'],
    1:[''],
    1:[''],
    1:[''],
    1:[''],
    1:[''],
    1:[''],
    
}
num_dic
232/42:
# # Checks!
# all([a.islower() for a in alpha]) # all are lower case!
# any(["'" in a for a in alpha]) # all are lower case!
num_dic = {
    0:['s','z'],
    1:['t','d'],
    2:['n'],
    3:['m'],
    4:['r'],
    5:['l'],
    6:['ch','sh','j'],
    7:['k','g'],
    8:['v','f'],
    9:['p','b']
}
num_dic
232/43:
# # Checks!
# all([a.islower() for a in alpha]) # all are lower case!
# any(["'" in a for a in alpha]) # all are lower case!
num_dic = {
    0:['s','z'],
    1:['t','d'],
    2:['n'],
    3:['m'],
    4:['r'],
    5:['l'],
    6:['ch','sh','j'],
    7:['k','g'],
    8:['v','f'],
    9:['p','b']
}
num_dic[6]
232/44:
allow = ['a','e','i','o','u','w','q','y']
number = 592

number =str(number)
number
232/45:
allow = ['a','e','i','o','u','w','q','y']
number = 592

words = []
number =str(number)
# for i in number:
wrd = 'rd'
[s.translate({ord(i): None for i in wrd}) for s in alpha]
232/46:
allow = ['a','e','i','o','u','w','q','y']
number = 592

words = []
number =str(number)
# for i in number:
wrd = 'rd'
[s.translate({ord(i): None for i in wrd}) for s in alpha].sort(key=len,reverse=True)
232/47:
allow = ['a','e','i','o','u','w','q','y']
number = 592

words = []
number =str(number)
# for i in number:
wrd = 'rd'
words = [s.translate({ord(i): None for i in wrd}) for s in alpha]
words.sort(key=len,reverse=True)
232/48:
allow = ['a','e','i','o','u','w','q','y']
number = 592

words = []
number =str(number)
# for i in number:
wrd = 'rd'
words = [s.translate({ord(i): None for i in wrd}) for s in alpha]
words.sort(key=len,reverse=True)
alpha
232/49:
def load_words(p='words'):
    assert p in ['words','words_alpha','dic']
    if p == 'dic':
        with open('words_dictionary.json','r') as f:
            words = json.load(f)
    else:
        with open(f'{p}.txt') as word_file:
            words = set(word_file.read().split())

    return words

alpha = load_words('words_alpha')
# words = load_words()
# dic = load_words('dic')
232/50:
allow = ['a','e','i','o','u','w','q','y']
number = 592

words = []
number =str(number)
# for i in number:
wrd = 'rd'
words = [s.translate({ord(i): None for i in wrd}) for s in alpha]
words.sort(key=len,reverse=True)
232/51:
def load_words(p='words'):
    assert p in ['words','words_alpha','dic']
    if p == 'dic':
        with open('words_dictionary.json','r') as f:
            words = json.load(f)
    else:
        with open(f'{p}.txt') as word_file:
            words = set(word_file.read().split())

    return words

alpha = sorted(load_words('words_alpha'))
# words = load_words()
# dic = load_words('dic')
232/52:
s = 'abacd'
s.replace('a','')
s
232/53:
s = 'abacd'
s.replace('a',None)
s
232/54:
s = 'abacd'
a = s.replace('a','')
a
232/55:
s = 'abacd'
a = s.replace('a','')
a
232/56:
s = 'abacd'
a = s.replace('e','')
a
232/57:
s = ['abacd','gebca','deer','red']
word = 'red'
case = 'rd'

start = 0
for i in case:
    word.index(i,start)
232/58:
s = ['abacd','gebca','deer','red']
word = 'red'
case = 'rd'

start = 0
for i in case:
    print(word.index(i,start))
232/59:
s = ['abacd','gebca','deer','red']
word = 'red'
case = 'rd'

# start = 0
# def check_word(word, case)
# for i in case:
#     print(word.index(i,start))
#     start = word.index(i,start)

case in words
232/60:
allow = ['a','e','i','o','u','w'] #,'q','y'
number = 592

words = []
number =str(number)
# for i in number:
wrd = 'rd'
words = [s.translate({ord(i): None for i in wrd}) for s in alpha]
words.sort(key=len,reverse=True)
232/61:
import json
import pandas as pd
import re
234/1:
import matplotlib.pyplot as plt
import pandas as pd
234/2:
import matplotlib.pyplot as plt
import pandas as pd
import matplotlib
matplotlib.use('ggplot')
234/3:
import matplotlib.pyplot as plt
import pandas as pd
plt.style.use('ggplot')
234/4:
# 3.
df3 = pd.DataFrame([[10,11,15,14,8,5],[8,7,4,3,9,10],[72,67,81,93,54,66]],columns=['Hours Studied','Hours Watching TV','Examination Score'])
df3
234/5:
# 3.
df3 = pd.DataFrame(([10,11,15,14,8,5],[8,7,4,3,9,10],[72,67,81,93,54,66]),columns=['Hours Studied','Hours Watching TV','Examination Score'])
df3
234/6:
# 3.
df3 = pd.DataFrame(data=[[10,11,15,14,8,5],[8,7,4,3,9,10],[72,67,81,93,54,66]].T,columns=['Hours Studied','Hours Watching TV','Examination Score'])
df3
234/7:
import matplotlib.pyplot as plt
import pandas as pd
import itertools
plt.style.use('ggplot')
234/8:
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import itertools
plt.style.use('ggplot')
234/9:
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
plt.style.use('ggplot')
234/10:
# 3.
data3 = np.array([[10,11,15,14,8,5],[8,7,4,3,9,10],[72,67,81,93,54,66]])
df3 = pd.DataFrame(data3.T,columns=['Hours Studied','Hours Watching TV','Examination Score'])
df3
234/11:
# 3.
data3 = np.array([[10,11,15,14,8,5],[8,7,4,3,9,10],[72,67,81,93,54,66]])
df3 = pd.DataFrame(data3.T,columns=['Hours Studied','Hours Watching TV','Examination Score'])
print(df3)
234/12:
# 3.
data3 = np.array([[10,11,15,14,8,5],[8,7,4,3,9,10],[72,67,81,93,54,66]])
df3 = pd.DataFrame(data3.T,columns=['Hours Studied','Hours Watching TV','Examination Score'])
df3
234/13:
# 3.
data3 = np.array([[10,11,15,14,8,5],[8,7,4,3,9,10],[72,67,81,93,54,66]])
df3 = pd.DataFrame(data3.T,columns=['Hours Studied','Hours Watching TV','Examination Score'],index='Hours Studied')
df3
234/14:
# 3.
data3 = np.array([[10,11,15,14,8,5],[8,7,4,3,9,10],[72,67,81,93,54,66]])
df3 = pd.DataFrame(data3.T,columns=['Hours Studied','Hours Watching TV','Examination Score'],index=['Hours Studied'])
df3
234/15:
# 3.
data3 = np.array([[10,11,15,14,8,5],[8,7,4,3,9,10],[72,67,81,93,54,66]])
df3 = pd.DataFrame(data3.T,columns=['Hours Studied','Hours Watching TV','Examination Score'],index=['Hours Studied'])
df3
234/16:
# 3.
data3 = np.array([[8,7,4,3,9,10],[72,67,81,93,54,66]])
df3 = pd.DataFrame(data3.T,columns=['Hours Studied','Hours Watching TV','Examination Score'],index=[10,11,15,14,8,5])
df3
234/17:
# 3.
data3 = np.array([[10,11,15,14,8,5],[8,7,4,3,9,10],[72,67,81,93,54,66]])
df3 = pd.DataFrame(data3.T,columns=['Hours Studied','Hours Watching TV','Examination Score'],index=None)
df3
234/18:
# 3.
data3 = np.array([[10,11,15,14,8,5],[8,7,4,3,9,10],[72,67,81,93,54,66]])
df3 = pd.DataFrame(data3.T,columns=['Hours Studied','Hours Watching TV','Examination Score'])
df3
234/19: plt.scatter(df3['Hours Studied'],df3['Examination Score'],label = 'Examination Score')
234/20:
plt.scatter(df3['Hours Studied'],df3['Examination Score'],label = 'Examination Score')
plt.legend()
234/21:
plt.scatter(df3['Hours Studied'],df3['Examination Score'],title = 'Hours Studied vs. Examination Score')
plt.legend()
234/22:
plt.scatter(df3['Hours Studied'],df3['Examination Score'],title = 'Hours Studied vs. Examination Score')
# plt.legend()
234/23:
plt.scatter(df3['Hours Studied'],df3['Examination Score'],Title = 'Hours Studied vs. Examination Score')
# plt.legend()
238/1:
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import os 
from sklearn.preprocessing import MinMaxScaler
238/2:
# I got it from https://finance.yahoo.com/quote/GE/history?p=GE&.tsrc=fin-srch

df = pd.read_csv('2- ge.us.txt',delimiter=',',usecols=['Date','Open','High','Low','Close', 'Volume'])
238/3:
# I got it from https://finance.yahoo.com/quote/GE/history?p=GE&.tsrc=fin-srch

df = pd.read_csv('2- ge.us.csv',delimiter=',',usecols=['Date','Open','High','Low','Close', 'Volume'])
238/4:
# Sort DataFrame by date
df = df.sort_values('Date')

# Double check the result
print(df.shape)
df.head()
238/5:
# Sort DataFrame by date
df = df.sort_values('Date')

# Double check the result
print(df.shape)
df.head()
238/6:
plt.figure(figsize = (18,9))
plt.plot(range(df.shape[0]),(df['Low']+df['High'])/2.0)
plt.xticks(range(0,df.shape[0],500),df['Date'].loc[::500],rotation=45)
plt.xlabel('Date',fontsize=18)
plt.ylabel('Mid Price',fontsize=18)
plt.show()
238/7: df['mid'] = (df['Low']+df['High'])/2.0
238/8:
SEQ_LEN = 60  # how long of a preceeding sequence to collect for RNN
FUTURE_PERIOD_PREDICT = 1  # how far into the future are we trying to predict?
RATIO_TO_PREDICT = "mid"
238/9:
SEQ_LEN = 60  # how long of a preceeding sequence to collect for RNN
FUTURE_PERIOD_PREDICT = 1  # how far into the future are we trying to predict?
RATIO_TO_PREDICT = "mid"
238/10:
def classify(current, future):
    if float(future) > float(current):
        return 1
    else:
        return 0
238/11: df['mid'] = (df['Low']+df['High'])/2.0
238/12: df['mid'] = (df['Low']+df['High'])/2.0
238/13: df['mid'] = (df['Low']+df['High'])/2.0
238/14: df['mid'] = (df['Low']+df['High'])/2.0
238/15:
SEQ_LEN = 60  # how long of a preceeding sequence to collect for RNN
FUTURE_PERIOD_PREDICT = 1  # how far into the future are we trying to predict?
RATIO_TO_PREDICT = "mid"
238/16:
def classify(current, future):
    if float(future) > float(current):
        return 1
    else:
        return 0
238/17:
def classify(current, future):
    if float(future) > float(current):
        return 1
    else:
        return 0
241/1:
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import os 
from sklearn.preprocessing import MinMaxScaler
241/2:
# I got it from https://finance.yahoo.com/quote/GE/history?p=GE&.tsrc=fin-srch

df = pd.read_csv('2- ge.us.csv',delimiter=',',usecols=['Date','Open','High','Low','Close', 'Volume'])
241/3:
# Sort DataFrame by date
df = df.sort_values('Date')

# Double check the result
print(df.shape)
df.head()
241/4:
plt.figure(figsize = (18,9))
plt.plot(range(df.shape[0]),(df['Low']+df['High'])/2.0)
plt.xticks(range(0,df.shape[0],500),df['Date'].loc[::500],rotation=45)
plt.xlabel('Date',fontsize=18)
plt.ylabel('Mid Price',fontsize=18)
plt.show()
241/5: df['mid'] = (df['Low']+df['High'])/2.0
241/6:
SEQ_LEN = 60  # how long of a preceeding sequence to collect for RNN
FUTURE_PERIOD_PREDICT = 1  # how far into the future are we trying to predict?
RATIO_TO_PREDICT = "mid"
248/1:
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import os 
from sklearn.preprocessing import MinMaxScaler
248/2:
# I got it from https://finance.yahoo.com/quote/GE/history?p=GE&.tsrc=fin-srch

df = pd.read_csv('2- ge.us.csv',delimiter=',',usecols=['Date','Open','High','Low','Close', 'Volume'])
248/3:
# Sort DataFrame by date
df = df.sort_values('Date')

# Double check the result
print(df.shape)
df.head()
248/4:
plt.figure(figsize = (18,9))
plt.plot(range(df.shape[0]),(df['Low']+df['High'])/2.0)
plt.xticks(range(0,df.shape[0],500),df['Date'].loc[::500],rotation=45)
plt.xlabel('Date',fontsize=18)
plt.ylabel('Mid Price',fontsize=18)
plt.show()
248/5: df['mid'] = (df['Low']+df['High'])/2.0
248/6:
SEQ_LEN = 60  # how long of a preceeding sequence to collect for RNN
FUTURE_PERIOD_PREDICT = 1  # how far into the future are we trying to predict?
RATIO_TO_PREDICT = "mid"
248/7:
SEQ_LEN = 60  # how long of a preceeding sequence to collect for RNN
FUTURE_PERIOD_PREDICT = 1  # how far into the future are we trying to predict?
RATIO_TO_PREDICT = "mid"
248/8:
def classify(current, future):
    if float(future) > float(current):
        return 1
    else:
        return 0
248/9: df['future'] = df[RATIO_TO_PREDICT].shift(-FUTURE_PERIOD_PREDICT)
248/10: df['target'] = list(map(classify, df[RATIO_TO_PREDICT], df['future']))
248/11: df.head()
248/12: df.tail()
248/13:
times = sorted(df.index.values)  # get the times
last_10pct = sorted(df.index.values)[-int(0.1*len(times))]  # get the last 10% of the times
last_20pct = sorted(df.index.values)[-int(0.2*len(times))]  # get the last 20% of the times

test_df = df[(df.index >= last_10pct)]
validation_df = df[(df.index >= last_20pct) & (df.index < last_10pct)]  
train_df = df[(df.index < last_20pct)]  # now the train_df is all the data up to the last 20%
248/14:
from collections import deque
import numpy as np
import random
248/15:
train_df.drop(columns=["Date", "future", 'Open', 'High', 'Low', 'Close', 'Volume'], inplace=True)
validation_df.drop(columns=["Date", "future", 'Open', 'High', 'Low', 'Close', 'Volume'], inplace=True)
test_df.drop(columns=["Date", "future", 'Open', 'High', 'Low', 'Close', 'Volume'], inplace=True)# don't need this anymore.
248/16: train_df.head()
248/17:
train_data = train_df[RATIO_TO_PREDICT].as_matrix()
valid_data = validation_df[RATIO_TO_PREDICT].as_matrix()
test_data = test_df[RATIO_TO_PREDICT].as_matrix()
248/18:
train_data = train_df[RATIO_TO_PREDICT].values
valid_data = validation_df[RATIO_TO_PREDICT].values
test_data = test_df[RATIO_TO_PREDICT].values
248/19:
train_data = train_df[RATIO_TO_PREDICT].values
valid_data = validation_df[RATIO_TO_PREDICT].values
test_data = test_df[RATIO_TO_PREDICT].values
248/20:
train_data = train_df[RATIO_TO_PREDICT].values
valid_data = validation_df[RATIO_TO_PREDICT].values
test_data = test_df[RATIO_TO_PREDICT].values
test_data
248/21:
train_data = train_data.reshape(-1,1)
valid_data = valid_data.reshape(-1,1)
test_data = test_data.reshape(-1,1)
248/22: scaler = MinMaxScaler()
248/23:
# Train the Scaler with training data and smooth data
smoothing_window_size = 2500
for di in range(0,10000,smoothing_window_size):
    scaler.fit(train_data[di:di+smoothing_window_size,:])
    train_data[di:di+smoothing_window_size,:] = scaler.transform(train_data[di:di+smoothing_window_size,:])

# You normalize the last bit of remaining data
scaler.fit(train_data[di+smoothing_window_size:,:])
train_data[di+smoothing_window_size:,:] = scaler.transform(train_data[di+smoothing_window_size:,:])
248/24:
# Reshape both train and test data
train_data = train_data.reshape(-1)

# Normalize test data and validation data
valid_data = scaler.transform(valid_data).reshape(-1)
test_data = scaler.transform(test_data).reshape(-1)
248/25:
# Now perform exponential moving average smoothing
# So the data will have a smoother curve than the original ragged data
EMA = 0.0
gamma = 0.1
for ti in range(11000):
    EMA = gamma*train_data[ti] + (1-gamma)*EMA
    train_data[ti] = EMA

# Used for visualization and test purposes
all_mid_data = np.concatenate([train_data,valid_data, test_data],axis=0)
248/26:
X_train = []
y_train = []
for i in range(SEQ_LEN, len(train_data)):
    X_train.append(train_data[i-SEQ_LEN:i])
    y_train.append(train_data[i + (FUTURE_PERIOD_PREDICT-1)])
X_train, y_train = np.array(X_train), np.array(y_train)

X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))
248/27:
X_valid = []
y_valid = []
for i in range(SEQ_LEN, len(valid_data)):
    X_valid.append(valid_data[i-SEQ_LEN:i])
    y_valid.append(valid_data[i+(FUTURE_PERIOD_PREDICT-1)])
X_valid, y_valid = np.array(X_valid), np.array(y_valid)

X_valid = np.reshape(X_valid, (X_valid.shape[0], X_valid.shape[1], 1))
248/28:
X_test = []
y_test = []
for i in range(SEQ_LEN, len(test_data)):
    X_test.append(test_data[i-SEQ_LEN:i])
    y_test.append(test_data[i+(FUTURE_PERIOD_PREDICT-1)])
    
X_test, y_test = np.array(X_test), np.array(y_test)
X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))
248/29: y_train.shape
248/30: y_valid.shape
248/31:
X_train_2 = []
y_train_2 = []
for i in range(SEQ_LEN, len(train_data)):
    X_train_2.append(train_data[i-SEQ_LEN:i])
    y_train_2.append(train_data[i + (FUTURE_PERIOD_PREDICT-1)])
X_train_2, y_train_2 = np.array(X_train_2), np.array(y_train_2)

X_train_2 = np.reshape(X_train_2, (X_train_2.shape[0], X_train_2.shape[1], 1))
248/32:
## show predictions
plt.figure(figsize=(15, 5))

plt.plot(np.arange(y_train_2.shape[0]), y_train_2, color='blue', label='train target')

plt.plot(np.arange(y_train_2.shape[0], y_train_2.shape[0]+y_valid.shape[0]), y_valid,
         color='gray', label='valid target')

plt.plot(np.arange(y_train_2.shape[0]+y_valid.shape[0],
                   y_train_2.shape[0]+y_valid.shape[0]+y_test.shape[0]),
         y_test, color='black', label='test target')


plt.title('Sparation des donnes')
plt.xlabel('time [days]')
plt.ylabel('normalized price')
plt.legend(loc='best');
248/33:
from sklearn.utils import shuffle
X_train, y_train = shuffle(X_train, y_train)
248/34:
EPOCHS = 10  # how many passes through our data
BATCH_SIZE = 1024  # how many batches? Try smaller batch if you're getting OOM (out of memory) errors.
import time

NAME = f"{SEQ_LEN}-SEQ-{FUTURE_PERIOD_PREDICT}-PRED-{int(time.time())}"  # a unique name for the model
248/35: !pip install -q tensorflow==2.0.0-alpha0
248/36: # !pip install -q tensorflow==2.0.0-alpha0
248/37:
# https://www.kaggle.com/shujian/transformer-with-lstm

import random, os, sys
import numpy as np
from tensorflow.keras.models import *
from tensorflow.keras.layers import *
from tensorflow.keras.callbacks import *
from tensorflow.keras.initializers import *
import tensorflow as tf
from tensorflow.python.keras.layers import Layer

try:
    from dataloader import TokenList, pad_to_longest
    # for transformer
except: pass



embed_size = 60

class LayerNormalization(Layer):
    def __init__(self, eps=1e-6, **kwargs):
        self.eps = eps
        super(LayerNormalization, self).__init__(**kwargs)
    def build(self, input_shape):
        self.gamma = self.add_weight(name='gamma', shape=input_shape[-1:],
                                     initializer=Ones(), trainable=True)
        self.beta = self.add_weight(name='beta', shape=input_shape[-1:],
                                    initializer=Zeros(), trainable=True)
        super(LayerNormalization, self).build(input_shape)
    def call(self, x):
        mean = K.mean(x, axis=-1, keepdims=True)
        std = K.std(x, axis=-1, keepdims=True)
        return self.gamma * (x - mean) / (std + self.eps) + self.beta
    def compute_output_shape(self, input_shape):
        return input_shape

class ScaledDotProductAttention():
    def __init__(self, d_model, attn_dropout=0.1):
        self.temper = np.sqrt(d_model)
        self.dropout = Dropout(attn_dropout)
    def __call__(self, q, k, v, mask):
        attn = Lambda(lambda x:K.batch_dot(x[0],x[1],axes=[2,2])/self.temper)([q, k])
        if mask is not None:
            mmask = Lambda(lambda x:(-1e+10)*(1-x))(mask)
            attn = Add()([attn, mmask])
        attn = Activation('softmax')(attn)
        attn = self.dropout(attn)
        output = Lambda(lambda x:K.batch_dot(x[0], x[1]))([attn, v])
        return output, attn

class MultiHeadAttention():
    # mode 0 - big martixes, faster; mode 1 - more clear implementation
    def __init__(self, n_head, d_model, d_k, d_v, dropout, mode=0, use_norm=True):
        self.mode = mode
        self.n_head = n_head
        self.d_k = d_k
        self.d_v = d_v
        self.dropout = dropout
        if mode == 0:
            self.qs_layer = Dense(n_head*d_k, use_bias=False)
            self.ks_layer = Dense(n_head*d_k, use_bias=False)
            self.vs_layer = Dense(n_head*d_v, use_bias=False)
        elif mode == 1:
            self.qs_layers = []
            self.ks_layers = []
            self.vs_layers = []
            for _ in range(n_head):
                self.qs_layers.append(TimeDistributed(Dense(d_k, use_bias=False)))
                self.ks_layers.append(TimeDistributed(Dense(d_k, use_bias=False)))
                self.vs_layers.append(TimeDistributed(Dense(d_v, use_bias=False)))
        self.attention = ScaledDotProductAttention(d_model)
        self.layer_norm = LayerNormalization() if use_norm else None
        self.w_o = TimeDistributed(Dense(d_model))

    def __call__(self, q, k, v, mask=None):
        d_k, d_v = self.d_k, self.d_v
        n_head = self.n_head

        if self.mode == 0:
            qs = self.qs_layer(q)  # [batch_size, len_q, n_head*d_k]
            ks = self.ks_layer(k)
            vs = self.vs_layer(v)

            def reshape1(x):
                s = tf.shape(x)   # [batch_size, len_q, n_head * d_k]
                x = tf.reshape(x, [s[0], s[1], n_head, d_k])
                x = tf.transpose(x, [2, 0, 1, 3])  
                x = tf.reshape(x, [-1, s[1], d_k])  # [n_head * batch_size, len_q, d_k]
                return x
            qs = Lambda(reshape1)(qs)
            ks = Lambda(reshape1)(ks)
            vs = Lambda(reshape1)(vs)

            if mask is not None:
                mask = Lambda(lambda x:K.repeat_elements(x, n_head, 0))(mask)
            head, attn = self.attention(qs, ks, vs, mask=mask)  
                
            def reshape2(x):
                s = tf.shape(x)   # [n_head * batch_size, len_v, d_v]
                x = tf.reshape(x, [n_head, -1, s[1], s[2]]) 
                x = tf.transpose(x, [1, 2, 0, 3])
                x = tf.reshape(x, [-1, s[1], n_head*d_v])  # [batch_size, len_v, n_head * d_v]
                return x
            head = Lambda(reshape2)(head)
        elif self.mode == 1:
            heads = []; attns = []
            for i in range(n_head):
                qs = self.qs_layers[i](q)   
                ks = self.ks_layers[i](k) 
                vs = self.vs_layers[i](v) 
                head, attn = self.attention(qs, ks, vs, mask)
                heads.append(head); attns.append(attn)
            head = Concatenate()(heads) if n_head > 1 else heads[0]
            attn = Concatenate()(attns) if n_head > 1 else attns[0]

        outputs = self.w_o(head)
        outputs = Dropout(self.dropout)(outputs)
        if not self.layer_norm: return outputs, attn
        # outputs = Add()([outputs, q]) # sl: fix
        return self.layer_norm(outputs), attn

class PositionwiseFeedForward():
    def __init__(self, d_hid, d_inner_hid, dropout=0.1):
        self.w_1 = Conv1D(d_inner_hid, 1, activation='relu')
        self.w_2 = Conv1D(d_hid, 1)
        self.layer_norm = LayerNormalization()
        self.dropout = Dropout(dropout)
    def __call__(self, x):
        output = self.w_1(x) 
        output = self.w_2(output)
        output = self.dropout(output)
        output = Add()([output, x])
        return self.layer_norm(output)

class EncoderLayer():
    def __init__(self, d_model, d_inner_hid, n_head, d_k, d_v, dropout=0.1):
        self.self_att_layer = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)
        self.pos_ffn_layer  = PositionwiseFeedForward(d_model, d_inner_hid, dropout=dropout)
    def __call__(self, enc_input, mask=None):
        output, slf_attn = self.self_att_layer(enc_input, enc_input, enc_input, mask=mask)
        output = self.pos_ffn_layer(output)
        return output, slf_attn


def GetPosEncodingMatrix(max_len, d_emb):
    pos_enc = np.array([
        [pos / np.power(10000, 2 * (j // 2) / d_emb) for j in range(d_emb)] 
        if pos != 0 else np.zeros(d_emb) 
            for pos in range(max_len)
            ])
    pos_enc[1:, 0::2] = np.sin(pos_enc[1:, 0::2]) # dim 2i
    pos_enc[1:, 1::2] = np.cos(pos_enc[1:, 1::2]) # dim 2i+1
    return pos_enc

def GetPadMask(q, k):
    ones = K.expand_dims(K.ones_like(q, 'float32'), -1)
    mask = K.cast(K.expand_dims(K.not_equal(k, 0), 1), 'float32')
    mask = K.batch_dot(ones, mask, axes=[2,1])
    return mask

def GetSubMask(s):
    len_s = tf.shape(s)[1]
    bs = tf.shape(s)[:1]
    mask = K.cumsum(tf.eye(len_s, batch_shape=bs), 1)
    return mask

class Transformer():
    def __init__(self, len_limit, embedding_matrix, d_model=embed_size, \
              d_inner_hid=512, n_head=10, d_k=64, d_v=64, layers=2, dropout=0.1, \
              share_word_emb=False, **kwargs):
        self.name = 'Transformer'
        self.len_limit = len_limit
        self.src_loc_info = False # True # sl: fix later
        self.d_model = d_model
        self.decode_model = None
        d_emb = d_model

        pos_emb = Embedding(len_limit, d_emb, trainable=False, \
                            weights=[GetPosEncodingMatrix(len_limit, d_emb)])

        i_word_emb = Embedding(max_features, d_emb, weights=[embedding_matrix]) # Add Kaggle provided embedding here

        self.encoder = Encoder(d_model, d_inner_hid, n_head, d_k, d_v, layers, dropout, \
                               word_emb=i_word_emb, pos_emb=pos_emb)

        
    def get_pos_seq(self, x):
        mask = K.cast(K.not_equal(x, 0), 'int32')
        pos = K.cumsum(K.ones_like(x, 'int32'), 1)
        return pos * mask

    def compile(self, active_layers=999):
        src_seq_input = Input(shape=(None, ))
        x = Embedding(max_features, embed_size, weights=[embedding_matrix])(src_seq_input)
        
        # LSTM before attention layers
        x = Bidirectional(LSTM(128, return_sequences=True))(x)
        x = Bidirectional(LSTM(64, return_sequences=True))(x) 
        
        x, slf_attn = MultiHeadAttention(n_head=3, d_model=300, d_k=64, d_v=64, dropout=0.1)(x, x, x)
        
        avg_pool = GlobalAveragePooling1D()(x)
        max_pool = GlobalMaxPooling1D()(x)
        conc = concatenate([avg_pool, max_pool])
        conc = Dense(64, activation="relu")(conc)
        x = Dense(1, activation="sigmoid")(conc)   
        
        
        self.model = Model(inputs=src_seq_input, outputs=x)
        self.model.compile(optimizer = 'adam', loss = 'mean_squared_error', metrics=['accuracy'])
248/38:
def build_model():
    inp = Input(shape = (SEQ_LEN, 1))
    
    # LSTM before attention layers
    x = Bidirectional(LSTM(128, return_sequences=True))(inp)
    x = Bidirectional(LSTM(64, return_sequences=True))(x) 
        
    x, slf_attn = MultiHeadAttention(n_head=3, d_model=300, d_k=64, d_v=64, dropout=0.1)(x, x, x)
        
    avg_pool = GlobalAveragePooling1D()(x)
    max_pool = GlobalMaxPooling1D()(x)
    conc = concatenate([avg_pool, max_pool])
    conc = Dense(64, activation="relu")(conc)
    x = Dense(1, activation="sigmoid")(conc)      

    model = Model(inputs = inp, outputs = x)
    model.compile(
        loss = "mean_squared_error", 
        #optimizer = Adam(lr = config["lr"], decay = config["lr_d"]), 
        optimizer = "adam")
    
    # Save entire model to a HDF5 file
    #model.save('my_model.h5')
    
    return model
248/39: multi_head = build_model()
248/40:
# https://www.kaggle.com/shujian/transformer-with-lstm

import random, os, sys
import numpy as np
from tensorflow.keras.models import *
from tensorflow.keras.layers import *
from tensorflow.keras.callbacks import *
from tensorflow.keras.initializers import *
import tensorflow as tf
from tensorflow.python.keras.layers import Layer

try:
    from dataloader import TokenList, pad_to_longest
    # for transformer
except: pass



embed_size = 60

class LayerNormalization(Layer):
    def __init__(self, eps=1e-6, **kwargs):
        self.eps = eps
        super(LayerNormalization, self).__init__(**kwargs)
    def build(self, input_shape):
        self.gamma = self.add_weight(name='gamma', shape=input_shape[-1:],
                                     initializer=Ones(), trainable=True)
        self.beta = self.add_weight(name='beta', shape=input_shape[-1:],
                                    initializer=Zeros(), trainable=True)
        super(LayerNormalization, self).build(input_shape)
    def call(self, x):
        mean = K.mean(x, axis=-1, keepdims=True)
        std = K.std(x, axis=-1, keepdims=True)
        return self.gamma * (x - mean) / (std + self.eps) + self.beta
    def compute_output_shape(self, input_shape):
        return input_shape

class ScaledDotProductAttention():
    def __init__(self, d_model, attn_dropout=0.1):
        self.temper = np.sqrt(d_model)
        self.dropout = Dropout(attn_dropout)
    def __call__(self, q, k, v, mask):
        attn = Lambda(lambda x:K.batch_dot(x[0],x[1],axes=[2,2])/self.temper)([q, k])
        if mask is not None:
            mmask = Lambda(lambda x:(-1e+10)*(1-x))(mask)
            attn = Add()([attn, mmask])
        attn = Activation('softmax')(attn)
        attn = self.dropout(attn)
        output = Lambda(lambda x:K.batch_dot(x[0], x[1]))([attn, v])
        return output, attn

class MultiHeadAttention():
    # mode 0 - big martixes, faster; mode 1 - more clear implementation
    def __init__(self, n_head, d_model, d_k, d_v, dropout, mode=0, use_norm=True):
        self.mode = mode
        self.n_head = n_head
        self.d_k = d_k
        self.d_v = d_v
        self.dropout = dropout
        if mode == 0:
            self.qs_layer = Dense(n_head*d_k, use_bias=False)
            self.ks_layer = Dense(n_head*d_k, use_bias=False)
            self.vs_layer = Dense(n_head*d_v, use_bias=False)
        elif mode == 1:
            self.qs_layers = []
            self.ks_layers = []
            self.vs_layers = []
            for _ in range(n_head):
                self.qs_layers.append(TimeDistributed(Dense(d_k, use_bias=False)))
                self.ks_layers.append(TimeDistributed(Dense(d_k, use_bias=False)))
                self.vs_layers.append(TimeDistributed(Dense(d_v, use_bias=False)))
        self.attention = ScaledDotProductAttention(d_model)
        self.layer_norm = LayerNormalization() if use_norm else None
        self.w_o = TimeDistributed(Dense(d_model))

    def __call__(self, q, k, v, mask=None):
        d_k, d_v = self.d_k, self.d_v
        n_head = self.n_head

        if self.mode == 0:
            qs = self.qs_layer(q)  # [batch_size, len_q, n_head*d_k]
            ks = self.ks_layer(k)
            vs = self.vs_layer(v)

            def reshape1(x):
                s = tf.shape(x)   # [batch_size, len_q, n_head * d_k]
                x = tf.reshape(x, [s[0], s[1], n_head, d_k])
                x = tf.transpose(x, [2, 0, 1, 3])  
                x = tf.reshape(x, [-1, s[1], d_k])  # [n_head * batch_size, len_q, d_k]
                return x
            qs = Lambda(reshape1)(qs)
            ks = Lambda(reshape1)(ks)
            vs = Lambda(reshape1)(vs)

            if mask is not None:
                mask = Lambda(lambda x:K.repeat_elements(x, n_head, 0))(mask)
            head, attn = self.attention(qs, ks, vs, mask=mask)  
                
            def reshape2(x):
                s = tf.shape(x)   # [n_head * batch_size, len_v, d_v]
                x = tf.reshape(x, [n_head, -1, s[1], s[2]]) 
                x = tf.transpose(x, [1, 2, 0, 3])
                x = tf.reshape(x, [-1, s[1], n_head*d_v])  # [batch_size, len_v, n_head * d_v]
                return x
            head = Lambda(reshape2)(head)
        elif self.mode == 1:
            heads = []; attns = []
            for i in range(n_head):
                qs = self.qs_layers[i](q)   
                ks = self.ks_layers[i](k) 
                vs = self.vs_layers[i](v) 
                head, attn = self.attention(qs, ks, vs, mask)
                heads.append(head); attns.append(attn)
            head = Concatenate()(heads) if n_head > 1 else heads[0]
            attn = Concatenate()(attns) if n_head > 1 else attns[0]

        outputs = self.w_o(head)
        outputs = Dropout(self.dropout)(outputs)
        if not self.layer_norm: return outputs, attn
        # outputs = Add()([outputs, q]) # sl: fix
        return self.layer_norm(outputs), attn

class PositionwiseFeedForward():
    def __init__(self, d_hid, d_inner_hid, dropout=0.1):
        self.w_1 = Conv1D(d_inner_hid, 1, activation='relu')
        self.w_2 = Conv1D(d_hid, 1)
        self.layer_norm = LayerNormalization()
        self.dropout = Dropout(dropout)
    def __call__(self, x):
        output = self.w_1(x) 
        output = self.w_2(output)
        output = self.dropout(output)
        output = Add()([output, x])
        return self.layer_norm(output)

class EncoderLayer():
    def __init__(self, d_model, d_inner_hid, n_head, d_k, d_v, dropout=0.1):
        self.self_att_layer = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)
        self.pos_ffn_layer  = PositionwiseFeedForward(d_model, d_inner_hid, dropout=dropout)
    def __call__(self, enc_input, mask=None):
        output, slf_attn = self.self_att_layer(enc_input, enc_input, enc_input, mask=mask)
        output = self.pos_ffn_layer(output)
        return output, slf_attn


def GetPosEncodingMatrix(max_len, d_emb):
    pos_enc = np.array([
        [pos / np.power(10000, 2 * (j // 2) / d_emb) for j in range(d_emb)] 
        if pos != 0 else np.zeros(d_emb) 
            for pos in range(max_len)
            ])
    pos_enc[1:, 0::2] = np.sin(pos_enc[1:, 0::2]) # dim 2i
    pos_enc[1:, 1::2] = np.cos(pos_enc[1:, 1::2]) # dim 2i+1
    return pos_enc

def GetPadMask(q, k):
    ones = K.expand_dims(K.ones_like(q, 'float32'), -1)
    mask = K.cast(K.expand_dims(K.not_equal(k, 0), 1), 'float32')
    mask = K.batch_dot(ones, mask, axes=[2,1])
    return mask

def GetSubMask(s):
    len_s = tf.shape(s)[1]
    bs = tf.shape(s)[:1]
    mask = K.cumsum(tf.eye(len_s, batch_shape=bs), 1)
    return mask

class Transformer():
    def __init__(self, len_limit, embedding_matrix, d_model=embed_size, \
              d_inner_hid=512, n_head=10, d_k=64, d_v=64, layers=2, dropout=0.1, \
              share_word_emb=False, **kwargs):
        self.name = 'Transformer'
        self.len_limit = len_limit
        self.src_loc_info = False # True # sl: fix later
        self.d_model = d_model
        self.decode_model = None
        d_emb = d_model

        pos_emb = Embedding(len_limit, d_emb, trainable=False, \
                            weights=[GetPosEncodingMatrix(len_limit, d_emb)])

        i_word_emb = Embedding(max_features, d_emb, weights=[embedding_matrix]) # Add Kaggle provided embedding here

        self.encoder = Encoder(d_model, d_inner_hid, n_head, d_k, d_v, layers, dropout, \
                               word_emb=i_word_emb, pos_emb=pos_emb)

        
    def get_pos_seq(self, x):
        mask = K.cast(K.not_equal(x, 0), 'int32')
        pos = K.cumsum(K.ones_like(x, 'int32'), 1)
        return pos * mask

    def compile(self, active_layers=999):
        src_seq_input = Input(shape=(None, ))
        x = Embedding(max_features, embed_size, weights=[embedding_matrix])(src_seq_input)
        
        # LSTM before attention layers
        x = Bidirectional(LSTM(128, return_sequences=True))(x)
        x = Bidirectional(LSTM(64, return_sequences=True))(x) 
        
        x, slf_attn = MultiHeadAttention(n_head=3, d_model=300, d_k=64, d_v=64, dropout=0.1)(x, x, x)
        
        avg_pool = GlobalAveragePooling1D()(x)
        max_pool = GlobalMaxPooling1D()(x)
        conc = concatenate([avg_pool, max_pool])
        conc = Dense(64, activation="relu")(conc)
        x = Dense(1, activation="sigmoid")(conc)   
        
        
        self.model = Model(inputs=src_seq_input, outputs=x)
        self.model.compile(optimizer = 'adam', loss = 'mean_squared_error', metrics=['accuracy'])
248/41:
def build_model():
    inp = Input(shape = (SEQ_LEN, 1))
    
    # LSTM before attention layers
    x = Bidirectional(LSTM(128, return_sequences=True))(inp)
    x = Bidirectional(LSTM(64, return_sequences=True))(x) 
        
    x, slf_attn = MultiHeadAttention(n_head=3, d_model=300, d_k=64, d_v=64, dropout=0.1)(x, x, x)
        
    avg_pool = GlobalAveragePooling1D()(x)
    max_pool = GlobalMaxPooling1D()(x)
    conc = concatenate([avg_pool, max_pool])
    conc = Dense(64, activation="relu")(conc)
    x = Dense(1, activation="sigmoid")(conc)      

    model = Model(inputs = inp, outputs = x)
    model.compile(
        loss = "mean_squared_error", 
        #optimizer = Adam(lr = config["lr"], decay = config["lr_d"]), 
        optimizer = "adam")
    
    # Save entire model to a HDF5 file
    #model.save('my_model.h5')
    
    return model
248/42:
def build_model():
    inp = Input(shape = (SEQ_LEN, 1))
    
    # LSTM before attention layers
    x = Bidirectional(LSTM(128, return_sequences=True))(inp)
    x = Bidirectional(LSTM(64, return_sequences=True))(x) 
        
    x, slf_attn = MultiHeadAttention(n_head=3, d_model=300, d_k=64, d_v=64, dropout=0.1)(x, x, x)
        
    avg_pool = GlobalAveragePooling1D()(x)
    max_pool = GlobalMaxPooling1D()(x)
    conc = concatenate([avg_pool, max_pool])
    conc = Dense(64, activation="relu")(conc)
    x = Dense(1, activation="sigmoid")(conc)      

    model = Model(inputs = inp, outputs = x)
    model.compile(
        loss = "mean_squared_error", 
        #optimizer = Adam(lr = config["lr"], decay = config["lr_d"]), 
        optimizer = "adam")
    
    # Save entire model to a HDF5 file
    #model.save('my_model.h5')
    
    return model
248/43:
def build_model():
    inp = Input(shape = (SEQ_LEN, 1))
    
    # LSTM before attention layers
    x = Bidirectional(LSTM(128, return_sequences=True))(inp)
    x = Bidirectional(LSTM(64, return_sequences=True))(x) 
        
    x, slf_attn = MultiHeadAttention(n_head=3, d_model=300, d_k=64, d_v=64, dropout=0.1)(x, x, x)
        
    avg_pool = GlobalAveragePooling1D()(x)
    max_pool = GlobalMaxPooling1D()(x)
    conc = concatenate([avg_pool, max_pool])
    conc = Dense(64, activation="relu")(conc)
    x = Dense(1, activation="sigmoid")(conc)      

    model = Model(inputs = inp, outputs = x)
    model.compile(
        loss = "mean_squared_error", 
        #optimizer = Adam(lr = config["lr"], decay = config["lr_d"]), 
        optimizer = "adam")
    
    # Save entire model to a HDF5 file
    #model.save('my_model.h5')
    
    return model
248/44: multi_head = build_model()
248/45: multi_head = build_model()
248/46: multi_head = build_model()
250/1:
# I got it from https://finance.yahoo.com/quote/GE/history?p=GE&.tsrc=fin-srch
import sys
df = pd.read_csv('2- ge.us.csv',delimiter=',',usecols=['Date','Open','High','Low','Close', 'Volume'])
250/2:
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import os
from sklearn.preprocessing import MinMaxScaler
253/1:
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import os
from sklearn.preprocessing import MinMaxScaler
253/2:
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import os
from sklearn.preprocessing import MinMaxScaler
253/3:
# I got it from https://finance.yahoo.com/quote/GE/history?p=GE&.tsrc=fin-srch
df = pd.read_csv('2- ge.us.csv',delimiter=',',usecols=['Date','Open','High','Low','Close', 'Volume'])
pd.read_csv()
253/4:
# I got it from https://finance.yahoo.com/quote/GE/history?p=GE&.tsrc=fin-srch
df = pd.read_csv('2- ge.us.csv',delimiter=',',usecols=['Date','Open','High','Low','Close', 'Volume'])
253/5:
# I got it from https://finance.yahoo.com/quote/GE/history?p=GE&.tsrc=fin-srch
df = pd.read_csv('2- ge.us.csv',delimiter=',',usecols=['Date','Open','High','Low','Close', 'Volume'])
253/6:
# Sort DataFrame by date
df = df.sort_values('Date')
pd.
# Double check the result
print(df.shape)
df.head()
253/7:
# Sort DataFrame by date
df = df.sort_values('Date')
# Double check the result
print(df.shape)
df.head()
253/8:
plt.figure(figsize = (18,9))
plt.plot(range(df.shape[0]),(df['Low']+df['High'])/2.0)
plt.xticks(range(0,df.shape[0],500),df['Date'].loc[::500],rotation=45)
plt.xlabel('Date',fontsize=18)
plt.ylabel('Mid Price',fontsize=18)
plt.show()
253/9: df['mid'] = (df['Low']+df['High'])/2.0
253/10:
SEQ_LEN = 60  # how long of a preceeding sequence to collect for RNN
FUTURE_PERIOD_PREDICT = 1  # how far into the future are we trying to predict?
RATIO_TO_PREDICT = "mid"
253/11:
def classify(current, future):
    if float(future) > float(current):
        return 1
    else:
        return 0
253/12: df['future'] = df[RATIO_TO_PREDICT].shift(-FUTURE_PERIOD_PREDICT)
253/13: df['target'] = list(map(classify, df[RATIO_TO_PREDICT], df['future']))
253/14: df.head()
253/15: df.tail()
253/16:
times = sorted(df.index.values)  # get the times
last_10pct = sorted(df.index.values)[-int(0.1*len(times))]  # get the last 10% of the times
last_20pct = sorted(df.index.values)[-int(0.2*len(times))]  # get the last 20% of the times

test_df = df[(df.index >= last_10pct)]
validation_df = df[(df.index >= last_20pct) & (df.index < last_10pct)]  
train_df = df[(df.index < last_20pct)]  # now the train_df is all the data up to the last 20%
253/17:
from collections import deque
import numpy as np
import random
253/18:
train_df.drop(columns=["Date", "future", 'Open', 'High', 'Low', 'Close', 'Volume'], inplace=True)
validation_df.drop(columns=["Date", "future", 'Open', 'High', 'Low', 'Close', 'Volume'], inplace=True)
test_df.drop(columns=["Date", "future", 'Open', 'High', 'Low', 'Close', 'Volume'], inplace=True)# don't need this anymore.
253/19: train_df.head()
253/20:
train_data = train_df[RATIO_TO_PREDICT].values
valid_data = validation_df[RATIO_TO_PREDICT].values
test_data = test_df[RATIO_TO_PREDICT].values
test_data
253/21:
train_data = train_data.reshape(-1,1)
valid_data = valid_data.reshape(-1,1)
test_data = test_data.reshape(-1,1)
253/22: scaler = MinMaxScaler()
253/23:
# Train the Scaler with training data and smooth data
smoothing_window_size = 2500
for di in range(0,10000,smoothing_window_size):
    scaler.fit(train_data[di:di+smoothing_window_size,:])
    train_data[di:di+smoothing_window_size,:] = scaler.transform(train_data[di:di+smoothing_window_size,:])

# You normalize the last bit of remaining data
scaler.fit(train_data[di+smoothing_window_size:,:])
train_data[di+smoothing_window_size:,:] = scaler.transform(train_data[di+smoothing_window_size:,:])
253/24:
# Reshape both train and test data
train_data = train_data.reshape(-1)

# Normalize test data and validation data
valid_data = scaler.transform(valid_data).reshape(-1)
test_data = scaler.transform(test_data).reshape(-1)
253/25:
# Now perform exponential moving average smoothing
# So the data will have a smoother curve than the original ragged data
EMA = 0.0
gamma = 0.1
for ti in range(11000):
    EMA = gamma*train_data[ti] + (1-gamma)*EMA
    train_data[ti] = EMA

# Used for visualization and test purposes
all_mid_data = np.concatenate([train_data,valid_data, test_data],axis=0)
253/26:
X_train = []
y_train = []
for i in range(SEQ_LEN, len(train_data)):
    X_train.append(train_data[i-SEQ_LEN:i])
    y_train.append(train_data[i + (FUTURE_PERIOD_PREDICT-1)])
X_train, y_train = np.array(X_train), np.array(y_train)

X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))
253/27:
X_valid = []
y_valid = []
for i in range(SEQ_LEN, len(valid_data)):
    X_valid.append(valid_data[i-SEQ_LEN:i])
    y_valid.append(valid_data[i+(FUTURE_PERIOD_PREDICT-1)])
X_valid, y_valid = np.array(X_valid), np.array(y_valid)

X_valid = np.reshape(X_valid, (X_valid.shape[0], X_valid.shape[1], 1))
253/28:
X_test = []
y_test = []
for i in range(SEQ_LEN, len(test_data)):
    X_test.append(test_data[i-SEQ_LEN:i])
    y_test.append(test_data[i+(FUTURE_PERIOD_PREDICT-1)])
    
X_test, y_test = np.array(X_test), np.array(y_test)
X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))
253/29: y_train.shape
253/30: y_valid.shape
253/31:
X_train_2 = []
y_train_2 = []
for i in range(SEQ_LEN, len(train_data)):
    X_train_2.append(train_data[i-SEQ_LEN:i])
    y_train_2.append(train_data[i + (FUTURE_PERIOD_PREDICT-1)])
X_train_2, y_train_2 = np.array(X_train_2), np.array(y_train_2)

X_train_2 = np.reshape(X_train_2, (X_train_2.shape[0], X_train_2.shape[1], 1))
253/32:
## show predictions
plt.figure(figsize=(15, 5))

plt.plot(np.arange(y_train_2.shape[0]), y_train_2, color='blue', label='train target')

plt.plot(np.arange(y_train_2.shape[0], y_train_2.shape[0]+y_valid.shape[0]), y_valid,
         color='gray', label='valid target')

plt.plot(np.arange(y_train_2.shape[0]+y_valid.shape[0],
                   y_train_2.shape[0]+y_valid.shape[0]+y_test.shape[0]),
         y_test, color='black', label='test target')


plt.title('Sparation des donnes')
plt.xlabel('time [days]')
plt.ylabel('normalized price')
plt.legend(loc='best');
253/33:
from sklearn.utils import shuffle
X_train, y_train = shuffle(X_train, y_train)
253/34:
EPOCHS = 10  # how many passes through our data
BATCH_SIZE = 1024  # how many batches? Try smaller batch if you're getting OOM (out of memory) errors.
import time

NAME = f"{SEQ_LEN}-SEQ-{FUTURE_PERIOD_PREDICT}-PRED-{int(time.time())}"  # a unique name for the model
253/35: # !pip install -q tensorflow==2.0.0-alpha0
253/36:
# https://www.kaggle.com/shujian/transformer-with-lstm

import random, os, sys
import numpy as np
from tensorflow.keras.models import *
from tensorflow.keras.layers import *
from tensorflow.keras.callbacks import *
from tensorflow.keras.initializers import *
import tensorflow as tf
from tensorflow.python.keras.layers import Layer

try:
    from dataloader import TokenList, pad_to_longest
    # for transformer
except: pass



embed_size = 60

class LayerNormalization(Layer):
    def __init__(self, eps=1e-6, **kwargs):
        self.eps = eps
        super(LayerNormalization, self).__init__(**kwargs)
    def build(self, input_shape):
        self.gamma = self.add_weight(name='gamma', shape=input_shape[-1:],
                                     initializer=Ones(), trainable=True)
        self.beta = self.add_weight(name='beta', shape=input_shape[-1:],
                                    initializer=Zeros(), trainable=True)
        super(LayerNormalization, self).build(input_shape)
    def call(self, x):
        mean = K.mean(x, axis=-1, keepdims=True)
        std = K.std(x, axis=-1, keepdims=True)
        return self.gamma * (x - mean) / (std + self.eps) + self.beta
    def compute_output_shape(self, input_shape):
        return input_shape

class ScaledDotProductAttention():
    def __init__(self, d_model, attn_dropout=0.1):
        self.temper = np.sqrt(d_model)
        self.dropout = Dropout(attn_dropout)
    def __call__(self, q, k, v, mask):
        attn = Lambda(lambda x:K.batch_dot(x[0],x[1],axes=[2,2])/self.temper)([q, k])
        if mask is not None:
            mmask = Lambda(lambda x:(-1e+10)*(1-x))(mask)
            attn = Add()([attn, mmask])
        attn = Activation('softmax')(attn)
        attn = self.dropout(attn)
        output = Lambda(lambda x:K.batch_dot(x[0], x[1]))([attn, v])
        return output, attn

class MultiHeadAttention():
    # mode 0 - big martixes, faster; mode 1 - more clear implementation
    def __init__(self, n_head, d_model, d_k, d_v, dropout, mode=0, use_norm=True):
        self.mode = mode
        self.n_head = n_head
        self.d_k = d_k
        self.d_v = d_v
        self.dropout = dropout
        if mode == 0:
            self.qs_layer = Dense(n_head*d_k, use_bias=False)
            self.ks_layer = Dense(n_head*d_k, use_bias=False)
            self.vs_layer = Dense(n_head*d_v, use_bias=False)
        elif mode == 1:
            self.qs_layers = []
            self.ks_layers = []
            self.vs_layers = []
            for _ in range(n_head):
                self.qs_layers.append(TimeDistributed(Dense(d_k, use_bias=False)))
                self.ks_layers.append(TimeDistributed(Dense(d_k, use_bias=False)))
                self.vs_layers.append(TimeDistributed(Dense(d_v, use_bias=False)))
        self.attention = ScaledDotProductAttention(d_model)
        self.layer_norm = LayerNormalization() if use_norm else None
        self.w_o = TimeDistributed(Dense(d_model))

    def __call__(self, q, k, v, mask=None):
        d_k, d_v = self.d_k, self.d_v
        n_head = self.n_head

        if self.mode == 0:
            qs = self.qs_layer(q)  # [batch_size, len_q, n_head*d_k]
            ks = self.ks_layer(k)
            vs = self.vs_layer(v)

            def reshape1(x):
                s = tf.shape(x)   # [batch_size, len_q, n_head * d_k]
                x = tf.reshape(x, [s[0], s[1], n_head, d_k])
                x = tf.transpose(x, [2, 0, 1, 3])  
                x = tf.reshape(x, [-1, s[1], d_k])  # [n_head * batch_size, len_q, d_k]
                return x
            qs = Lambda(reshape1)(qs)
            ks = Lambda(reshape1)(ks)
            vs = Lambda(reshape1)(vs)

            if mask is not None:
                mask = Lambda(lambda x:K.repeat_elements(x, n_head, 0))(mask)
            head, attn = self.attention(qs, ks, vs, mask=mask)  
                
            def reshape2(x):
                s = tf.shape(x)   # [n_head * batch_size, len_v, d_v]
                x = tf.reshape(x, [n_head, -1, s[1], s[2]]) 
                x = tf.transpose(x, [1, 2, 0, 3])
                x = tf.reshape(x, [-1, s[1], n_head*d_v])  # [batch_size, len_v, n_head * d_v]
                return x
            head = Lambda(reshape2)(head)
        elif self.mode == 1:
            heads = []; attns = []
            for i in range(n_head):
                qs = self.qs_layers[i](q)   
                ks = self.ks_layers[i](k) 
                vs = self.vs_layers[i](v) 
                head, attn = self.attention(qs, ks, vs, mask)
                heads.append(head); attns.append(attn)
            head = Concatenate()(heads) if n_head > 1 else heads[0]
            attn = Concatenate()(attns) if n_head > 1 else attns[0]

        outputs = self.w_o(head)
        outputs = Dropout(self.dropout)(outputs)
        if not self.layer_norm: return outputs, attn
        # outputs = Add()([outputs, q]) # sl: fix
        return self.layer_norm(outputs), attn

class PositionwiseFeedForward():
    def __init__(self, d_hid, d_inner_hid, dropout=0.1):
        self.w_1 = Conv1D(d_inner_hid, 1, activation='relu')
        self.w_2 = Conv1D(d_hid, 1)
        self.layer_norm = LayerNormalization()
        self.dropout = Dropout(dropout)
    def __call__(self, x):
        output = self.w_1(x) 
        output = self.w_2(output)
        output = self.dropout(output)
        output = Add()([output, x])
        return self.layer_norm(output)

class EncoderLayer():
    def __init__(self, d_model, d_inner_hid, n_head, d_k, d_v, dropout=0.1):
        self.self_att_layer = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)
        self.pos_ffn_layer  = PositionwiseFeedForward(d_model, d_inner_hid, dropout=dropout)
    def __call__(self, enc_input, mask=None):
        output, slf_attn = self.self_att_layer(enc_input, enc_input, enc_input, mask=mask)
        output = self.pos_ffn_layer(output)
        return output, slf_attn


def GetPosEncodingMatrix(max_len, d_emb):
    pos_enc = np.array([
        [pos / np.power(10000, 2 * (j // 2) / d_emb) for j in range(d_emb)] 
        if pos != 0 else np.zeros(d_emb) 
            for pos in range(max_len)
            ])
    pos_enc[1:, 0::2] = np.sin(pos_enc[1:, 0::2]) # dim 2i
    pos_enc[1:, 1::2] = np.cos(pos_enc[1:, 1::2]) # dim 2i+1
    return pos_enc

def GetPadMask(q, k):
    ones = K.expand_dims(K.ones_like(q, 'float32'), -1)
    mask = K.cast(K.expand_dims(K.not_equal(k, 0), 1), 'float32')
    mask = K.batch_dot(ones, mask, axes=[2,1])
    return mask

def GetSubMask(s):
    len_s = tf.shape(s)[1]
    bs = tf.shape(s)[:1]
    mask = K.cumsum(tf.eye(len_s, batch_shape=bs), 1)
    return mask

class Transformer():
    def __init__(self, len_limit, embedding_matrix, d_model=embed_size, \
              d_inner_hid=512, n_head=10, d_k=64, d_v=64, layers=2, dropout=0.1, \
              share_word_emb=False, **kwargs):
        self.name = 'Transformer'
        self.len_limit = len_limit
        self.src_loc_info = False # True # sl: fix later
        self.d_model = d_model
        self.decode_model = None
        d_emb = d_model

        pos_emb = Embedding(len_limit, d_emb, trainable=False, \
                            weights=[GetPosEncodingMatrix(len_limit, d_emb)])

        i_word_emb = Embedding(max_features, d_emb, weights=[embedding_matrix]) # Add Kaggle provided embedding here

        self.encoder = Encoder(d_model, d_inner_hid, n_head, d_k, d_v, layers, dropout, \
                               word_emb=i_word_emb, pos_emb=pos_emb)

        
    def get_pos_seq(self, x):
        mask = K.cast(K.not_equal(x, 0), 'int32')
        pos = K.cumsum(K.ones_like(x, 'int32'), 1)
        return pos * mask

    def compile(self, active_layers=999):
        src_seq_input = Input(shape=(None, ))
        x = Embedding(max_features, embed_size, weights=[embedding_matrix])(src_seq_input)
        
        # LSTM before attention layers
        x = Bidirectional(LSTM(128, return_sequences=True))(x)
        x = Bidirectional(LSTM(64, return_sequences=True))(x) 
        
        x, slf_attn = MultiHeadAttention(n_head=3, d_model=300, d_k=64, d_v=64, dropout=0.1)(x, x, x)
        
        avg_pool = GlobalAveragePooling1D()(x)
        max_pool = GlobalMaxPooling1D()(x)
        conc = concatenate([avg_pool, max_pool])
        conc = Dense(64, activation="relu")(conc)
        x = Dense(1, activation="sigmoid")(conc)   
        
        
        self.model = Model(inputs=src_seq_input, outputs=x)
        self.model.compile(optimizer = 'adam', loss = 'mean_squared_error', metrics=['accuracy'])
253/37: pd.
253/38:
# https://www.kaggle.com/shujian/transformer-with-lstm

import random, os, sys
import numpy as np
from tensorflow.keras.models import *
from tensorflow.keras.layers import *
from tensorflow.keras.callbacks import *
from tensorflow.keras.initializers import *
import tensorflow as tf
from tensorflow.python.keras.layers import Layer

try:
    from dataloader import TokenList, pad_to_longest
    # for transformer
except: pass



embed_size = 60

class LayerNormalization(Layer):
    def __init__(self, eps=1e-6, **kwargs):
        self.eps = eps
        super(LayerNormalization, self).__init__(**kwargs)
    def build(self, input_shape):
        self.gamma = self.add_weight(name='gamma', shape=input_shape[-1:],
                                     initializer=Ones(), trainable=True)
        self.beta = self.add_weight(name='beta', shape=input_shape[-1:],
                                    initializer=Zeros(), trainable=True)
        super(LayerNormalization, self).build(input_shape)
    def call(self, x):
        mean = K.mean(x, axis=-1, keepdims=True)
        std = K.std(x, axis=-1, keepdims=True)
        return self.gamma * (x - mean) / (std + self.eps) + self.beta
    def compute_output_shape(self, input_shape):
        return input_shape

class ScaledDotProductAttention():
    def __init__(self, d_model, attn_dropout=0.1):
        self.temper = np.sqrt(d_model)
        self.dropout = Dropout(attn_dropout)
    def __call__(self, q, k, v, mask):
        attn = Lambda(lambda x:K.batch_dot(x[0],x[1],axes=[2,2])/self.temper)([q, k])
        if mask is not None:
            mmask = Lambda(lambda x:(-1e+10)*(1-x))(mask)
            attn = Add()([attn, mmask])
        attn = Activation('softmax')(attn)
        attn = self.dropout(attn)
        output = Lambda(lambda x:K.batch_dot(x[0], x[1]))([attn, v])
        return output, attn

class MultiHeadAttention():
    # mode 0 - big martixes, faster; mode 1 - more clear implementation
    def __init__(self, n_head, d_model, d_k, d_v, dropout, mode=0, use_norm=True):
        self.mode = mode
        self.n_head = n_head
        self.d_k = d_k
        self.d_v = d_v
        self.dropout = dropout
        if mode == 0:
            self.qs_layer = Dense(n_head*d_k, use_bias=False)
            self.ks_layer = Dense(n_head*d_k, use_bias=False)
            self.vs_layer = Dense(n_head*d_v, use_bias=False)
        elif mode == 1:
            self.qs_layers = []
            self.ks_layers = []
            self.vs_layers = []
            for _ in range(n_head):
                self.qs_layers.append(TimeDistributed(Dense(d_k, use_bias=False)))
                self.ks_layers.append(TimeDistributed(Dense(d_k, use_bias=False)))
                self.vs_layers.append(TimeDistributed(Dense(d_v, use_bias=False)))
        self.attention = ScaledDotProductAttention(d_model)
        self.layer_norm = LayerNormalization() if use_norm else None
        self.w_o = TimeDistributed(Dense(d_model))

    def __call__(self, q, k, v, mask=None):
        d_k, d_v = self.d_k, self.d_v
        n_head = self.n_head

        if self.mode == 0:
            qs = self.qs_layer(q)  # [batch_size, len_q, n_head*d_k]
            ks = self.ks_layer(k)
            vs = self.vs_layer(v)

            def reshape1(x):
                s = tf.shape(x)   # [batch_size, len_q, n_head * d_k]
                x = tf.reshape(x, [s[0], s[1], n_head, d_k])
                x = tf.transpose(x, [2, 0, 1, 3])  
                x = tf.reshape(x, [-1, s[1], d_k])  # [n_head * batch_size, len_q, d_k]
                return x
            qs = Lambda(reshape1)(qs)
            ks = Lambda(reshape1)(ks)
            vs = Lambda(reshape1)(vs)

            if mask is not None:
                mask = Lambda(lambda x:K.repeat_elements(x, n_head, 0))(mask)
            head, attn = self.attention(qs, ks, vs, mask=mask)  
                
            def reshape2(x):
                s = tf.shape(x)   # [n_head * batch_size, len_v, d_v]
                x = tf.reshape(x, [n_head, -1, s[1], s[2]]) 
                x = tf.transpose(x, [1, 2, 0, 3])
                x = tf.reshape(x, [-1, s[1], n_head*d_v])  # [batch_size, len_v, n_head * d_v]
                return x
            head = Lambda(reshape2)(head)
        elif self.mode == 1:
            heads = []; attns = []
            for i in range(n_head):
                qs = self.qs_layers[i](q)   
                ks = self.ks_layers[i](k) 
                vs = self.vs_layers[i](v) 
                head, attn = self.attention(qs, ks, vs, mask)
                heads.append(head); attns.append(attn)
            head = Concatenate()(heads) if n_head > 1 else heads[0]
            attn = Concatenate()(attns) if n_head > 1 else attns[0]

        outputs = self.w_o(head)
        outputs = Dropout(self.dropout)(outputs)
        if not self.layer_norm: return outputs, attn
        # outputs = Add()([outputs, q]) # sl: fix
        return self.layer_norm(outputs), attn

class PositionwiseFeedForward():
    def __init__(self, d_hid, d_inner_hid, dropout=0.1):
        self.w_1 = Conv1D(d_inner_hid, 1, activation='relu')
        self.w_2 = Conv1D(d_hid, 1)
        self.layer_norm = LayerNormalization()
        self.dropout = Dropout(dropout)
    def __call__(self, x):
        output = self.w_1(x) 
        output = self.w_2(output)
        output = self.dropout(output)
        output = Add()([output, x])
        return self.layer_norm(output)

class EncoderLayer():
    def __init__(self, d_model, d_inner_hid, n_head, d_k, d_v, dropout=0.1):
        self.self_att_layer = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)
        self.pos_ffn_layer  = PositionwiseFeedForward(d_model, d_inner_hid, dropout=dropout)
    def __call__(self, enc_input, mask=None):
        output, slf_attn = self.self_att_layer(enc_input, enc_input, enc_input, mask=mask)
        output = self.pos_ffn_layer(output)
        return output, slf_attn


def GetPosEncodingMatrix(max_len, d_emb):
    pos_enc = np.array([
        [pos / np.power(10000, 2 * (j // 2) / d_emb) for j in range(d_emb)] 
        if pos != 0 else np.zeros(d_emb) 
            for pos in range(max_len)
            ])
    pos_enc[1:, 0::2] = np.sin(pos_enc[1:, 0::2]) # dim 2i
    pos_enc[1:, 1::2] = np.cos(pos_enc[1:, 1::2]) # dim 2i+1
    return pos_enc

def GetPadMask(q, k):
    ones = K.expand_dims(K.ones_like(q, 'float32'), -1)
    mask = K.cast(K.expand_dims(K.not_equal(k, 0), 1), 'float32')
    mask = K.batch_dot(ones, mask, axes=[2,1])
    return mask

def GetSubMask(s):
    len_s = tf.shape(s)[1]
    bs = tf.shape(s)[:1]
    mask = K.cumsum(tf.eye(len_s, batch_shape=bs), 1)
    return mask

class Transformer():
    def __init__(self, len_limit, embedding_matrix, d_model=embed_size, \
              d_inner_hid=512, n_head=10, d_k=64, d_v=64, layers=2, dropout=0.1, \
              share_word_emb=False, **kwargs):
        self.name = 'Transformer'
        self.len_limit = len_limit
        self.src_loc_info = False # True # sl: fix later
        self.d_model = d_model
        self.decode_model = None
        d_emb = d_model

        pos_emb = Embedding(len_limit, d_emb, trainable=False, \
                            weights=[GetPosEncodingMatrix(len_limit, d_emb)])

        i_word_emb = Embedding(max_features, d_emb, weights=[embedding_matrix]) # Add Kaggle provided embedding here

        self.encoder = Encoder(d_model, d_inner_hid, n_head, d_k, d_v, layers, dropout, \
                               word_emb=i_word_emb, pos_emb=pos_emb)

        
    def get_pos_seq(self, x):
        mask = K.cast(K.not_equal(x, 0), 'int32')
        pos = K.cumsum(K.ones_like(x, 'int32'), 1)
        return pos * mask

    def compile(self, active_layers=999):
        src_seq_input = Input(shape=(None, ))
        x = Embedding(max_features, embed_size, weights=[embedding_matrix])(src_seq_input)
        
        # LSTM before attention layers
        x = Bidirectional(LSTM(128, return_sequences=True))(x)
        x = Bidirectional(LSTM(64, return_sequences=True))(x) 
        
        x, slf_attn = MultiHeadAttention(n_head=3, d_model=300, d_k=64, d_v=64, dropout=0.1)(x, x, x)
        
        avg_pool = GlobalAveragePooling1D()(x)
        max_pool = GlobalMaxPooling1D()(x)
        conc = concatenate([avg_pool, max_pool])
        conc = Dense(64, activation="relu")(conc)
        x = Dense(1, activation="sigmoid")(conc)   
        
        
        self.model = Model(inputs=src_seq_input, outputs=x)
        self.model.compile(optimizer = 'adam', loss = 'mean_squared_error', metrics=['accuracy'])
253/39:
def build_model():
    inp = Input(shape = (SEQ_LEN, 1))
    
    # LSTM before attention layers
    x = Bidirectional(LSTM(128, return_sequences=True))(inp)
    x = Bidirectional(LSTM(64, return_sequences=True))(x) 
        
    x, slf_attn = MultiHeadAttention(n_head=3, d_model=300, d_k=64, d_v=64, dropout=0.1)(x, x, x)
        
    avg_pool = GlobalAveragePooling1D()(x)
    max_pool = GlobalMaxPooling1D()(x)
    conc = concatenate([avg_pool, max_pool])
    conc = Dense(64, activation="relu")(conc)
    x = Dense(1, activation="sigmoid")(conc)      

    model = Model(inputs = inp, outputs = x)
    model.compile(
        loss = "mean_squared_error", 
        #optimizer = Adam(lr = config["lr"], decay = config["lr_d"]), 
        optimizer = "adam")
    
    # Save entire model to a HDF5 file
    #model.save('my_model.h5')
    
    return model
253/40:
def build_model():
    inp = Input(shape = (SEQ_LEN, 1))
    
    # LSTM before attention layers
    x = Bidirectional(LSTM(128, return_sequences=True))(inp)
    x = Bidirectional(LSTM(64, return_sequences=True))(x) 
        
    x, slf_attn = MultiHeadAttention(n_head=3, d_model=300, d_k=64, d_v=64, dropout=0.1)(x, x, x)
        
    avg_pool = GlobalAveragePooling1D()(x)
    max_pool = GlobalMaxPooling1D()(x)
    conc = concatenate([avg_pool, max_pool])
    conc = Dense(64, activation="relu")(conc)
    x = Dense(1, activation="sigmoid")(conc)      

    model = Model(inputs = inp, outputs = x)
    model.compile(
        loss = "mean_squared_error", 
        #optimizer = Adam(lr = config["lr"], decay = config["lr_d"]), 
        optimizer = "adam")
    
    # Save entire model to a HDF5 file
    #model.save('my_model.h5')
    
    return model
253/41: multi_head = build_model()
256/1: import teansorflow as tf
256/2: import tensorflow as tf
256/3:
import tensorflow as tf
tf.__version__
257/1:
import tensorflow as tf
tf.__version__
257/2:
import tensorflow as tf
tf.__version__
257/3:
import tensorflow as tf
tf.__version__
257/4:
import tensorflow as tf
tf.__version__
257/5:
import tensorflow as tf
tf.__version__
257/6:
import tensorflow as tf
tf.__version__
257/7:
import tensorflow as tf
tf.__version__
257/8:
import tensorflow as tf
tf.__version__
259/1: import tensorflow
259/2: 
259/3: 
259/4: 
258/1: import pandas as pd
258/2: %config IPCompleter.greedy=True
258/3: %config IPCompleter.greedy=True
258/4: a=3
258/5: a=3
260/1: import tensorflow
263/1:
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import os
from sklearn.preprocessing import MinMaxScaler
263/2:
# I got it from https://finance.yahoo.com/quote/GE/history?p=GE&.tsrc=fin-srch
df = pd.read_csv('2- ge.us.csv',delimiter=',',usecols=['Date','Open','High','Low','Close', 'Volume'])
263/3:
# I got it from https://finance.yahoo.com/quote/GE/history?p=GE&.tsrc=fin-srch
df = pd.read_csv('2- ge.us.csv',delimiter=',',usecols=['Date','Open','High','Low','Close', 'Volume'])
263/4:
# Sort DataFrame by date
df = df.sort_values('Date')
# Double check the result
print(df.shape)
df.head()
263/5:
plt.figure(figsize = (18,9))
plt.plot(range(df.shape[0]),(df['Low']+df['High'])/2.0)
plt.xticks(range(0,df.shape[0],500),df['Date'].loc[::500],rotation=45)
plt.xlabel('Date',fontsize=18)
plt.ylabel('Mid Price',fontsize=18)
plt.show()
263/6: df['mid'] = (df['Low']+df['High'])/2.0
263/7:
SEQ_LEN = 60  # how long of a preceeding sequence to collect for RNN
FUTURE_PERIOD_PREDICT = 1  # how far into the future are we trying to predict?
RATIO_TO_PREDICT = "mid"
263/8:
def classify(current, future):
    if float(future) > float(current):
        return 1
    else:
        return 0
263/9: df['future'] = df[RATIO_TO_PREDICT].shift(-FUTURE_PERIOD_PREDICT)
263/10: df['target'] = list(map(classify, df[RATIO_TO_PREDICT], df['future']))
263/11: df.head()
263/12: df.tail()
263/13:
times = sorted(df.index.values)  # get the times
last_10pct = sorted(df.index.values)[-int(0.1*len(times))]  # get the last 10% of the times
last_20pct = sorted(df.index.values)[-int(0.2*len(times))]  # get the last 20% of the times

test_df = df[(df.index >= last_10pct)]
validation_df = df[(df.index >= last_20pct) & (df.index < last_10pct)]  
train_df = df[(df.index < last_20pct)]  # now the train_df is all the data up to the last 20%
263/14:
from collections import deque
import numpy as np
import random
263/15:
train_df.drop(columns=["Date", "future", 'Open', 'High', 'Low', 'Close', 'Volume'], inplace=True)
validation_df.drop(columns=["Date", "future", 'Open', 'High', 'Low', 'Close', 'Volume'], inplace=True)
test_df.drop(columns=["Date", "future", 'Open', 'High', 'Low', 'Close', 'Volume'], inplace=True)# don't need this anymore.
263/16: train_df.head()
263/17:
train_data = train_df[RATIO_TO_PREDICT].values
valid_data = validation_df[RATIO_TO_PREDICT].values
test_data = test_df[RATIO_TO_PREDICT].values
test_data
263/18:
train_data = train_data.reshape(-1,1)
valid_data = valid_data.reshape(-1,1)
test_data = test_data.reshape(-1,1)
263/19: scaler = MinMaxScaler()
263/20:
# Train the Scaler with training data and smooth data
smoothing_window_size = 2500
for di in range(0,10000,smoothing_window_size):
    scaler.fit(train_data[di:di+smoothing_window_size,:])
    train_data[di:di+smoothing_window_size,:] = scaler.transform(train_data[di:di+smoothing_window_size,:])

# You normalize the last bit of remaining data
scaler.fit(train_data[di+smoothing_window_size:,:])
train_data[di+smoothing_window_size:,:] = scaler.transform(train_data[di+smoothing_window_size:,:])
263/21:
# Reshape both train and test data
train_data = train_data.reshape(-1)

# Normalize test data and validation data
valid_data = scaler.transform(valid_data).reshape(-1)
test_data = scaler.transform(test_data).reshape(-1)
263/22:
# Now perform exponential moving average smoothing
# So the data will have a smoother curve than the original ragged data
EMA = 0.0
gamma = 0.1
for ti in range(11000):
    EMA = gamma*train_data[ti] + (1-gamma)*EMA
    train_data[ti] = EMA

# Used for visualization and test purposes
all_mid_data = np.concatenate([train_data,valid_data, test_data],axis=0)
263/23:
X_train = []
y_train = []
for i in range(SEQ_LEN, len(train_data)):
    X_train.append(train_data[i-SEQ_LEN:i])
    y_train.append(train_data[i + (FUTURE_PERIOD_PREDICT-1)])
X_train, y_train = np.array(X_train), np.array(y_train)

X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))
263/24:
X_valid = []
y_valid = []
for i in range(SEQ_LEN, len(valid_data)):
    X_valid.append(valid_data[i-SEQ_LEN:i])
    y_valid.append(valid_data[i+(FUTURE_PERIOD_PREDICT-1)])
X_valid, y_valid = np.array(X_valid), np.array(y_valid)

X_valid = np.reshape(X_valid, (X_valid.shape[0], X_valid.shape[1], 1))
263/25:
X_test = []
y_test = []
for i in range(SEQ_LEN, len(test_data)):
    X_test.append(test_data[i-SEQ_LEN:i])
    y_test.append(test_data[i+(FUTURE_PERIOD_PREDICT-1)])
    
X_test, y_test = np.array(X_test), np.array(y_test)
X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))
263/26: y_train.shape
263/27: y_valid.shape
263/28:
X_train_2 = []
y_train_2 = []
for i in range(SEQ_LEN, len(train_data)):
    X_train_2.append(train_data[i-SEQ_LEN:i])
    y_train_2.append(train_data[i + (FUTURE_PERIOD_PREDICT-1)])
X_train_2, y_train_2 = np.array(X_train_2), np.array(y_train_2)

X_train_2 = np.reshape(X_train_2, (X_train_2.shape[0], X_train_2.shape[1], 1))
263/29:
## show predictions
plt.figure(figsize=(15, 5))

plt.plot(np.arange(y_train_2.shape[0]), y_train_2, color='blue', label='train target')

plt.plot(np.arange(y_train_2.shape[0], y_train_2.shape[0]+y_valid.shape[0]), y_valid,
         color='gray', label='valid target')

plt.plot(np.arange(y_train_2.shape[0]+y_valid.shape[0],
                   y_train_2.shape[0]+y_valid.shape[0]+y_test.shape[0]),
         y_test, color='black', label='test target')


plt.title('Sparation des donnes')
plt.xlabel('time [days]')
plt.ylabel('normalized price')
plt.legend(loc='best');
263/30:
from sklearn.utils import shuffle
X_train, y_train = shuffle(X_train, y_train)
263/31:
EPOCHS = 10  # how many passes through our data
BATCH_SIZE = 1024  # how many batches? Try smaller batch if you're getting OOM (out of memory) errors.
import time

NAME = f"{SEQ_LEN}-SEQ-{FUTURE_PERIOD_PREDICT}-PRED-{int(time.time())}"  # a unique name for the model
263/32: # !pip install -q tensorflow==2.0.0-alpha0
263/33:
# https://www.kaggle.com/shujian/transformer-with-lstm

import random, os, sys
import numpy as np
from tensorflow.keras.models import *
from tensorflow.keras.layers import *
from tensorflow.keras.callbacks import *
from tensorflow.keras.initializers import *
import tensorflow as tf
from tensorflow.python.keras.layers import Layer

try:
    from dataloader import TokenList, pad_to_longest
    # for transformer
except: pass



embed_size = 60

class LayerNormalization(Layer):
    def __init__(self, eps=1e-6, **kwargs):
        self.eps = eps
        super(LayerNormalization, self).__init__(**kwargs)
    def build(self, input_shape):
        self.gamma = self.add_weight(name='gamma', shape=input_shape[-1:],
                                     initializer=Ones(), trainable=True)
        self.beta = self.add_weight(name='beta', shape=input_shape[-1:],
                                    initializer=Zeros(), trainable=True)
        super(LayerNormalization, self).build(input_shape)
    def call(self, x):
        mean = K.mean(x, axis=-1, keepdims=True)
        std = K.std(x, axis=-1, keepdims=True)
        return self.gamma * (x - mean) / (std + self.eps) + self.beta
    def compute_output_shape(self, input_shape):
        return input_shape

class ScaledDotProductAttention():
    def __init__(self, d_model, attn_dropout=0.1):
        self.temper = np.sqrt(d_model)
        self.dropout = Dropout(attn_dropout)
    def __call__(self, q, k, v, mask):
        attn = Lambda(lambda x:K.batch_dot(x[0],x[1],axes=[2,2])/self.temper)([q, k])
        if mask is not None:
            mmask = Lambda(lambda x:(-1e+10)*(1-x))(mask)
            attn = Add()([attn, mmask])
        attn = Activation('softmax')(attn)
        attn = self.dropout(attn)
        output = Lambda(lambda x:K.batch_dot(x[0], x[1]))([attn, v])
        return output, attn

class MultiHeadAttention():
    # mode 0 - big martixes, faster; mode 1 - more clear implementation
    def __init__(self, n_head, d_model, d_k, d_v, dropout, mode=0, use_norm=True):
        self.mode = mode
        self.n_head = n_head
        self.d_k = d_k
        self.d_v = d_v
        self.dropout = dropout
        if mode == 0:
            self.qs_layer = Dense(n_head*d_k, use_bias=False)
            self.ks_layer = Dense(n_head*d_k, use_bias=False)
            self.vs_layer = Dense(n_head*d_v, use_bias=False)
        elif mode == 1:
            self.qs_layers = []
            self.ks_layers = []
            self.vs_layers = []
            for _ in range(n_head):
                self.qs_layers.append(TimeDistributed(Dense(d_k, use_bias=False)))
                self.ks_layers.append(TimeDistributed(Dense(d_k, use_bias=False)))
                self.vs_layers.append(TimeDistributed(Dense(d_v, use_bias=False)))
        self.attention = ScaledDotProductAttention(d_model)
        self.layer_norm = LayerNormalization() if use_norm else None
        self.w_o = TimeDistributed(Dense(d_model))

    def __call__(self, q, k, v, mask=None):
        d_k, d_v = self.d_k, self.d_v
        n_head = self.n_head

        if self.mode == 0:
            qs = self.qs_layer(q)  # [batch_size, len_q, n_head*d_k]
            ks = self.ks_layer(k)
            vs = self.vs_layer(v)

            def reshape1(x):
                s = tf.shape(x)   # [batch_size, len_q, n_head * d_k]
                x = tf.reshape(x, [s[0], s[1], n_head, d_k])
                x = tf.transpose(x, [2, 0, 1, 3])  
                x = tf.reshape(x, [-1, s[1], d_k])  # [n_head * batch_size, len_q, d_k]
                return x
            qs = Lambda(reshape1)(qs)
            ks = Lambda(reshape1)(ks)
            vs = Lambda(reshape1)(vs)

            if mask is not None:
                mask = Lambda(lambda x:K.repeat_elements(x, n_head, 0))(mask)
            head, attn = self.attention(qs, ks, vs, mask=mask)  
                
            def reshape2(x):
                s = tf.shape(x)   # [n_head * batch_size, len_v, d_v]
                x = tf.reshape(x, [n_head, -1, s[1], s[2]]) 
                x = tf.transpose(x, [1, 2, 0, 3])
                x = tf.reshape(x, [-1, s[1], n_head*d_v])  # [batch_size, len_v, n_head * d_v]
                return x
            head = Lambda(reshape2)(head)
        elif self.mode == 1:
            heads = []; attns = []
            for i in range(n_head):
                qs = self.qs_layers[i](q)   
                ks = self.ks_layers[i](k) 
                vs = self.vs_layers[i](v) 
                head, attn = self.attention(qs, ks, vs, mask)
                heads.append(head); attns.append(attn)
            head = Concatenate()(heads) if n_head > 1 else heads[0]
            attn = Concatenate()(attns) if n_head > 1 else attns[0]

        outputs = self.w_o(head)
        outputs = Dropout(self.dropout)(outputs)
        if not self.layer_norm: return outputs, attn
        # outputs = Add()([outputs, q]) # sl: fix
        return self.layer_norm(outputs), attn

class PositionwiseFeedForward():
    def __init__(self, d_hid, d_inner_hid, dropout=0.1):
        self.w_1 = Conv1D(d_inner_hid, 1, activation='relu')
        self.w_2 = Conv1D(d_hid, 1)
        self.layer_norm = LayerNormalization()
        self.dropout = Dropout(dropout)
    def __call__(self, x):
        output = self.w_1(x) 
        output = self.w_2(output)
        output = self.dropout(output)
        output = Add()([output, x])
        return self.layer_norm(output)

class EncoderLayer():
    def __init__(self, d_model, d_inner_hid, n_head, d_k, d_v, dropout=0.1):
        self.self_att_layer = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)
        self.pos_ffn_layer  = PositionwiseFeedForward(d_model, d_inner_hid, dropout=dropout)
    def __call__(self, enc_input, mask=None):
        output, slf_attn = self.self_att_layer(enc_input, enc_input, enc_input, mask=mask)
        output = self.pos_ffn_layer(output)
        return output, slf_attn


def GetPosEncodingMatrix(max_len, d_emb):
    pos_enc = np.array([
        [pos / np.power(10000, 2 * (j // 2) / d_emb) for j in range(d_emb)] 
        if pos != 0 else np.zeros(d_emb) 
            for pos in range(max_len)
            ])
    pos_enc[1:, 0::2] = np.sin(pos_enc[1:, 0::2]) # dim 2i
    pos_enc[1:, 1::2] = np.cos(pos_enc[1:, 1::2]) # dim 2i+1
    return pos_enc

def GetPadMask(q, k):
    ones = K.expand_dims(K.ones_like(q, 'float32'), -1)
    mask = K.cast(K.expand_dims(K.not_equal(k, 0), 1), 'float32')
    mask = K.batch_dot(ones, mask, axes=[2,1])
    return mask

def GetSubMask(s):
    len_s = tf.shape(s)[1]
    bs = tf.shape(s)[:1]
    mask = K.cumsum(tf.eye(len_s, batch_shape=bs), 1)
    return mask

class Transformer():
    def __init__(self, len_limit, embedding_matrix, d_model=embed_size, \
              d_inner_hid=512, n_head=10, d_k=64, d_v=64, layers=2, dropout=0.1, \
              share_word_emb=False, **kwargs):
        self.name = 'Transformer'
        self.len_limit = len_limit
        self.src_loc_info = False # True # sl: fix later
        self.d_model = d_model
        self.decode_model = None
        d_emb = d_model

        pos_emb = Embedding(len_limit, d_emb, trainable=False, \
                            weights=[GetPosEncodingMatrix(len_limit, d_emb)])

        i_word_emb = Embedding(max_features, d_emb, weights=[embedding_matrix]) # Add Kaggle provided embedding here

        self.encoder = Encoder(d_model, d_inner_hid, n_head, d_k, d_v, layers, dropout, \
                               word_emb=i_word_emb, pos_emb=pos_emb)

        
    def get_pos_seq(self, x):
        mask = K.cast(K.not_equal(x, 0), 'int32')
        pos = K.cumsum(K.ones_like(x, 'int32'), 1)
        return pos * mask

    def compile(self, active_layers=999):
        src_seq_input = Input(shape=(None, ))
        x = Embedding(max_features, embed_size, weights=[embedding_matrix])(src_seq_input)
        
        # LSTM before attention layers
        x = Bidirectional(LSTM(128, return_sequences=True))(x)
        x = Bidirectional(LSTM(64, return_sequences=True))(x) 
        
        x, slf_attn = MultiHeadAttention(n_head=3, d_model=300, d_k=64, d_v=64, dropout=0.1)(x, x, x)
        
        avg_pool = GlobalAveragePooling1D()(x)
        max_pool = GlobalMaxPooling1D()(x)
        conc = concatenate([avg_pool, max_pool])
        conc = Dense(64, activation="relu")(conc)
        x = Dense(1, activation="sigmoid")(conc)   
        
        
        self.model = Model(inputs=src_seq_input, outputs=x)
        self.model.compile(optimizer = 'adam', loss = 'mean_squared_error', metrics=['accuracy'])
263/34:
def build_model():
    inp = Input(shape = (SEQ_LEN, 1))
    
    # LSTM before attention layers
    x = Bidirectional(LSTM(128, return_sequences=True))(inp)
    x = Bidirectional(LSTM(64, return_sequences=True))(x) 
        
    x, slf_attn = MultiHeadAttention(n_head=3, d_model=300, d_k=64, d_v=64, dropout=0.1)(x, x, x)
        
    avg_pool = GlobalAveragePooling1D()(x)
    max_pool = GlobalMaxPooling1D()(x)
    conc = concatenate([avg_pool, max_pool])
    conc = Dense(64, activation="relu")(conc)
    x = Dense(1, activation="sigmoid")(conc)      

    model = Model(inputs = inp, outputs = x)
    model.compile(
        loss = "mean_squared_error", 
        #optimizer = Adam(lr = config["lr"], decay = config["lr_d"]), 
        optimizer = "adam")
    
    # Save entire model to a HDF5 file
    #model.save('my_model.h5')
    
    return model
263/35: multi_head = build_model()
263/36: multi_head.summary()
263/37:
multi_head.fit(X_train, y_train,
                    batch_size=BATCH_SIZE,
                    epochs=EPOCHS,
                    validation_data=(X_valid, y_valid), 
                    #callbacks = [checkpoint , lr_reduce]
             )
263/38:
predicted_stock_price_multi_head = multi_head.predict(X_test)
#predicted_stock_price = scaler.inverse_transform(predicted_stock_price)


predicted_stock_price_multi_head.shape
263/39: predicted_stock_price_multi_head = np.vstack((np.full((60,1), np.nan), predicted_stock_price_multi_head))
263/40:
plt.figure(figsize = (18,9))
plt.plot(test_data, color = 'black', label = 'GE Stock Price')
plt.plot(predicted_stock_price_multi_head, color = 'green', label = 'Predicted GE Mid Price')
plt.title('GE Mid Price Prediction', fontsize=30)
#plt.xticks(range(0,df.shape[0],50),df['Date'].loc[::50],rotation=45)
plt.xlabel('Date')
plt.ylabel('GE Mid Price')
plt.legend(fontsize=18)
plt.show()
263/41:
# Reshape both train and test data
train_data = train_data.reshape(-1)

# Normalize test data and validation data
valid_data = scaler.transform(valid_data).reshape(-1)
test_data = scaler.transform(test_data).reshape(-1)
263/42:
# Now perform exponential moving average smoothing
# So the data will have a smoother curve than the original ragged data
EMA = 0.0
gamma = 0.1
for ti in range(11000):
    EMA = gamma*train_data[ti] + (1-gamma)*EMA
    train_data[ti] = EMA

# Used for visualization and test purposes
all_mid_data = np.concatenate([train_data,valid_data, test_data],axis=0)
263/43:
X_train = []
y_train = []
for i in range(SEQ_LEN, len(train_data)):
    X_train.append(train_data[i-SEQ_LEN:i])
    y_train.append(train_data[i + (FUTURE_PERIOD_PREDICT-1)])
X_train, y_train = np.array(X_train), np.array(y_train)

X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))
263/44:
X_valid = []
y_valid = []
for i in range(SEQ_LEN, len(valid_data)):
    X_valid.append(valid_data[i-SEQ_LEN:i])
    y_valid.append(valid_data[i+(FUTURE_PERIOD_PREDICT-1)])
X_valid, y_valid = np.array(X_valid), np.array(y_valid)

X_valid = np.reshape(X_valid, (X_valid.shape[0], X_valid.shape[1], 1))
263/45:
X_test = []
y_test = []
for i in range(SEQ_LEN, len(test_data)):
    X_test.append(test_data[i-SEQ_LEN:i])
    y_test.append(test_data[i+(FUTURE_PERIOD_PREDICT-1)])
    
X_test, y_test = np.array(X_test), np.array(y_test)
X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))
263/46: y_train.shape
263/47: y_valid.shape
263/48:
X_train_2 = []
y_train_2 = []
for i in range(SEQ_LEN, len(train_data)):
    X_train_2.append(train_data[i-SEQ_LEN:i])
    y_train_2.append(train_data[i + (FUTURE_PERIOD_PREDICT-1)])
X_train_2, y_train_2 = np.array(X_train_2), np.array(y_train_2)

X_train_2 = np.reshape(X_train_2, (X_train_2.shape[0], X_train_2.shape[1], 1))
263/49:
## show predictions
plt.figure(figsize=(15, 5))

plt.plot(np.arange(y_train_2.shape[0]), y_train_2, color='blue', label='train target')

plt.plot(np.arange(y_train_2.shape[0], y_train_2.shape[0]+y_valid.shape[0]), y_valid,
         color='gray', label='valid target')

plt.plot(np.arange(y_train_2.shape[0]+y_valid.shape[0],
                   y_train_2.shape[0]+y_valid.shape[0]+y_test.shape[0]),
         y_test, color='black', label='test target')


plt.title('Sparation des donnes')
plt.xlabel('time [days]')
plt.ylabel('normalized price')
plt.legend(loc='best');
263/50:
from sklearn.utils import shuffle
X_train, y_train = shuffle(X_train, y_train)
263/51:
EPOCHS = 10  # how many passes through our data
BATCH_SIZE = 1024  # how many batches? Try smaller batch if you're getting OOM (out of memory) errors.
import time

NAME = f"{SEQ_LEN}-SEQ-{FUTURE_PERIOD_PREDICT}-PRED-{int(time.time())}"  # a unique name for the model
263/52: # !pip install -q tensorflow==2.0.0-alpha0
263/53:
# https://www.kaggle.com/shujian/transformer-with-lstm

import random, os, sys
import numpy as np
from tensorflow.keras.models import *
from tensorflow.keras.layers import *
from tensorflow.keras.callbacks import *
from tensorflow.keras.initializers import *
import tensorflow as tf
from tensorflow.python.keras.layers import Layer

try:
    from dataloader import TokenList, pad_to_longest
    # for transformer
except: pass



embed_size = 60

class LayerNormalization(Layer):
    def __init__(self, eps=1e-6, **kwargs):
        self.eps = eps
        super(LayerNormalization, self).__init__(**kwargs)
    def build(self, input_shape):
        self.gamma = self.add_weight(name='gamma', shape=input_shape[-1:],
                                     initializer=Ones(), trainable=True)
        self.beta = self.add_weight(name='beta', shape=input_shape[-1:],
                                    initializer=Zeros(), trainable=True)
        super(LayerNormalization, self).build(input_shape)
    def call(self, x):
        mean = K.mean(x, axis=-1, keepdims=True)
        std = K.std(x, axis=-1, keepdims=True)
        return self.gamma * (x - mean) / (std + self.eps) + self.beta
    def compute_output_shape(self, input_shape):
        return input_shape

class ScaledDotProductAttention():
    def __init__(self, d_model, attn_dropout=0.1):
        self.temper = np.sqrt(d_model)
        self.dropout = Dropout(attn_dropout)
    def __call__(self, q, k, v, mask):
        attn = Lambda(lambda x:K.batch_dot(x[0],x[1],axes=[2,2])/self.temper)([q, k])
        if mask is not None:
            mmask = Lambda(lambda x:(-1e+10)*(1-x))(mask)
            attn = Add()([attn, mmask])
        attn = Activation('softmax')(attn)
        attn = self.dropout(attn)
        output = Lambda(lambda x:K.batch_dot(x[0], x[1]))([attn, v])
        return output, attn

class MultiHeadAttention():
    # mode 0 - big martixes, faster; mode 1 - more clear implementation
    def __init__(self, n_head, d_model, d_k, d_v, dropout, mode=0, use_norm=True):
        self.mode = mode
        self.n_head = n_head
        self.d_k = d_k
        self.d_v = d_v
        self.dropout = dropout
        if mode == 0:
            self.qs_layer = Dense(n_head*d_k, use_bias=False)
            self.ks_layer = Dense(n_head*d_k, use_bias=False)
            self.vs_layer = Dense(n_head*d_v, use_bias=False)
        elif mode == 1:
            self.qs_layers = []
            self.ks_layers = []
            self.vs_layers = []
            for _ in range(n_head):
                self.qs_layers.append(TimeDistributed(Dense(d_k, use_bias=False)))
                self.ks_layers.append(TimeDistributed(Dense(d_k, use_bias=False)))
                self.vs_layers.append(TimeDistributed(Dense(d_v, use_bias=False)))
        self.attention = ScaledDotProductAttention(d_model)
        self.layer_norm = LayerNormalization() if use_norm else None
        self.w_o = TimeDistributed(Dense(d_model))

    def __call__(self, q, k, v, mask=None):
        d_k, d_v = self.d_k, self.d_v
        n_head = self.n_head

        if self.mode == 0:
            qs = self.qs_layer(q)  # [batch_size, len_q, n_head*d_k]
            ks = self.ks_layer(k)
            vs = self.vs_layer(v)

            def reshape1(x):
                s = tf.shape(x)   # [batch_size, len_q, n_head * d_k]
                x = tf.reshape(x, [s[0], s[1], n_head, d_k])
                x = tf.transpose(x, [2, 0, 1, 3])  
                x = tf.reshape(x, [-1, s[1], d_k])  # [n_head * batch_size, len_q, d_k]
                return x
            qs = Lambda(reshape1)(qs)
            ks = Lambda(reshape1)(ks)
            vs = Lambda(reshape1)(vs)

            if mask is not None:
                mask = Lambda(lambda x:K.repeat_elements(x, n_head, 0))(mask)
            head, attn = self.attention(qs, ks, vs, mask=mask)  
                
            def reshape2(x):
                s = tf.shape(x)   # [n_head * batch_size, len_v, d_v]
                x = tf.reshape(x, [n_head, -1, s[1], s[2]]) 
                x = tf.transpose(x, [1, 2, 0, 3])
                x = tf.reshape(x, [-1, s[1], n_head*d_v])  # [batch_size, len_v, n_head * d_v]
                return x
            head = Lambda(reshape2)(head)
        elif self.mode == 1:
            heads = []; attns = []
            for i in range(n_head):
                qs = self.qs_layers[i](q)   
                ks = self.ks_layers[i](k) 
                vs = self.vs_layers[i](v) 
                head, attn = self.attention(qs, ks, vs, mask)
                heads.append(head); attns.append(attn)
            head = Concatenate()(heads) if n_head > 1 else heads[0]
            attn = Concatenate()(attns) if n_head > 1 else attns[0]

        outputs = self.w_o(head)
        outputs = Dropout(self.dropout)(outputs)
        if not self.layer_norm: return outputs, attn
        # outputs = Add()([outputs, q]) # sl: fix
        return self.layer_norm(outputs), attn

class PositionwiseFeedForward():
    def __init__(self, d_hid, d_inner_hid, dropout=0.1):
        self.w_1 = Conv1D(d_inner_hid, 1, activation='relu')
        self.w_2 = Conv1D(d_hid, 1)
        self.layer_norm = LayerNormalization()
        self.dropout = Dropout(dropout)
    def __call__(self, x):
        output = self.w_1(x) 
        output = self.w_2(output)
        output = self.dropout(output)
        output = Add()([output, x])
        return self.layer_norm(output)

class EncoderLayer():
    def __init__(self, d_model, d_inner_hid, n_head, d_k, d_v, dropout=0.1):
        self.self_att_layer = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)
        self.pos_ffn_layer  = PositionwiseFeedForward(d_model, d_inner_hid, dropout=dropout)
    def __call__(self, enc_input, mask=None):
        output, slf_attn = self.self_att_layer(enc_input, enc_input, enc_input, mask=mask)
        output = self.pos_ffn_layer(output)
        return output, slf_attn


def GetPosEncodingMatrix(max_len, d_emb):
    pos_enc = np.array([
        [pos / np.power(10000, 2 * (j // 2) / d_emb) for j in range(d_emb)] 
        if pos != 0 else np.zeros(d_emb) 
            for pos in range(max_len)
            ])
    pos_enc[1:, 0::2] = np.sin(pos_enc[1:, 0::2]) # dim 2i
    pos_enc[1:, 1::2] = np.cos(pos_enc[1:, 1::2]) # dim 2i+1
    return pos_enc

def GetPadMask(q, k):
    ones = K.expand_dims(K.ones_like(q, 'float32'), -1)
    mask = K.cast(K.expand_dims(K.not_equal(k, 0), 1), 'float32')
    mask = K.batch_dot(ones, mask, axes=[2,1])
    return mask

def GetSubMask(s):
    len_s = tf.shape(s)[1]
    bs = tf.shape(s)[:1]
    mask = K.cumsum(tf.eye(len_s, batch_shape=bs), 1)
    return mask

class Transformer():
    def __init__(self, len_limit, embedding_matrix, d_model=embed_size, \
              d_inner_hid=512, n_head=10, d_k=64, d_v=64, layers=2, dropout=0.1, \
              share_word_emb=False, **kwargs):
        self.name = 'Transformer'
        self.len_limit = len_limit
        self.src_loc_info = False # True # sl: fix later
        self.d_model = d_model
        self.decode_model = None
        d_emb = d_model

        pos_emb = Embedding(len_limit, d_emb, trainable=False, \
                            weights=[GetPosEncodingMatrix(len_limit, d_emb)])

        i_word_emb = Embedding(max_features, d_emb, weights=[embedding_matrix]) # Add Kaggle provided embedding here

        self.encoder = Encoder(d_model, d_inner_hid, n_head, d_k, d_v, layers, dropout, \
                               word_emb=i_word_emb, pos_emb=pos_emb)

        
    def get_pos_seq(self, x):
        mask = K.cast(K.not_equal(x, 0), 'int32')
        pos = K.cumsum(K.ones_like(x, 'int32'), 1)
        return pos * mask

    def compile(self, active_layers=999):
        src_seq_input = Input(shape=(None, ))
        x = Embedding(max_features, embed_size, weights=[embedding_matrix])(src_seq_input)
        
        # LSTM before attention layers
        x = Bidirectional(LSTM(128, return_sequences=True))(x)
        x = Bidirectional(LSTM(64, return_sequences=True))(x) 
        
        x, slf_attn = MultiHeadAttention(n_head=3, d_model=300, d_k=64, d_v=64, dropout=0.1)(x, x, x)
        
        avg_pool = GlobalAveragePooling1D()(x)
        max_pool = GlobalMaxPooling1D()(x)
        conc = concatenate([avg_pool, max_pool])
        conc = Dense(64, activation="relu")(conc)
        x = Dense(1, activation="sigmoid")(conc)   
        
        
        self.model = Model(inputs=src_seq_input, outputs=x)
        self.model.compile(optimizer = 'adam', loss = 'mean_squared_error', metrics=['accuracy'])
263/54:
def build_model():
    inp = Input(shape = (SEQ_LEN, 1))
    
    # LSTM before attention layers
    x = Bidirectional(LSTM(128, return_sequences=True))(inp)
    x = Bidirectional(LSTM(64, return_sequences=True))(x) 
        
    x, slf_attn = MultiHeadAttention(n_head=3, d_model=300, d_k=64, d_v=64, dropout=0.1)(x, x, x)
        
    avg_pool = GlobalAveragePooling1D()(x)
    max_pool = GlobalMaxPooling1D()(x)
    conc = concatenate([avg_pool, max_pool])
    conc = Dense(64, activation="relu")(conc)
    x = Dense(1, activation="sigmoid")(conc)      

    model = Model(inputs = inp, outputs = x)
    model.compile(
        loss = "mean_squared_error", 
        #optimizer = Adam(lr = config["lr"], decay = config["lr_d"]), 
        optimizer = "adam")
    
    # Save entire model to a HDF5 file
    #model.save('my_model.h5')
    
    return model
263/55: multi_head = build_model()
263/56: multi_head.summary()
263/57:
multi_head.fit(X_train, y_train,
                    batch_size=BATCH_SIZE,
                    epochs=EPOCHS,
                    validation_data=(X_valid, y_valid), 
                    #callbacks = [checkpoint , lr_reduce]
             )
263/58:
predicted_stock_price_multi_head = multi_head.predict(X_test)
#predicted_stock_price = scaler.inverse_transform(predicted_stock_price)


predicted_stock_price_multi_head.shape
263/59: predicted_stock_price_multi_head = np.vstack((np.full((60,1), np.nan), predicted_stock_price_multi_head))
263/60:
plt.figure(figsize = (18,9))
plt.plot(test_data, color = 'black', label = 'GE Stock Price')
plt.plot(predicted_stock_price_multi_head, color = 'green', label = 'Predicted GE Mid Price')
plt.title('GE Mid Price Prediction', fontsize=30)
#plt.xticks(range(0,df.shape[0],50),df['Date'].loc[::50],rotation=45)
plt.xlabel('Date')
plt.ylabel('GE Mid Price')
plt.legend(fontsize=18)
plt.show()
263/61: df
263/62:
# https://www.kaggle.com/shujian/transformer-with-lstm

import random, os, sys
import numpy as np
from tensorflow.keras.models import *
from tensorflow.keras.layers import *
from tensorflow.keras.callbacks import *
from tensorflow.keras.initializers import *
import tensorflow as tf
from tensorflow.python.keras.layers import Layer

try:
    from dataloader import TokenList, pad_to_longest
    # for transformer
except: pass



embed_size = 60

class LayerNormalization(Layer):
    def __init__(self, eps=1e-6, **kwargs):
        self.eps = eps
        super(LayerNormalization, self).__init__(**kwargs)
    def build(self, input_shape):
        self.gamma = self.add_weight(name='gamma', shape=input_shape[-1:],
                                     initializer=Ones(), trainable=True)
        self.beta = self.add_weight(name='beta', shape=input_shape[-1:],
                                    initializer=Zeros(), trainable=True)
        super(LayerNormalization, self).build(input_shape)
    def call(self, x):
        mean = K.mean(x, axis=-1, keepdims=True)
        std = K.std(x, axis=-1, keepdims=True)
        return self.gamma * (x - mean) / (std + self.eps) + self.beta
    def compute_output_shape(self, input_shape):
        return input_shape

class ScaledDotProductAttention():
    def __init__(self, d_model, attn_dropout=0.1):
        self.temper = np.sqrt(d_model)
        self.dropout = Dropout(attn_dropout)
    def __call__(self, q, k, v, mask):
        attn = Lambda(lambda x:K.batch_dot(x[0],x[1],axes=[2,2])/self.temper)([q, k])
        if mask is not None:
            mmask = Lambda(lambda x:(-1e+10)*(1-x))(mask)
            attn = Add()([attn, mmask])
        attn = Activation('softmax')(attn)
        attn = self.dropout(attn)
        output = Lambda(lambda x:K.batch_dot(x[0], x[1]))([attn, v])
        return output, attn

class MultiHeadAttention():
    # mode 0 - big martixes, faster; mode 1 - more clear implementation
    def __init__(self, n_head, d_model, d_k, d_v, dropout, mode=0, use_norm=True):
        self.mode = mode
        self.n_head = n_head
        self.d_k = d_k
        self.d_v = d_v
        self.dropout = dropout
        if mode == 0:
            self.qs_layer = Dense(n_head*d_k, use_bias=False)
            self.ks_layer = Dense(n_head*d_k, use_bias=False)
            self.vs_layer = Dense(n_head*d_v, use_bias=False)
        elif mode == 1:
            self.qs_layers = []
            self.ks_layers = []
            self.vs_layers = []
            for _ in range(n_head):
                self.qs_layers.append(TimeDistributed(Dense(d_k, use_bias=False)))
                self.ks_layers.append(TimeDistributed(Dense(d_k, use_bias=False)))
                self.vs_layers.append(TimeDistributed(Dense(d_v, use_bias=False)))
        self.attention = ScaledDotProductAttention(d_model)
        self.layer_norm = LayerNormalization() if use_norm else None
        self.w_o = TimeDistributed(Dense(d_model))

    def __call__(self, q, k, v, mask=None):
        d_k, d_v = self.d_k, self.d_v
        n_head = self.n_head

        if self.mode == 0:
            qs = self.qs_layer(q)  # [batch_size, len_q, n_head*d_k]
            ks = self.ks_layer(k)
            vs = self.vs_layer(v)

            def reshape1(x):
                s = tf.shape(x)   # [batch_size, len_q, n_head * d_k]
                x = tf.reshape(x, [s[0], s[1], n_head, d_k])
                x = tf.transpose(x, [2, 0, 1, 3])  
                x = tf.reshape(x, [-1, s[1], d_k])  # [n_head * batch_size, len_q, d_k]
                return x
            qs = Lambda(reshape1)(qs)
            ks = Lambda(reshape1)(ks)
            vs = Lambda(reshape1)(vs)

            if mask is not None:
                mask = Lambda(lambda x:K.repeat_elements(x, n_head, 0))(mask)
            head, attn = self.attention(qs, ks, vs, mask=mask)  
                
            def reshape2(x):
                s = tf.shape(x)   # [n_head * batch_size, len_v, d_v]
                x = tf.reshape(x, [n_head, -1, s[1], s[2]]) 
                x = tf.transpose(x, [1, 2, 0, 3])
                x = tf.reshape(x, [-1, s[1], n_head*d_v])  # [batch_size, len_v, n_head * d_v]
                return x
            head = Lambda(reshape2)(head)
        elif self.mode == 1:
            heads = []; attns = []
            for i in range(n_head):
                qs = self.qs_layers[i](q)   
                ks = self.ks_layers[i](k) 
                vs = self.vs_layers[i](v) 
                head, attn = self.attention(qs, ks, vs, mask)
                heads.append(head); attns.append(attn)
            head = Concatenate()(heads) if n_head > 1 else heads[0]
            attn = Concatenate()(attns) if n_head > 1 else attns[0]

        outputs = self.w_o(head)
        outputs = Dropout(self.dropout)(outputs)
        if not self.layer_norm: return outputs, attn
        # outputs = Add()([outputs, q]) # sl: fix
        return self.layer_norm(outputs), attn

class PositionwiseFeedForward():
    def __init__(self, d_hid, d_inner_hid, dropout=0.1):
        self.w_1 = Conv1D(d_inner_hid, 1, activation='relu')
        self.w_2 = Conv1D(d_hid, 1)
        self.layer_norm = LayerNormalization()
        self.dropout = Dropout(dropout)
    def __call__(self, x):
        output = self.w_1(x) 
        output = self.w_2(output)
        output = self.dropout(output)
        output = Add()([output, x])
        return self.layer_norm(output)

class EncoderLayer():
    def __init__(self, d_model, d_inner_hid, n_head, d_k, d_v, dropout=0.1):
        self.self_att_layer = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)
        self.pos_ffn_layer  = PositionwiseFeedForward(d_model, d_inner_hid, dropout=dropout)
    def __call__(self, enc_input, mask=None):
        output, slf_attn = self.self_att_layer(enc_input, enc_input, enc_input, mask=mask)
        output = self.pos_ffn_layer(output)
        return output, slf_attn


def GetPosEncodingMatrix(max_len, d_emb):
    pos_enc = np.array([
        [pos / np.power(10000, 2 * (j // 2) / d_emb) for j in range(d_emb)] 
        if pos != 0 else np.zeros(d_emb) 
            for pos in range(max_len)
            ])
    pos_enc[1:, 0::2] = np.sin(pos_enc[1:, 0::2]) # dim 2i
    pos_enc[1:, 1::2] = np.cos(pos_enc[1:, 1::2]) # dim 2i+1
    return pos_enc

def GetPadMask(q, k):
    ones = K.expand_dims(K.ones_like(q, 'float32'), -1)
    mask = K.cast(K.expand_dims(K.not_equal(k, 0), 1), 'float32')
    mask = K.batch_dot(ones, mask, axes=[2,1])
    return mask

def GetSubMask(s):
    len_s = tf.shape(s)[1]
    bs = tf.shape(s)[:1]
    mask = K.cumsum(tf.eye(len_s, batch_shape=bs), 1)
    return mask

class Transformer():
    def __init__(self, len_limit, embedding_matrix, d_model=embed_size, \
              d_inner_hid=512, n_head=10, d_k=64, d_v=64, layers=2, dropout=0.1, \
              share_word_emb=False, **kwargs):
        self.name = 'Transformer'
        self.len_limit = len_limit
        self.src_loc_info = False # True # sl: fix later
        self.d_model = d_model
        self.decode_model = None
        d_emb = d_model

        pos_emb = Embedding(len_limit, d_emb, trainable=False, \
                            weights=[GetPosEncodingMatrix(len_limit, d_emb)])

        i_word_emb = Embedding(max_features, d_emb, weights=[embedding_matrix]) # Add Kaggle provided embedding here

        self.encoder = Encoder(d_model, d_inner_hid, n_head, d_k, d_v, layers, dropout, \
                               word_emb=i_word_emb, pos_emb=pos_emb)

        
    def get_pos_seq(self, x):
        mask = K.cast(K.not_equal(x, 0), 'int32')
        pos = K.cumsum(K.ones_like(x, 'int32'), 1)
        return pos * mask

    def compile(self, active_layers=999):
        src_seq_input = Input(shape=(None, ))
        x = Embedding(max_features, embed_size, weights=[embedding_matrix])(src_seq_input)
        
        # LSTM before attention layers
        x = Bidirectional(LSTM(128, return_sequences=True))(x)
        x = Bidirectional(LSTM(64, return_sequences=True))(x) 
        
        x, slf_attn = MultiHeadAttention(n_head=3, d_model=300, d_k=64, d_v=64, dropout=0.1)(x, x, x)
        
        avg_pool = GlobalAveragePooling1D()(x)
        max_pool = GlobalMaxPooling1D()(x)
        conc = concatenate([avg_pool, max_pool])
        conc = Dense(64, activation="relu")(conc)
        x = Dense(1, activation="sigmoid")(conc)   
        
        
        self.model = Model(inputs=src_seq_input, outputs=x)
        self.model.compile(optimizer = 'adam', loss = 'mean_squared_error', metrics=['accuracy'])
263/63:
# https://www.kaggle.com/shujian/transformer-with-lstm

import random, os, sys
import numpy as np
from tensorflow.keras.models import *
from tensorflow.keras.layers import *
from tensorflow.keras.callbacks import *
from tensorflow.keras.initializers import *
import tensorflow as tf
from tensorflow.python.keras.layers import Layer

try:
    from dataloader import TokenList, pad_to_longest
    # for transformer
except: pass



embed_size = 60

class LayerNormalization(Layer):
    def __init__(self, eps=1e-6, **kwargs):
        self.eps = eps
        super(LayerNormalization, self).__init__(**kwargs)
    def build(self, input_shape):
        self.gamma = self.add_weight(name='gamma', shape=input_shape[-1:],
                                     initializer=Ones(), trainable=True)
        self.beta = self.add_weight(name='beta', shape=input_shape[-1:],
                                    initializer=Zeros(), trainable=True)
        super(LayerNormalization, self).build(input_shape)
    def call(self, x):
        mean = K.mean(x, axis=-1, keepdims=True)
        std = K.std(x, axis=-1, keepdims=True)
        return self.gamma * (x - mean) / (std + self.eps) + self.beta
    def compute_output_shape(self, input_shape):
        return input_shape

class ScaledDotProductAttention():
    def __init__(self, d_model, attn_dropout=0.1):
        self.temper = np.sqrt(d_model)
        self.dropout = Dropout(attn_dropout)
    def __call__(self, q, k, v, mask):
        attn = Lambda(lambda x: k.batch_dot(x[0],x[1],axes=[2,2])/self.temper)([q, k])
        if mask is not None:
            mmask = Lambda(lambda x:(-1e+10)*(1-x))(mask)
            attn = Add()([attn, mmask])
        attn = Activation('softmax')(attn)
        attn = self.dropout(attn)
        output = Lambda(lambda x:K.batch_dot(x[0], x[1]))([attn, v])
        return output, attn

class MultiHeadAttention():
    # mode 0 - big martixes, faster; mode 1 - more clear implementation
    def __init__(self, n_head, d_model, d_k, d_v, dropout, mode=0, use_norm=True):
        self.mode = mode
        self.n_head = n_head
        self.d_k = d_k
        self.d_v = d_v
        self.dropout = dropout
        if mode == 0:
            self.qs_layer = Dense(n_head*d_k, use_bias=False)
            self.ks_layer = Dense(n_head*d_k, use_bias=False)
            self.vs_layer = Dense(n_head*d_v, use_bias=False)
        elif mode == 1:
            self.qs_layers = []
            self.ks_layers = []
            self.vs_layers = []
            for _ in range(n_head):
                self.qs_layers.append(TimeDistributed(Dense(d_k, use_bias=False)))
                self.ks_layers.append(TimeDistributed(Dense(d_k, use_bias=False)))
                self.vs_layers.append(TimeDistributed(Dense(d_v, use_bias=False)))
        self.attention = ScaledDotProductAttention(d_model)
        self.layer_norm = LayerNormalization() if use_norm else None
        self.w_o = TimeDistributed(Dense(d_model))

    def __call__(self, q, k, v, mask=None):
        d_k, d_v = self.d_k, self.d_v
        n_head = self.n_head

        if self.mode == 0:
            qs = self.qs_layer(q)  # [batch_size, len_q, n_head*d_k]
            ks = self.ks_layer(k)
            vs = self.vs_layer(v)

            def reshape1(x):
                s = tf.shape(x)   # [batch_size, len_q, n_head * d_k]
                x = tf.reshape(x, [s[0], s[1], n_head, d_k])
                x = tf.transpose(x, [2, 0, 1, 3])  
                x = tf.reshape(x, [-1, s[1], d_k])  # [n_head * batch_size, len_q, d_k]
                return x
            qs = Lambda(reshape1)(qs)
            ks = Lambda(reshape1)(ks)
            vs = Lambda(reshape1)(vs)

            if mask is not None:
                mask = Lambda(lambda x:K.repeat_elements(x, n_head, 0))(mask)
            head, attn = self.attention(qs, ks, vs, mask=mask)  
                
            def reshape2(x):
                s = tf.shape(x)   # [n_head * batch_size, len_v, d_v]
                x = tf.reshape(x, [n_head, -1, s[1], s[2]]) 
                x = tf.transpose(x, [1, 2, 0, 3])
                x = tf.reshape(x, [-1, s[1], n_head*d_v])  # [batch_size, len_v, n_head * d_v]
                return x
            head = Lambda(reshape2)(head)
        elif self.mode == 1:
            heads = []; attns = []
            for i in range(n_head):
                qs = self.qs_layers[i](q)   
                ks = self.ks_layers[i](k) 
                vs = self.vs_layers[i](v) 
                head, attn = self.attention(qs, ks, vs, mask)
                heads.append(head); attns.append(attn)
            head = Concatenate()(heads) if n_head > 1 else heads[0]
            attn = Concatenate()(attns) if n_head > 1 else attns[0]

        outputs = self.w_o(head)
        outputs = Dropout(self.dropout)(outputs)
        if not self.layer_norm: return outputs, attn
        # outputs = Add()([outputs, q]) # sl: fix
        return self.layer_norm(outputs), attn

class PositionwiseFeedForward():
    def __init__(self, d_hid, d_inner_hid, dropout=0.1):
        self.w_1 = Conv1D(d_inner_hid, 1, activation='relu')
        self.w_2 = Conv1D(d_hid, 1)
        self.layer_norm = LayerNormalization()
        self.dropout = Dropout(dropout)
    def __call__(self, x):
        output = self.w_1(x) 
        output = self.w_2(output)
        output = self.dropout(output)
        output = Add()([output, x])
        return self.layer_norm(output)

class EncoderLayer():
    def __init__(self, d_model, d_inner_hid, n_head, d_k, d_v, dropout=0.1):
        self.self_att_layer = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)
        self.pos_ffn_layer  = PositionwiseFeedForward(d_model, d_inner_hid, dropout=dropout)
    def __call__(self, enc_input, mask=None):
        output, slf_attn = self.self_att_layer(enc_input, enc_input, enc_input, mask=mask)
        output = self.pos_ffn_layer(output)
        return output, slf_attn


def GetPosEncodingMatrix(max_len, d_emb):
    pos_enc = np.array([
        [pos / np.power(10000, 2 * (j // 2) / d_emb) for j in range(d_emb)] 
        if pos != 0 else np.zeros(d_emb) 
            for pos in range(max_len)
            ])
    pos_enc[1:, 0::2] = np.sin(pos_enc[1:, 0::2]) # dim 2i
    pos_enc[1:, 1::2] = np.cos(pos_enc[1:, 1::2]) # dim 2i+1
    return pos_enc

def GetPadMask(q, k):
    ones = K.expand_dims(K.ones_like(q, 'float32'), -1)
    mask = K.cast(K.expand_dims(K.not_equal(k, 0), 1), 'float32')
    mask = K.batch_dot(ones, mask, axes=[2,1])
    return mask

def GetSubMask(s):
    len_s = tf.shape(s)[1]
    bs = tf.shape(s)[:1]
    mask = K.cumsum(tf.eye(len_s, batch_shape=bs), 1)
    return mask

class Transformer():
    def __init__(self, len_limit, embedding_matrix, d_model=embed_size, \
              d_inner_hid=512, n_head=10, d_k=64, d_v=64, layers=2, dropout=0.1, \
              share_word_emb=False, **kwargs):
        self.name = 'Transformer'
        self.len_limit = len_limit
        self.src_loc_info = False # True # sl: fix later
        self.d_model = d_model
        self.decode_model = None
        d_emb = d_model

        pos_emb = Embedding(len_limit, d_emb, trainable=False, \
                            weights=[GetPosEncodingMatrix(len_limit, d_emb)])

        i_word_emb = Embedding(max_features, d_emb, weights=[embedding_matrix]) # Add Kaggle provided embedding here

        self.encoder = Encoder(d_model, d_inner_hid, n_head, d_k, d_v, layers, dropout, \
                               word_emb=i_word_emb, pos_emb=pos_emb)

        
    def get_pos_seq(self, x):
        mask = K.cast(K.not_equal(x, 0), 'int32')
        pos = K.cumsum(K.ones_like(x, 'int32'), 1)
        return pos * mask

    def compile(self, active_layers=999):
        src_seq_input = Input(shape=(None, ))
        x = Embedding(max_features, embed_size, weights=[embedding_matrix])(src_seq_input)
        
        # LSTM before attention layers
        x = Bidirectional(LSTM(128, return_sequences=True))(x)
        x = Bidirectional(LSTM(64, return_sequences=True))(x) 
        
        x, slf_attn = MultiHeadAttention(n_head=3, d_model=300, d_k=64, d_v=64, dropout=0.1)(x, x, x)
        
        avg_pool = GlobalAveragePooling1D()(x)
        max_pool = GlobalMaxPooling1D()(x)
        conc = concatenate([avg_pool, max_pool])
        conc = Dense(64, activation="relu")(conc)
        x = Dense(1, activation="sigmoid")(conc)   
        
        
        self.model = Model(inputs=src_seq_input, outputs=x)
        self.model.compile(optimizer = 'adam', loss = 'mean_squared_error', metrics=['accuracy'])
263/64:
# https://www.kaggle.com/shujian/transformer-with-lstm

import random, os, sys
import numpy as np
from tensorflow.keras.models import *
from tensorflow.keras.layers import *
from tensorflow.keras.callbacks import *
from tensorflow.keras.initializers import *
import tensorflow as tf
from tensorflow.python.keras.layers import Layer

try:
    from dataloader import TokenList, pad_to_longest
    # for transformer
except: pass



embed_size = 60

class LayerNormalization(Layer):
    def __init__(self, eps=1e-6, **kwargs):
        self.eps = eps
        super(LayerNormalization, self).__init__(**kwargs)
    def build(self, input_shape):
        self.gamma = self.add_weight(name='gamma', shape=input_shape[-1:],
                                     initializer=Ones(), trainable=True)
        self.beta = self.add_weight(name='beta', shape=input_shape[-1:],
                                    initializer=Zeros(), trainable=True)
        super(LayerNormalization, self).build(input_shape)
    def call(self, x):
        mean = K.mean(x, axis=-1, keepdims=True)
        std = K.std(x, axis=-1, keepdims=True)
        return self.gamma * (x - mean) / (std + self.eps) + self.beta
    def compute_output_shape(self, input_shape):
        return input_shape

class ScaledDotProductAttention():
    def __init__(self, d_model, attn_dropout=0.1):
        self.temper = np.sqrt(d_model)
        self.dropout = Dropout(attn_dropout)
    def __call__(self, q, k, v, mask):
        attn = Lambda(lambda x: k.batch_dot(x[0],x[1],axes=[2,2])/self.temper)([q, k])
        if mask is not None:
            mmask = Lambda(lambda x:(-1e+10)*(1-x))(mask)
            attn = Add()([attn, mmask])
        attn = Activation('softmax')(attn)
        attn = self.dropout(attn)
        output = Lambda(lambda x:K.batch_dot(x[0], x[1]))([attn, v])
        return output, attn

class MultiHeadAttention():
    # mode 0 - big martixes, faster; mode 1 - more clear implementation
    def __init__(self, n_head, d_model, d_k, d_v, dropout, mode=0, use_norm=True):
        self.mode = mode
        self.n_head = n_head
        self.d_k = d_k
        self.d_v = d_v
        self.dropout = dropout
        if mode == 0:
            self.qs_layer = Dense(n_head*d_k, use_bias=False)
            self.ks_layer = Dense(n_head*d_k, use_bias=False)
            self.vs_layer = Dense(n_head*d_v, use_bias=False)
        elif mode == 1:
            self.qs_layers = []
            self.ks_layers = []
            self.vs_layers = []
            for _ in range(n_head):
                self.qs_layers.append(TimeDistributed(Dense(d_k, use_bias=False)))
                self.ks_layers.append(TimeDistributed(Dense(d_k, use_bias=False)))
                self.vs_layers.append(TimeDistributed(Dense(d_v, use_bias=False)))
        self.attention = ScaledDotProductAttention(d_model)
        self.layer_norm = LayerNormalization() if use_norm else None
        self.w_o = TimeDistributed(Dense(d_model))

    def __call__(self, q, k, v, mask=None):
        d_k, d_v = self.d_k, self.d_v
        n_head = self.n_head

        if self.mode == 0:
            qs = self.qs_layer(q)  # [batch_size, len_q, n_head*d_k]
            ks = self.ks_layer(k)
            vs = self.vs_layer(v)

            def reshape1(x):
                s = tf.shape(x)   # [batch_size, len_q, n_head * d_k]
                x = tf.reshape(x, [s[0], s[1], n_head, d_k])
                x = tf.transpose(x, [2, 0, 1, 3])  
                x = tf.reshape(x, [-1, s[1], d_k])  # [n_head * batch_size, len_q, d_k]
                return x
            qs = Lambda(reshape1)(qs)
            ks = Lambda(reshape1)(ks)
            vs = Lambda(reshape1)(vs)

            if mask is not None:
                mask = Lambda(lambda x:K.repeat_elements(x, n_head, 0))(mask)
            head, attn = self.attention(qs, ks, vs, mask=mask)  
                
            def reshape2(x):
                s = tf.shape(x)   # [n_head * batch_size, len_v, d_v]
                x = tf.reshape(x, [n_head, -1, s[1], s[2]]) 
                x = tf.transpose(x, [1, 2, 0, 3])
                x = tf.reshape(x, [-1, s[1], n_head*d_v])  # [batch_size, len_v, n_head * d_v]
                return x
            head = Lambda(reshape2)(head)
        elif self.mode == 1:
            heads = []; attns = []
            for i in range(n_head):
                qs = self.qs_layers[i](q)   
                ks = self.ks_layers[i](k) 
                vs = self.vs_layers[i](v) 
                head, attn = self.attention(qs, ks, vs, mask)
                heads.append(head); attns.append(attn)
            head = Concatenate()(heads) if n_head > 1 else heads[0]
            attn = Concatenate()(attns) if n_head > 1 else attns[0]

        outputs = self.w_o(head)
        outputs = Dropout(self.dropout)(outputs)
        if not self.layer_norm: return outputs, attn
        # outputs = Add()([outputs, q]) # sl: fix
        return self.layer_norm(outputs), attn

class PositionwiseFeedForward():
    def __init__(self, d_hid, d_inner_hid, dropout=0.1):
        self.w_1 = Conv1D(d_inner_hid, 1, activation='relu')
        self.w_2 = Conv1D(d_hid, 1)
        self.layer_norm = LayerNormalization()
        self.dropout = Dropout(dropout)
    def __call__(self, x):
        output = self.w_1(x) 
        output = self.w_2(output)
        output = self.dropout(output)
        output = Add()([output, x])
        return self.layer_norm(output)

class EncoderLayer():
    def __init__(self, d_model, d_inner_hid, n_head, d_k, d_v, dropout=0.1):
        self.self_att_layer = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)
        self.pos_ffn_layer  = PositionwiseFeedForward(d_model, d_inner_hid, dropout=dropout)
    def __call__(self, enc_input, mask=None):
        output, slf_attn = self.self_att_layer(enc_input, enc_input, enc_input, mask=mask)
        output = self.pos_ffn_layer(output)
        return output, slf_attn


def GetPosEncodingMatrix(max_len, d_emb):
    pos_enc = np.array([
        [pos / np.power(10000, 2 * (j // 2) / d_emb) for j in range(d_emb)] 
        if pos != 0 else np.zeros(d_emb) 
            for pos in range(max_len)
            ])
    pos_enc[1:, 0::2] = np.sin(pos_enc[1:, 0::2]) # dim 2i
    pos_enc[1:, 1::2] = np.cos(pos_enc[1:, 1::2]) # dim 2i+1
    return pos_enc

def GetPadMask(q, k):
    ones = K.expand_dims(K.ones_like(q, 'float32'), -1)
    mask = K.cast(K.expand_dims(K.not_equal(k, 0), 1), 'float32')
    mask = K.batch_dot(ones, mask, axes=[2,1])
    return mask

def GetSubMask(s):
    len_s = tf.shape(s)[1]
    bs = tf.shape(s)[:1]
    mask = K.cumsum(tf.eye(len_s, batch_shape=bs), 1)
    return mask

class Transformer():
    def __init__(self, len_limit, embedding_matrix, d_model=embed_size, \
              d_inner_hid=512, n_head=10, d_k=64, d_v=64, layers=2, dropout=0.1, \
              share_word_emb=False, **kwargs):
        self.name = 'Transformer'
        self.len_limit = len_limit
        self.src_loc_info = False # True # sl: fix later
        self.d_model = d_model
        self.decode_model = None
        d_emb = d_model

        pos_emb = Embedding(len_limit, d_emb, trainable=False, \
                            weights=[GetPosEncodingMatrix(len_limit, d_emb)])

        i_word_emb = Embedding(max_features, d_emb, weights=[embedding_matrix]) # Add Kaggle provided embedding here

        self.encoder = Encoder(d_model, d_inner_hid, n_head, d_k, d_v, layers, dropout, \
                               word_emb=i_word_emb, pos_emb=pos_emb)

        
    def get_pos_seq(self, x):
        mask = K.cast(K.not_equal(x, 0), 'int32')
        pos = K.cumsum(K.ones_like(x, 'int32'), 1)
        return pos * mask

    def compile(self, active_layers=999):
        src_seq_input = Input(shape=(None, ))
        x = Embedding(max_features, embed_size, weights=[embedding_matrix])(src_seq_input)
        
        # LSTM before attention layers
        x = Bidirectional(LSTM(128, return_sequences=True))(x)
        x = Bidirectional(LSTM(64, return_sequences=True))(x) 
        
        x, slf_attn = MultiHeadAttention(n_head=3, d_model=300, d_k=64, d_v=64, dropout=0.1)(x, x, x)
        
        avg_pool = GlobalAveragePooling1D()(x)
        max_pool = GlobalMaxPooling1D()(x)
        conc = concatenate([avg_pool, max_pool])
        conc = Dense(64, activation="relu")(conc)
        x = Dense(1, activation="sigmoid")(conc)   
        
        
        self.model = Model(inputs=src_seq_input, outputs=x)
        self.model.compile(optimizer = 'adam', loss = 'mean_squared_error', metrics=['accuracy'])
263/65:
def build_model():
    inp = Input(shape = (SEQ_LEN, 1))
    
    # LSTM before attention layers
    x = Bidirectional(LSTM(128, return_sequences=True))(inp)
    x = Bidirectional(LSTM(64, return_sequences=True))(x) 
        
    x, slf_attn = MultiHeadAttention(n_head=3, d_model=300, d_k=64, d_v=64, dropout=0.1)(x, x, x)
        
    avg_pool = GlobalAveragePooling1D()(x)
    max_pool = GlobalMaxPooling1D()(x)
    conc = concatenate([avg_pool, max_pool])
    conc = Dense(64, activation="relu")(conc)
    x = Dense(1, activation="sigmoid")(conc)      

    model = Model(inputs = inp, outputs = x)
    model.compile(
        loss = "mean_squared_error", 
        #optimizer = Adam(lr = config["lr"], decay = config["lr_d"]), 
        optimizer = "adam")
    
    # Save entire model to a HDF5 file
    # model.save('my_model.h5')
    
    return model
263/66: multi_head = build_model()
265/1:
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import os
from sklearn.preprocessing import MinMaxScaler
265/2:
# Sort DataFrame by date
df = df.sort_values('Date')
# Double check the result
print(df.shape)
df.head()
265/3:
# To add a new cell, type '# %%'
# To add a new markdown cell, type '# %% [markdown]'
# %% [markdown]
# ## Stock Prediction model with Tensorflow 2.0! 
# 
# We're going to predict prices of General Electric's stock using a Transformer neural network
265/4:
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import os
from sklearn.preprocessing import MinMaxScaler
265/5:
# I got it from https://finance.yahoo.com/quote/GE/history?p=GE&.tsrc=fin-srch
df = pd.read_csv('2- ge.us.csv',delimiter=',',usecols=['Date','Open','High','Low','Close', 'Volume'])
265/6:
# To add a new cell, type '# %%'
# To add a new markdown cell, type '# %% [markdown]'
# %% [markdown]
# ## Stock Prediction model with Tensorflow 2.0! 
# 
# We're going to predict prices of General Electric's stock using a Transformer neural network
265/7:
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import os
from sklearn.preprocessing import MinMaxScaler
265/8:
# I got it from https://finance.yahoo.com/quote/GE/history?p=GE&.tsrc=fin-srch
df = pd.read_csv('2- ge.us.csv',delimiter=',',usecols=['Date','Open','High','Low','Close', 'Volume'])
271/1:
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import os
from sklearn.preprocessing import MinMaxScaler
271/2:
# I got it from https://finance.yahoo.com/quote/GE/history?p=GE&.tsrc=fin-srch
df = pd.read_csv('2- ge.us.csv',delimiter=',',usecols=['Date','Open','High','Low','Close', 'Volume'])
271/3:
# Sort DataFrame by date
df = df.sort_values('Date')
# Double check the result
print(df.shape)
df.head()
271/4:
plt.figure(figsize = (18,9))
plt.plot(range(df.shape[0]),(df['Low']+df['High'])/2.0)
plt.xticks(range(0,df.shape[0],500),df['Date'].loc[::500],rotation=45)
plt.xlabel('Date',fontsize=18)
plt.ylabel('Mid Price',fontsize=18)
plt.show()
271/5: df['mid'] = (df['Low']+df['High'])/2.0
271/6:
SEQ_LEN = 60  # how long of a preceeding sequence to collect for RNN
FUTURE_PERIOD_PREDICT = 1  # how far into the future are we trying to predict?
RATIO_TO_PREDICT = "mid"
271/7:
def classify(current, future):
    if float(future) > float(current):
        return 1
    else:
        return 0
271/8: df['future'] = df[RATIO_TO_PREDICT].shift(-FUTURE_PERIOD_PREDICT)
271/9: df['target'] = list(map(classify, df[RATIO_TO_PREDICT], df['future']))
271/10: df.head()
271/11: df.tail()
271/12:
times = sorted(df.index.values)  # get the times
last_10pct = sorted(df.index.values)[-int(0.1*len(times))]  # get the last 10% of the times
last_20pct = sorted(df.index.values)[-int(0.2*len(times))]  # get the last 20% of the times

test_df = df[(df.index >= last_10pct)]
validation_df = df[(df.index >= last_20pct) & (df.index < last_10pct)]  
train_df = df[(df.index < last_20pct)]  # now the train_df is all the data up to the last 20%
271/13:
from collections import deque
import numpy as np
import random
271/14:
train_df.drop(columns=["Date", "future", 'Open', 'High', 'Low', 'Close', 'Volume'], inplace=True)
validation_df.drop(columns=["Date", "future", 'Open', 'High', 'Low', 'Close', 'Volume'], inplace=True)
test_df.drop(columns=["Date", "future", 'Open', 'High', 'Low', 'Close', 'Volume'], inplace=True)# don't need this anymore.
271/15: train_df.head()
271/16:
train_data = train_df[RATIO_TO_PREDICT].values
valid_data = validation_df[RATIO_TO_PREDICT].values
test_data = test_df[RATIO_TO_PREDICT].values
test_data
271/17:
train_data = train_data.reshape(-1,1)
valid_data = valid_data.reshape(-1,1)
test_data = test_data.reshape(-1,1)
271/18: scaler = MinMaxScaler()
271/19:
# Train the Scaler with training data and smooth data
smoothing_window_size = 2500
for di in range(0,10000,smoothing_window_size):
    scaler.fit(train_data[di:di+smoothing_window_size,:])
    train_data[di:di+smoothing_window_size,:] = scaler.transform(train_data[di:di+smoothing_window_size,:])

# You normalize the last bit of remaining data
scaler.fit(train_data[di+smoothing_window_size:,:])
train_data[di+smoothing_window_size:,:] = scaler.transform(train_data[di+smoothing_window_size:,:])
271/20:
# Reshape both train and test data
train_data = train_data.reshape(-1)

# Normalize test data and validation data
valid_data = scaler.transform(valid_data).reshape(-1)
test_data = scaler.transform(test_data).reshape(-1)
271/21:
# Now perform exponential moving average smoothing
# So the data will have a smoother curve than the original ragged data
EMA = 0.0
gamma = 0.1
for ti in range(11000):
    EMA = gamma*train_data[ti] + (1-gamma)*EMA
    train_data[ti] = EMA

# Used for visualization and test purposes
all_mid_data = np.concatenate([train_data,valid_data, test_data],axis=0)
271/22:
X_train = []
y_train = []
for i in range(SEQ_LEN, len(train_data)):
    X_train.append(train_data[i-SEQ_LEN:i])
    y_train.append(train_data[i + (FUTURE_PERIOD_PREDICT-1)])
X_train, y_train = np.array(X_train), np.array(y_train)

X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))
271/23:
X_valid = []
y_valid = []
for i in range(SEQ_LEN, len(valid_data)):
    X_valid.append(valid_data[i-SEQ_LEN:i])
    y_valid.append(valid_data[i+(FUTURE_PERIOD_PREDICT-1)])
X_valid, y_valid = np.array(X_valid), np.array(y_valid)

X_valid = np.reshape(X_valid, (X_valid.shape[0], X_valid.shape[1], 1))
271/24:
X_test = []
y_test = []
for i in range(SEQ_LEN, len(test_data)):
    X_test.append(test_data[i-SEQ_LEN:i])
    y_test.append(test_data[i+(FUTURE_PERIOD_PREDICT-1)])
    
X_test, y_test = np.array(X_test), np.array(y_test)
X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))
271/25: y_train.shape
271/26: y_valid.shape
271/27:
X_train_2 = []
y_train_2 = []
for i in range(SEQ_LEN, len(train_data)):
    X_train_2.append(train_data[i-SEQ_LEN:i])
    y_train_2.append(train_data[i + (FUTURE_PERIOD_PREDICT-1)])
X_train_2, y_train_2 = np.array(X_train_2), np.array(y_train_2)

X_train_2 = np.reshape(X_train_2, (X_train_2.shape[0], X_train_2.shape[1], 1))
271/28:
## show predictions
plt.figure(figsize=(15, 5))

plt.plot(np.arange(y_train_2.shape[0]), y_train_2, color='blue', label='train target')

plt.plot(np.arange(y_train_2.shape[0], y_train_2.shape[0]+y_valid.shape[0]), y_valid,
         color='gray', label='valid target')

plt.plot(np.arange(y_train_2.shape[0]+y_valid.shape[0],
                   y_train_2.shape[0]+y_valid.shape[0]+y_test.shape[0]),
         y_test, color='black', label='test target')


plt.title('Sparation des donnes')
plt.xlabel('time [days]')
plt.ylabel('normalized price')
plt.legend(loc='best');
271/29:
from sklearn.utils import shuffle
X_train, y_train = shuffle(X_train, y_train)
271/30:
EPOCHS = 10  # how many passes through our data
BATCH_SIZE = 1024  # how many batches? Try smaller batch if you're getting OOM (out of memory) errors.
import time

NAME = f"{SEQ_LEN}-SEQ-{FUTURE_PERIOD_PREDICT}-PRED-{int(time.time())}"  # a unique name for the model
271/31: # !pip install -q tensorflow==2.0.0-alpha0
271/32:
import random, os, sys
import numpy as np
from keras.models import *
from keras.layers import *
from keras.callbacks import *
from keras.initializers import *
import tensorflow as tf

try:
    from tqdm import tqdm
    from dataloader import TokenList, pad_to_longest
    # for transformer
except: pass

class LayerNormalization(Layer):
    def __init__(self, eps=1e-6, **kwargs):
        self.eps = eps
        super(LayerNormalization, self).__init__(**kwargs)
    def build(self, input_shape):
        self.gamma = self.add_weight(name='gamma', shape=input_shape[-1:], initializer=Ones(), trainable=True)
        self.beta = self.add_weight(name='beta', shape=input_shape[-1:], initializer=Zeros(), trainable=True)
        super(LayerNormalization, self).build(input_shape)
    def call(self, x):
        mean = K.mean(x, axis=-1, keepdims=True)
        std = K.std(x, axis=-1, keepdims=True)
        return self.gamma * (x - mean) / (std + self.eps) + self.beta
    def compute_output_shape(self, input_shape):
        return input_shape

# It's safe to use a 1-d mask for self-attention
class ScaledDotProductAttention():
    def __init__(self, attn_dropout=0.1):
        self.dropout = Dropout(attn_dropout)
    def __call__(self, q, k, v, mask):   # mask_k or mask_qk
        temper = tf.sqrt(tf.cast(tf.shape(k)[-1], dtype='float32'))
        attn = Lambda(lambda x:K.batch_dot(x[0],x[1],axes=[2,2])/temper)([q, k])  # shape=(batch, q, k)
        if mask is not None:
            mmask = Lambda(lambda x:(-1e+9)*(1.-K.cast(x, 'float32')))(mask)
            attn = Add()([attn, mmask])
        attn = Activation('softmax')(attn)
        attn = self.dropout(attn)
        output = Lambda(lambda x:K.batch_dot(x[0], x[1]))([attn, v])
        return output, attn

class MultiHeadAttention():
    # mode 0 - big martixes, faster; mode 1 - more clear implementation
    def __init__(self, n_head, d_model, dropout, mode=0):
        self.mode = mode
        self.n_head = n_head
        self.d_k = self.d_v = d_k = d_v = d_model // n_head
        self.dropout = dropout
        if mode == 0:
            self.qs_layer = Dense(n_head*d_k, use_bias=False)
            self.ks_layer = Dense(n_head*d_k, use_bias=False)
            self.vs_layer = Dense(n_head*d_v, use_bias=False)
        elif mode == 1:
            self.qs_layers = []
            self.ks_layers = []
            self.vs_layers = []
            for _ in range(n_head):
                self.qs_layers.append(TimeDistributed(Dense(d_k, use_bias=False)))
                self.ks_layers.append(TimeDistributed(Dense(d_k, use_bias=False)))
                self.vs_layers.append(TimeDistributed(Dense(d_v, use_bias=False)))
        self.attention = ScaledDotProductAttention()
        self.w_o = TimeDistributed(Dense(d_model))

    def __call__(self, q, k, v, mask=None):
        d_k, d_v = self.d_k, self.d_v
        n_head = self.n_head

        if self.mode == 0:
            qs = self.qs_layer(q)  # [batch_size, len_q, n_head*d_k]
            ks = self.ks_layer(k)
            vs = self.vs_layer(v)

            def reshape1(x):
                s = tf.shape(x)   # [batch_size, len_q, n_head * d_k]
                x = tf.reshape(x, [s[0], s[1], n_head, s[2]//n_head])
                x = tf.transpose(x, [2, 0, 1, 3])  
                x = tf.reshape(x, [-1, s[1], s[2]//n_head])  # [n_head * batch_size, len_q, d_k]
                return x
            qs = Lambda(reshape1)(qs)
            ks = Lambda(reshape1)(ks)
            vs = Lambda(reshape1)(vs)

            if mask is not None:
                mask = Lambda(lambda x:K.repeat_elements(x, n_head, 0))(mask)
            head, attn = self.attention(qs, ks, vs, mask=mask)  
                
            def reshape2(x):
                s = tf.shape(x)   # [n_head * batch_size, len_v, d_v]
                x = tf.reshape(x, [n_head, -1, s[1], s[2]]) 
                x = tf.transpose(x, [1, 2, 0, 3])
                x = tf.reshape(x, [-1, s[1], n_head*d_v])  # [batch_size, len_v, n_head * d_v]
                return x
            head = Lambda(reshape2)(head)
        elif self.mode == 1:
            heads = []; attns = []
            for i in range(n_head):
                qs = self.qs_layers[i](q)   
                ks = self.ks_layers[i](k) 
                vs = self.vs_layers[i](v) 
                head, attn = self.attention(qs, ks, vs, mask)
                heads.append(head); attns.append(attn)
            head = Concatenate()(heads) if n_head > 1 else heads[0]
            attn = Concatenate()(attns) if n_head > 1 else attns[0]

        outputs = self.w_o(head)
        outputs = Dropout(self.dropout)(outputs)
        return outputs, attn

class PositionwiseFeedForward():
    def __init__(self, d_hid, d_inner_hid, dropout=0.1):
        self.w_1 = Conv1D(d_inner_hid, 1, activation='relu')
        self.w_2 = Conv1D(d_hid, 1)
        self.layer_norm = LayerNormalization()
        self.dropout = Dropout(dropout)
    def __call__(self, x):
        output = self.w_1(x) 
        output = self.w_2(output)
        output = self.dropout(output)
        output = Add()([output, x])
        return self.layer_norm(output)

class EncoderLayer():
    def __init__(self, d_model, d_inner_hid, n_head, dropout=0.1):
        self.self_att_layer = MultiHeadAttention(n_head, d_model, dropout=dropout)
        self.pos_ffn_layer  = PositionwiseFeedForward(d_model, d_inner_hid, dropout=dropout)
        self.norm_layer = LayerNormalization()
    def __call__(self, enc_input, mask=None):
        output, slf_attn = self.self_att_layer(enc_input, enc_input, enc_input, mask=mask)
        output = self.norm_layer(Add()([enc_input, output]))
        output = self.pos_ffn_layer(output)
        return output, slf_attn

class DecoderLayer():
    def __init__(self, d_model, d_inner_hid, n_head, dropout=0.1):
        self.self_att_layer = MultiHeadAttention(n_head, d_model, dropout=dropout)
        self.enc_att_layer  = MultiHeadAttention(n_head, d_model, dropout=dropout)
        self.pos_ffn_layer  = PositionwiseFeedForward(d_model, d_inner_hid, dropout=dropout)
        self.norm_layer1 = LayerNormalization()
        self.norm_layer2 = LayerNormalization()
    def __call__(self, dec_input, enc_output, self_mask=None, enc_mask=None, dec_last_state=None):
        if dec_last_state is None: dec_last_state = dec_input
        output, slf_attn = self.self_att_layer(dec_input, dec_last_state, dec_last_state, mask=self_mask)
        x = self.norm_layer1(Add()([dec_input, output]))
        output, enc_attn = self.enc_att_layer(x, enc_output, enc_output, mask=enc_mask)
        x = self.norm_layer2(Add()([x, output]))
        output = self.pos_ffn_layer(x)
        return output, slf_attn, enc_attn

def GetPosEncodingMatrix(max_len, d_emb):
    pos_enc = np.array([
        [pos / np.power(10000, 2 * (j // 2) / d_emb) for j in range(d_emb)] 
        if pos != 0 else np.zeros(d_emb) 
            for pos in range(max_len)
            ])
    pos_enc[1:, 0::2] = np.sin(pos_enc[1:, 0::2]) # dim 2i
    pos_enc[1:, 1::2] = np.cos(pos_enc[1:, 1::2]) # dim 2i+1
    return pos_enc

def GetPadMask(q, k):
    '''
    shape: [B, Q, K]
    '''
    ones = K.expand_dims(K.ones_like(q, 'float32'), -1)
    mask = K.cast(K.expand_dims(K.not_equal(k, 0), 1), 'float32')
    mask = K.batch_dot(ones, mask, axes=[2,1])
    return mask

def GetSubMask(s):
    '''
    shape: [B, Q, K], lower triangle because the i-th row should have i 1s.
    '''
    len_s = tf.shape(s)[1]
    bs = tf.shape(s)[:1]
    mask = K.cumsum(tf.eye(len_s, batch_shape=bs), 1)
    return mask

class SelfAttention():
    def __init__(self, d_model, d_inner_hid, n_head, layers=6, dropout=0.1):
        self.layers = [EncoderLayer(d_model, d_inner_hid, n_head, dropout) for _ in range(layers)]
    def __call__(self, src_emb, src_seq, return_att=False, active_layers=999):
        if return_att: atts = []
        mask = Lambda(lambda x:K.cast(K.greater(x, 0), 'float32'))(src_seq)
        x = src_emb     
        for enc_layer in self.layers[:active_layers]:
            x, att = enc_layer(x, mask)
            if return_att: atts.append(att)
        return (x, atts) if return_att else x

class Decoder():
    def __init__(self, d_model, d_inner_hid, n_head, layers=6, dropout=0.1):
        self.layers = [DecoderLayer(d_model, d_inner_hid, n_head, dropout) for _ in range(layers)]
    def __call__(self, tgt_emb, tgt_seq, src_seq, enc_output, return_att=False, active_layers=999):
        x = tgt_emb
        self_pad_mask = Lambda(lambda x:GetPadMask(x, x))(tgt_seq)
        self_sub_mask = Lambda(GetSubMask)(tgt_seq)
        self_mask = Lambda(lambda x:K.minimum(x[0], x[1]))([self_pad_mask, self_sub_mask])
        enc_mask = Lambda(lambda x:GetPadMask(x[0], x[1]))([tgt_seq, src_seq])
        if return_att: self_atts, enc_atts = [], []
        for dec_layer in self.layers[:active_layers]:
            x, self_att, enc_att = dec_layer(x, enc_output, self_mask, enc_mask)
            if return_att: 
                self_atts.append(self_att)
                enc_atts.append(enc_att)
        return (x, self_atts, enc_atts) if return_att else x

class DecoderPerStep(Layer):
    def __init__(self, decoder):
        super().__init__()
        self.layers = decoder.layers
    def call(self, inputs):
        (x, src_seq, enc_output), tgt_embs = inputs[:3], inputs[3:]
        enc_mask = K.cast(K.greater(src_seq, 0), 'float32')
        llen = tf.shape(tgt_embs[0])[1]
        col_mask = K.cast(K.equal(K.cumsum(K.ones_like(tgt_embs[0], dtype='int32'), axis=1), llen), dtype='float32')
        rs = [x]
        for i, dec_layer in enumerate(self.layers):
            tgt_emb = tgt_embs[i] + x * col_mask
            x, _, _ = dec_layer(x, enc_output, enc_mask=enc_mask, dec_last_state=tgt_emb)
            rs.append(x)
        return rs
    def compute_output_shape(self, ishape):
        return [ishape[0] for _ in range(len(self.layers)+1)]

class ReadoutDecoderCell(Layer):
    def __init__(self, o_word_emb, pos_emb, decoder, target_layer, **kwargs):
        self.o_word_emb = o_word_emb
        self.pos_emb = pos_emb
        self.decoder = decoder
        self.target_layer = target_layer
        super().__init__(**kwargs)
    def call(self, inputs, states, constants, training=None):
        (tgt_curr_input, tgt_pos_input, dec_mask), dec_output = states[:3], list(states[3:])
        enc_output, enc_mask = constants

        time = K.max(tgt_pos_input)
        col_mask = K.cast(K.equal(K.cumsum(K.ones_like(dec_mask), axis=1), time), dtype='int32')
        dec_mask = dec_mask + col_mask

        tgt_emb = self.o_word_emb(tgt_curr_input)
        if self.pos_emb: tgt_emb = tgt_emb + self.pos_emb(tgt_pos_input, pos_input=True)

        x = tgt_emb
        xs = []
        cc = K.cast(K.expand_dims(col_mask), dtype='float32')
        for i, dec_layer in enumerate(self.decoder.layers):
            dec_last_state = dec_output[i] * (1-cc) + tf.einsum('ijk,ilj->ilk', x, cc)
            x, _, _ = dec_layer(x, enc_output, dec_mask, enc_mask, dec_last_state=dec_last_state)
            xs.append(dec_last_state)

        ff_output = self.target_layer(x)
        out = K.cast(K.argmax(ff_output, -1), dtype='int32')
        return out, [out, tgt_pos_input+1, dec_mask] + xs

class InferRNN(Layer):
    def __init__(self, cell, return_sequences=False, go_backwards=False, **kwargs):
        if not hasattr(cell, 'call'):
            raise ValueError('`cell` should have a `call` method. ' 'The RNN was passed:', cell)
        super().__init__(**kwargs)
        self.cell = cell
        self.return_sequences = return_sequences
        self.go_backwards = go_backwards

    def compute_output_shape(self, input_shape):
        return (input_shape[0], input_shape[1], 1) if self.return_sequences else (input_shape[0], 1)
            
    def __call__(self, inputs, initial_state=None, constants=None, **kwargs):
        if initial_state is not None:
            kwargs['initial_state'] = initial_state
        if constants is not None:
            kwargs['constants'] = constants
            self._num_constants = len(constants)
        return super().__call__(inputs, **kwargs)

    def call(self, inputs, mask=None, training=None, initial_state=None, constants=None):
        if isinstance(inputs, list):
            if self._num_constants is None: initial_state = inputs[1:]
            else: initial_state = inputs[1:-self._num_constants]
            inputs = inputs[0]
        input_shape = K.int_shape(inputs)
        timesteps = input_shape[1]

        kwargs = {}
        def step(inputs, states):
            constants = states[-self._num_constants:]
            states = states[:-self._num_constants]
            return self.cell.call(inputs, states, constants=constants, **kwargs)

        last_output, outputs, states = K.rnn(step, inputs, initial_state, constants=constants,
                                             go_backwards=self.go_backwards,
                                             mask=mask, unroll=False, input_length=timesteps)
        output = outputs if self.return_sequences else last_output
        return output

def decode_batch_greedy(src_seq, encode_model, decode_model, start_mark, end_mark, max_len=128):
    enc_ret = encode_model.predict_on_batch(src_seq)
    bs = src_seq.shape[0]
    target_one = np.zeros((bs, 1), dtype='int32')
    target_one[:,0] = start_mark
    d_model = decode_model.inputs[-1].shape[-1]
    n_dlayers = len(decode_model.inputs) - 3
    dec_outputs = [np.zeros((bs, 1, d_model)) for _ in range(n_dlayers)]
    ended = [0 for x in range(bs)]
    decoded_indexes = [[] for x in range(bs)]
    for i in range(max_len-1):
        outputs = decode_model.predict_on_batch([target_one, src_seq, enc_ret] + dec_outputs)
        new_dec_outputs, output = outputs[:-1], outputs[-1]
        for dec_output, new_out in zip(dec_outputs, new_dec_outputs): 
            dec_output[:,-1,:] = new_out[:,0,:]
        dec_outputs = [np.concatenate([x, np.zeros_like(new_out)], axis=1) for x in dec_outputs]

        sampled_indexes = np.argmax(output[:,0,:], axis=-1)
        for ii, sampled_index in enumerate(sampled_indexes):
            if sampled_index == end_mark: ended[ii] = 1
            if not ended[ii]: decoded_indexes[ii].append(sampled_index)
        if sum(ended) == bs: break
        target_one[:,0] = sampled_indexes
    return decoded_indexes

def decode_batch_beam_search(src_seq, topk, encode_model, decode_model, start_mark, end_mark, max_len=128, early_stop_mult=5):
    N = src_seq.shape[0]
    src_seq = src_seq.repeat(topk, 0)
    enc_ret = encode_model.predict_on_batch(src_seq)
    bs = src_seq.shape[0]

    target_one = np.zeros((bs, 1), dtype='int32')
    target_one[:,0] = start_mark
    d_model = decode_model.inputs[-1].shape[-1]
    n_dlayers = len(decode_model.inputs) - 3
    dec_outputs = [np.zeros((bs, 1, d_model)) for _ in range(n_dlayers)]

    final_results = []
    decoded_indexes = [[] for x in range(bs)]
    decoded_logps = [0] * bs
    lastks = [1 for x in range(N)]
    bests = {}
    for i in range(max_len-1):
        outputs = decode_model.predict_on_batch([target_one, src_seq, enc_ret] + dec_outputs)
        new_dec_outputs, output = outputs[:-1], outputs[-1]
        for dec_output, new_out in zip(dec_outputs, new_dec_outputs): 
            dec_output[:,-1,:] = new_out[:,0,:]

        dec_outputs = [np.concatenate([x, np.zeros_like(new_out)], axis=1) for x in dec_outputs]

        output = np.exp(output[:,0,:])
        output = np.log(output / np.sum(output, -1, keepdims=True) + 1e-8)

        next_dec_outputs = [x.copy() for x in dec_outputs]
        next_decoded_indexes = [1 for x in range(bs)]

        for ii in range(N):
            base = ii * topk
            cands = []
            for k, wprobs in zip(range(lastks[ii]), output[base:,:]):
                prev = base+k
                if len(decoded_indexes[prev]) > 0 and decoded_indexes[prev][-1] == end_mark: continue
                ind = np.argpartition(wprobs, -topk)[-topk:]
                wsorted = [(k,x) for k,x in zip(ind, wprobs[ind])]
                #wsorted = sorted(list(enumerate(wprobs)), key=lambda x:x[-1], reverse=True)   # slow
                for wid, wp in wsorted[:topk]: 
                    wprob = decoded_logps[prev]+wp
                    if wprob < bests.get(ii, -1e5) * early_stop_mult: continue
                    cands.append( (prev, wid, wprob) )
            cands.sort(key=lambda x:x[-1], reverse=True)    
            cands = cands[:topk]
            lastks[ii] = len(cands)
            for kk, zz in enumerate(cands):
                prev, wid, wprob = zz
                npos = base+kk
                for k in range(len(next_dec_outputs)):
                    next_dec_outputs[k][npos,:,:] = dec_outputs[k][prev]
                target_one[npos,0] = wid
                decoded_logps[npos] = wprob
                next_decoded_indexes[npos] = decoded_indexes[prev].copy()
                next_decoded_indexes[npos].append(wid)
                if wid == end_mark:
                    final_results.append( (ii, decoded_indexes[prev].copy(), wprob) ) 
                    if ii not in bests or wprob > bests[ii]: bests[ii] = wprob
        if sum(lastks) == 0: break
        dec_outputs = next_dec_outputs
        decoded_indexes = next_decoded_indexes
    return final_results

class Transformer:
    def __init__(self, i_tokens, o_tokens, len_limit, d_model=256, \
              d_inner_hid=512, n_head=4, layers=2, dropout=0.1, \
              share_word_emb=False):
        self.i_tokens = i_tokens
        self.o_tokens = o_tokens
        self.len_limit = len_limit
        self.d_model = d_model
        self.decode_model = None
        self.readout_model = None
        self.layers = layers
        d_emb = d_model

        self.src_loc_info = True

        d_k = d_v = d_model // n_head
        assert d_k * n_head == d_model and d_v == d_k

        self.pos_emb = PosEncodingLayer(len_limit, d_emb) if self.src_loc_info else None

        self.emb_dropout = Dropout(dropout)

        self.i_word_emb = Embedding(i_tokens.num(), d_emb)
        if share_word_emb: 
            assert i_tokens.num() == o_tokens.num()
            self.o_word_emb = i_word_emb
        else: self.o_word_emb = Embedding(o_tokens.num(), d_emb)

        self.encoder = SelfAttention(d_model, d_inner_hid, n_head, layers, dropout)
        self.decoder = Decoder(d_model, d_inner_hid, n_head, layers, dropout)
        self.target_layer = TimeDistributed(Dense(o_tokens.num(), use_bias=False))

    def compile(self, optimizer='adam', active_layers=999):
        src_seq_input = Input(shape=(None,), dtype='int32')
        tgt_seq_input = Input(shape=(None,), dtype='int32')

        src_seq = src_seq_input
        tgt_seq  = Lambda(lambda x:x[:,:-1])(tgt_seq_input)
        tgt_true = Lambda(lambda x:x[:,1:])(tgt_seq_input)

        src_emb = self.i_word_emb(src_seq)
        tgt_emb = self.o_word_emb(tgt_seq)

        if self.pos_emb: 
            src_emb = add_layer([src_emb, self.pos_emb(src_seq)])
            tgt_emb = add_layer([tgt_emb, self.pos_emb(tgt_seq)])
        src_emb = self.emb_dropout(src_emb)

        enc_output = self.encoder(src_emb, src_seq, active_layers=active_layers)
        dec_output = self.decoder(tgt_emb, tgt_seq, src_seq, enc_output, active_layers=active_layers)   
        final_output = self.target_layer(dec_output)

        def get_loss(y_pred, y_true):
            y_true = tf.cast(y_true, 'int32')
            loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_true, logits=y_pred)
            mask = tf.cast(tf.not_equal(y_true, 0), 'float32')
            loss = tf.reduce_sum(loss * mask, -1) / tf.reduce_sum(mask, -1)
            loss = K.mean(loss)
            return loss

        def get_accu(y_pred, y_true):
            mask = tf.cast(tf.not_equal(y_true, 0), 'float32')
            corr = K.cast(K.equal(K.cast(y_true, 'int32'), K.cast(K.argmax(y_pred, axis=-1), 'int32')), 'float32')
            corr = K.sum(corr * mask, -1) / K.sum(mask, -1)
            return K.mean(corr)
                
        loss = get_loss(final_output, tgt_true)
        self.ppl = K.exp(loss)
        self.accu = get_accu(final_output, tgt_true)

        self.model = Model([src_seq_input, tgt_seq_input], final_output)
        self.model.add_loss([loss])
        
        self.model.compile(optimizer, None)
        self.model.metrics_names.append('ppl')
        self.model.metrics_tensors.append(self.ppl)
        self.model.metrics_names.append('accu')
        self.model.metrics_tensors.append(self.accu)

    def make_src_seq_matrix(self, input_seqs):
        if type(input_seqs[0]) == type(''): input_seqs = [input_seqs]
        maxlen = max(map(len, input_seqs))
        src_seq = np.zeros((len(input_seqs), maxlen+3), dtype='int32')
        src_seq[:,0] = self.i_tokens.startid()
        for i, seq in enumerate(input_seqs):
            for ii, z in enumerate(seq):
                src_seq[i,1+ii] = self.i_tokens.id(z)
            src_seq[i,1+len(seq)] = self.i_tokens.endid()
        return src_seq
    
    def make_readout_decode_model(self, max_output_len=32):
        src_seq_input = Input(shape=(None,), dtype='int32')
        tgt_start_input = Input(shape=(1,), dtype='int32')
        src_seq = src_seq_input
        enc_mask = Lambda(lambda x:K.cast(K.greater(x, 0), 'float32'))(src_seq)
        src_emb = self.i_word_emb(src_seq)
        if self.pos_emb: 
            src_emb = add_layer([src_emb, self.pos_emb(src_seq)])

        src_emb = self.emb_dropout(src_emb)
        enc_output = self.encoder(src_emb, src_seq)

        tgt_emb = self.o_word_emb(tgt_start_input)
        tgt_seq = Lambda(lambda x:K.repeat_elements(x, max_output_len, 1))(tgt_start_input)
        rep_input = Lambda(lambda x:K.repeat_elements(x, max_output_len, 1))(tgt_emb)
    
        cell = ReadoutDecoderCell(self.o_word_emb, self.pos_emb, self.decoder, self.target_layer)
        final_output = InferRNN(cell, return_sequences=True)(rep_input, 
                initial_state=[tgt_start_input, K.ones_like(tgt_start_input), K.zeros_like(tgt_seq)] + \
                        [rep_input for _ in self.decoder.layers], 
                constants=[enc_output, enc_mask])
        final_output = Lambda(lambda x:K.squeeze(x, -1))(final_output)
        self.readout_model = Model([src_seq_input, tgt_start_input], final_output)
        
    def decode_sequence_readout_x(self, X, batch_size=32, max_output_len=64):
        if self.readout_model is None: self.make_readout_decode_model(max_output_len)
        target_seq = np.zeros((X.shape[0], 1), dtype='int32')
        target_seq[:,0] = self.o_tokens.startid()
        ret = self.readout_model.predict([X, target_seq], batch_size=batch_size, verbose=1)
        return ret

    def generate_sentence(self, rets, delimiter=''):
        sents = []
        for x in rets:
            end_pos = min([i for i, z in enumerate(x) if z == self.o_tokens.endid()]+[len(x)])
            rsent = [*map(self.o_tokens.token, x)][:end_pos]
            sents.append(delimiter.join(rsent))
        return sents

    def decode_sequence_readout(self, input_seqs, delimiter=''):
        if self.readout_model is None: self.make_readout_decode_model()
        src_seq = self.make_src_seq_matrix(input_seqs)
        target_seq = np.zeros((src_seq.shape[0],1), dtype='int32')
        target_seq[:,0] = self.o_tokens.startid()
        rets = self.readout_model.predict([src_seq, target_seq])
        rets = self.generate_sentence(rets, delimiter)
        if type(input_seqs[0]) is type('') and len(rets) == 1: rets = rets[0]
        return rets

    def make_fast_decode_model(self):
        src_seq_input = Input(shape=(None,), dtype='int32')
        src_emb = self.i_word_emb(src_seq_input)
        if self.pos_emb: src_emb = add_layer([src_emb, self.pos_emb(src_seq_input)])
        src_emb = self.emb_dropout(src_emb)
        enc_output = self.encoder(src_emb, src_seq_input)
        self.encode_model = Model(src_seq_input, enc_output)

        self.decoder_pre_step = DecoderPerStep(self.decoder)
        
        src_seq_input = Input(shape=(None,), dtype='int32')
        tgt_one_input = Input(shape=(1,), dtype='int32')
        enc_ret_input = Input(shape=(None, self.d_model))
        dec_ret_inputs = [Input(shape=(None, self.d_model)) for _ in self.decoder.layers]

        tgt_pos = Lambda(lambda x:tf.shape(x)[1])(dec_ret_inputs[0])

        tgt_one = self.o_word_emb(tgt_one_input)
        if self.pos_emb: tgt_one = add_layer([tgt_one, self.pos_emb(tgt_pos, pos_input=True)])

        dec_outputs = self.decoder_pre_step([tgt_one, src_seq_input, enc_ret_input]+dec_ret_inputs) 
        final_output = self.target_layer(dec_outputs[-1])

        self.decode_model = Model([tgt_one_input, src_seq_input, enc_ret_input]+dec_ret_inputs, 
                            dec_outputs[:-1]+[final_output])
        

    def decode_sequence_fast(self, input_seqs, batch_size=32, delimiter='', verbose=0):
        if self.decode_model is None: self.make_fast_decode_model()
        src_seq = self.make_src_seq_matrix(input_seqs)

        start_mark, end_mark = self.o_tokens.startid(), self.o_tokens.endid()
        max_len = self.len_limit
        encode_model = self.encode_model
        decode_model = self.decode_model

        decode_batch = lambda x: decode_batch_greedy(x, encode_model, decode_model, start_mark, end_mark, max_len)
        
        rets = []
        rng = range(0, src_seq.shape[0], batch_size)
        if verbose and src_seq.shape[0] > batch_size: rng = tqdm(rng, total=len(rng))
        for iter in rng:
            rets.extend( decode_batch(src_seq[iter:iter+batch_size]) )
            
        rets = [delimiter.join(list(map(self.o_tokens.token, ret))) for ret in rets]
        if type(input_seqs[0]) is type('') and len(rets) == 1: rets = rets[0]
        return rets

    def beam_search(self, input_seqs, topk=5, batch_size=8, length_penalty=1, delimiter='', verbose=0):
        if self.decode_model is None: self.make_fast_decode_model()
        src_seq = self.make_src_seq_matrix(input_seqs)

        start_mark, end_mark = self.o_tokens.startid(), self.o_tokens.endid()
        max_len = self.len_limit
        encode_model = self.encode_model
        decode_model = self.decode_model

        decode_batch = lambda x: decode_batch_beam_search(x, topk, encode_model, decode_model,
                                                    start_mark, end_mark, max_len)
        
        rets = {}
        rng = range(0, src_seq.shape[0], batch_size)
        if verbose and src_seq.shape[0] > batch_size: rng = tqdm(rng, total=len(rng))

        for iter in rng:
            for i, x, y in decode_batch(src_seq[iter:iter+batch_size]):
                rets.setdefault(iter+i, []).append( (x, y/np.power(len(x)+1, length_penalty)) )
        rets = {x:sorted(ys,key=lambda x:x[-1], reverse=True) for x,ys in rets.items()}
        rets = [rets[i] for i in range(len(rets))]

        rets = [[(delimiter.join(list(map(self.o_tokens.token, x))), y) for x, y in r] for r in rets]
        if type(input_seqs[0]) is type('') and len(rets) == 1: rets = rets[0]
        return rets
    
class PosEncodingLayer:
    def __init__(self, max_len, d_emb):
        self.pos_emb_matrix = Embedding(max_len, d_emb, trainable=False, \
                           weights=[GetPosEncodingMatrix(max_len, d_emb)])
    def get_pos_seq(self, x):
        mask = K.cast(K.not_equal(x, 0), 'int32')
        pos = K.cumsum(K.ones_like(x, 'int32'), 1)
        return pos * mask
    def __call__(self, seq, pos_input=False):
        x = seq
        if not pos_input: x = Lambda(self.get_pos_seq)(x)
        return self.pos_emb_matrix(x)

class AddPosEncoding:
    def __call__(self, x):
        _, max_len, d_emb = K.int_shape(x)
        pos = GetPosEncodingMatrix(max_len, d_emb)
        x = Lambda(lambda x:x+pos)(x)
        return x

class LRSchedulerPerStep(Callback):
    def __init__(self, d_model, warmup=4000):
        self.basic = d_model**-0.5
        self.warm = warmup**-1.5
        self.step_num = 0
    def on_batch_begin(self, batch, logs = None):
        self.step_num += 1
        lr = self.basic * min(self.step_num**-0.5, self.step_num*self.warm)
        K.set_value(self.model.optimizer.lr, lr)

add_layer = Lambda(lambda x:x[0]+x[1], output_shape=lambda x:x[0])
# use this because keras may get wrong shapes with Add()([])

class QANet_ConvBlock:
    def __init__(self, dim, n_conv=2, kernel_size=7, dropout=0.1):
        self.convs = [SeparableConv1D(dim, kernel_size, activation='relu', padding='same') for _ in range(n_conv)]
        self.norm = LayerNormalization()
        self.dropout = Dropout(dropout)
    def __call__(self, x):
        for i in range(len(self.convs)):
            z = self.norm(x)
            if i % 2 == 0: z = self.dropout(z)
            z = self.convs[i](z)
            x = add_layer([x, z])
        return x

class QANet_Block:
    def __init__(self, dim, n_head, n_conv, kernel_size, dropout=0.1, add_pos=True):
        self.conv = QANet_ConvBlock(dim, n_conv=n_conv, kernel_size=kernel_size, dropout=dropout)
        self.self_att = MultiHeadAttention(n_head=n_head, d_model=dim, 
                                     d_k=dim//n_head, d_v=dim//n_head, 
                                     dropout=dropout, use_norm=False)
        self.feed_forward = PositionwiseFeedForward(dim, dim, dropout=dropout)
        self.norm = LayerNormalization()
        self.add_pos = add_pos
    def __call__(self, x, mask):
        if self.add_pos: x = AddPosEncoding()(x)
        x = self.conv(x)
        z = self.norm(x)
        z, _ = self.self_att(z, z, z, mask)
        x = add_layer([x, z])
        z = self.norm(x)
        z = self.feed_forward(z)
        x = add_layer([x, z])
        return x

class QANet_Encoder:
    def __init__(self, dim=128, n_head=8, n_conv=2, n_block=1, kernel_size=7, dropout=0.1, add_pos=True):
        self.dim = dim
        self.n_block = n_block
        self.conv_first = SeparableConv1D(dim, 1, padding='same')
        self.enc_block = QANet_Block(dim, n_head=n_head, n_conv=n_conv, kernel_size=kernel_size, 
                                dropout=dropout, add_pos=add_pos)
    def __call__(self, x, mask):
        if K.int_shape(x)[-1] != self.dim:
            x = self.conv_first(x)
        for i in range(self.n_block):
            x = self.enc_block(x, mask)
        return x


if __name__ == '__main__':
    print('done')
271/33:
def build_model():
    inp = Input(shape = (SEQ_LEN, 1))
    
    # LSTM before attention layers
    x = Bidirectional(LSTM(128, return_sequences=True))(inp)
    x = Bidirectional(LSTM(64, return_sequences=True))(x) 
        
    x, slf_attn = MultiHeadAttention(n_head=3, d_model=300, d_k=64, d_v=64, dropout=0.1)(x, x, x)
        
    avg_pool = GlobalAveragePooling1D()(x)
    max_pool = GlobalMaxPooling1D()(x)
    conc = concatenate([avg_pool, max_pool])
    conc = Dense(64, activation="relu")(conc)
    x = Dense(1, activation="sigmoid")(conc)      

    model = Model(inputs = inp, outputs = x)
    model.compile(
        loss = "mean_squared_error", 
        #optimizer = Adam(lr = config["lr"], decay = config["lr_d"]), 
        optimizer = "adam")
    
    # Save entire model to a HDF5 file
    # model.save('my_model.h5')
    
    return model
271/34: multi_head = build_model()
271/35: multi_head.summary()
271/36:
multi_head.fit(X_train, y_train,
                    batch_size=BATCH_SIZE,
                    epochs=EPOCHS,
                    validation_data=(X_valid, y_valid), 
                    #callbacks = [checkpoint , lr_reduce]
             )
271/37:
predicted_stock_price_multi_head = multi_head.predict(X_test)
#predicted_stock_price = scaler.inverse_transform(predicted_stock_price)


predicted_stock_price_multi_head.shape
271/38: predicted_stock_price_multi_head = np.vstack((np.full((60,1), np.nan), predicted_stock_price_multi_head))
271/39:
plt.figure(figsize = (18,9))
plt.plot(test_data, color = 'black', label = 'GE Stock Price')
plt.plot(predicted_stock_price_multi_head, color = 'green', label = 'Predicted GE Mid Price')
plt.title('GE Mid Price Prediction', fontsize=30)
#plt.xticks(range(0,df.shape[0],50),df['Date'].loc[::50],rotation=45)
plt.xlabel('Date')
plt.ylabel('GE Mid Price')
plt.legend(fontsize=18)
plt.show()
271/40:
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import os
from sklearn.preprocessing import MinMaxScaler
from transformer import *
271/41:
def build_model():
    inp = Input(shape = (SEQ_LEN, 1))
    
    # LSTM before attention layers
    x = Bidirectional(LSTM(128, return_sequences=True))(inp)
    x = Bidirectional(LSTM(64, return_sequences=True))(x) 
        
    x, slf_attn = MultiHeadAttention(n_head=3, d_model=300, d_k=64, d_v=64, dropout=0.1)(x, x, x)
        
    avg_pool = GlobalAveragePooling1D()(x)
    max_pool = GlobalMaxPooling1D()(x)
    conc = concatenate([avg_pool, max_pool])
    conc = Dense(64, activation="relu")(conc)
    x = Dense(1, activation="sigmoid")(conc)      

    model = Model(inputs = inp, outputs = x)
    model.compile(
        loss = "mean_squared_error", 
        #optimizer = Adam(lr = config["lr"], decay = config["lr_d"]), 
        optimizer = "adam")
    
    # Save entire model to a HDF5 file
    # model.save('my_model.h5')
    
    return model
271/42: multi_head = build_model()
271/43:
def build_model():
    inp = Input(shape = (SEQ_LEN, 1))
    
    # LSTM before attention layers
    x = Bidirectional(LSTM(128, return_sequences=True))(inp)
    x = Bidirectional(LSTM(64, return_sequences=True))(x) 
        
    x, slf_attn = MultiHeadAttention(n_head=3, d_model=300, d_inner_hid=64, dropout=0.1)(x, x, x)#d_k=64, d_v=64
        
    avg_pool = GlobalAveragePooling1D()(x)
    max_pool = GlobalMaxPooling1D()(x)
    conc = concatenate([avg_pool, max_pool])
    conc = Dense(64, activation="relu")(conc)
    x = Dense(1, activation="sigmoid")(conc)      

    model = Model(inputs = inp, outputs = x)
    model.compile(
        loss = "mean_squared_error", 
        #optimizer = Adam(lr = config["lr"], decay = config["lr_d"]), 
        optimizer = "adam")
    
    # Save entire model to a HDF5 file
    # model.save('my_model.h5')
    
    return model
271/44:
def build_model():
    inp = Input(shape = (SEQ_LEN, 1))
    
    # LSTM before attention layers
    x = Bidirectional(LSTM(128, return_sequences=True))(inp)
    x = Bidirectional(LSTM(64, return_sequences=True))(x) 
        
    x, slf_attn = MultiHeadAttention(n_head=3, d_model=300, d_inner_hid=64, dropout=0.1)(x, x, x)#d_k=64, d_v=64
        
    avg_pool = GlobalAveragePooling1D()(x)
    max_pool = GlobalMaxPooling1D()(x)
    conc = concatenate([avg_pool, max_pool])
    conc = Dense(64, activation="relu")(conc)
    x = Dense(1, activation="sigmoid")(conc)      

    model = Model(inputs = inp, outputs = x)
    model.compile(
        loss = "mean_squared_error", 
        #optimizer = Adam(lr = config["lr"], decay = config["lr_d"]), 
        optimizer = "adam")
    
    # Save entire model to a HDF5 file
    # model.save('my_model.h5')
    
    return model
271/45: multi_head = build_model()
271/46:
def build_model():
    inp = Input(shape = (SEQ_LEN, 1))
    
    # LSTM before attention layers
    x = Bidirectional(LSTM(128, return_sequences=True))(inp)
    x = Bidirectional(LSTM(64, return_sequences=True))(x) 
        
    x, slf_attn = MultiHeadAttention(n_head=3, d_model=300, k=64, v=64, dropout=0.1)(x, x, x)
        
    avg_pool = GlobalAveragePooling1D()(x)
    max_pool = GlobalMaxPooling1D()(x)
    conc = concatenate([avg_pool, max_pool])
    conc = Dense(64, activation="relu")(conc)
    x = Dense(1, activation="sigmoid")(conc)      

    model = Model(inputs = inp, outputs = x)
    model.compile(
        loss = "mean_squared_error", 
        #optimizer = Adam(lr = config["lr"], decay = config["lr_d"]), 
        optimizer = "adam")
    
    # Save entire model to a HDF5 file
    # model.save('my_model.h5')
    
    return model
271/47:
def build_model():
    inp = Input(shape = (SEQ_LEN, 1))
    
    # LSTM before attention layers
    x = Bidirectional(LSTM(128, return_sequences=True))(inp)
    x = Bidirectional(LSTM(64, return_sequences=True))(x) 
        
    x, slf_attn = MultiHeadAttention(n_head=3, d_model=300, k=64, v=64, dropout=0.1)(x, x, x)
        
    avg_pool = GlobalAveragePooling1D()(x)
    max_pool = GlobalMaxPooling1D()(x)
    conc = concatenate([avg_pool, max_pool])
    conc = Dense(64, activation="relu")(conc)
    x = Dense(1, activation="sigmoid")(conc)      

    model = Model(inputs = inp, outputs = x)
    model.compile(
        loss = "mean_squared_error", 
        #optimizer = Adam(lr = config["lr"], decay = config["lr_d"]), 
        optimizer = "adam")
    
    # Save entire model to a HDF5 file
    # model.save('my_model.h5')
    
    return model
271/48: multi_head = build_model()
271/49:
def build_model():
    inp = Input(shape = (SEQ_LEN, 1))
    
    # LSTM before attention layers
    x = Bidirectional(LSTM(128, return_sequences=True))(inp)
    x = Bidirectional(LSTM(64, return_sequences=True))(x) 
        
    x, slf_attn = MultiHeadAttention(n_head=3, d_model=300, dropout=0.1)(x, x, x)
        
    avg_pool = GlobalAveragePooling1D()(x)
    max_pool = GlobalMaxPooling1D()(x)
    conc = concatenate([avg_pool, max_pool])
    conc = Dense(64, activation="relu")(conc)
    x = Dense(1, activation="sigmoid")(conc)      

    model = Model(inputs = inp, outputs = x)
    model.compile(
        loss = "mean_squared_error", 
        #optimizer = Adam(lr = config["lr"], decay = config["lr_d"]), 
        optimizer = "adam")
    
    # Save entire model to a HDF5 file
    # model.save('my_model.h5')
    
    return model
271/50:
def build_model():
    inp = Input(shape = (SEQ_LEN, 1))
    
    # LSTM before attention layers
    x = Bidirectional(LSTM(128, return_sequences=True))(inp)
    x = Bidirectional(LSTM(64, return_sequences=True))(x) 
        
    x, slf_attn = MultiHeadAttention(n_head=3, d_model=300, dropout=0.1)(x, x, x)
        
    avg_pool = GlobalAveragePooling1D()(x)
    max_pool = GlobalMaxPooling1D()(x)
    conc = concatenate([avg_pool, max_pool])
    conc = Dense(64, activation="relu")(conc)
    x = Dense(1, activation="sigmoid")(conc)      

    model = Model(inputs = inp, outputs = x)
    model.compile(
        loss = "mean_squared_error", 
        #optimizer = Adam(lr = config["lr"], decay = config["lr_d"]), 
        optimizer = "adam")
    
    # Save entire model to a HDF5 file
    # model.save('my_model.h5')
    
    return model
271/51: multi_head = build_model()
271/52: multi_head.summary()
271/53:
multi_head.fit(X_train, y_train,
                    batch_size=BATCH_SIZE,
                    epochs=EPOCHS,
                    validation_data=(X_valid, y_valid), 
                    #callbacks = [checkpoint , lr_reduce]
             )
271/54:
predicted_stock_price_multi_head = multi_head.predict(X_test)
#predicted_stock_price = scaler.inverse_transform(predicted_stock_price)


predicted_stock_price_multi_head.shape
271/55:
predicted_stock_price_multi_head = multi_head.predict(X_test)
predicted_stock_price = scaler.inverse_transform(predicted_stock_price)


predicted_stock_price_multi_head.shape
271/56:
predicted_stock_price_multi_head = multi_head.predict(X_test)
predicted_stock_price = scaler.inverse_transform(predicted_stock_price_multi_head)


predicted_stock_price_multi_head.shape
271/57: predicted_stock_price_multi_head
271/58: predicted_stock_price_multi_head = np.vstack((np.full((60,1), np.nan), predicted_stock_price_multi_head))
271/59: predicted_stock_price_multi_head = np.vstack((np.full((60,1), np.nan), predicted_stock_price_multi_head))
271/60:
plt.figure(figsize = (18,9))
plt.plot(test_data, color = 'black', label = 'GE Stock Price')
plt.plot(predicted_stock_price_multi_head, color = 'green', label = 'Predicted GE Mid Price')
plt.title('GE Mid Price Prediction', fontsize=30)
#plt.xticks(range(0,df.shape[0],50),df['Date'].loc[::50],rotation=45)
plt.xlabel('Date')
plt.ylabel('GE Mid Price')
plt.legend(fontsize=18)
plt.show()
301/1:

from IPython.lib import passwd
passwd()
301/2: 
302/1:
#Import the libraries
import numpy as np
import pandas as pd
from sklearn.datasets import load_boston,load_iris
302/2:
#Import the libraries
import numpy as np
import pandas as pd
from sklearn.datasets import load_boston,load_iris
302/3:
#Load the data
boston = load_boston()

#Find features and target
x = boston.data
y = boston.target

#Find the dic keys
print(boston.keys())
302/4:
#find features name
columns = boston.feature_names
columns
302/5:
#Description of dataset
print(boston.DESCR)
302/6:
#Create dataframe
boston_df = pd.DataFrame(boston.data)
boston_df.columns = columns
boston_df_o = boston_df
boston_df.shape
302/7:
#Oulier detection - Univarite - Boxplot
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
sns.boxplot(x=boston_df['DIS'])
302/8:
#Check the correlation between features before multivariate outlier analysis
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline

plt.figure(figsize= (10,10), dpi=100)
sns.heatmap(boston_df.corr())
302/9:
#Multivariate outlier analysis
fig, ax = plt.subplots(figsize=(16,8))
ax.scatter(boston_df['INDUS'], boston_df['TAX'])
ax.set_xlabel('Proportion of non-retail business acres per town')
ax.set_ylabel('Full-value property-tax rate per $10,000')
plt.show()
302/10:
from scipy import stats
import numpy as np
z = np.abs(stats.zscore(boston_df))
print(z)
302/11: z.shape
302/12:
threshold = 3
print(np.where(z > 3))
302/13:
#print(boston_df[np.where(z > 3)])
print(z[55][1])
302/14: boston_df_o = boston_df_o[(z < 3).all(axis=1)]
302/15: boston_df.shape
302/16: boston_df_o.shape
302/17: boston_df_o1 = boston_df
302/18:
Q1 = boston_df_o1.quantile(0.25)
Q3 = boston_df_o1.quantile(0.75)
IQR = Q3 - Q1
print(IQR)

boston_df_out = boston_df_o1[~((boston_df_o1 < (Q1 - 1.5 * IQR)) |(boston_df_o1 > (Q3 + 1.5 * IQR))).any(axis=1)]
302/19: boston_df_out.shape
304/1:
#Import the libraries
import numpy as np
import pandas as pd
from sklearn.datasets import load_boston,load_iris
304/2:
#Import the libraries
import numpy as np
import pandas as pd
from sklearn.datasets import load_boston,load_iris
304/3:
#Load the data
boston = load_boston()

#Find features and target
x = boston.data
y = boston.target

#Find the dic keys
print(boston.keys())
304/4:
#find features name
columns = boston.feature_names
columns
304/5:
#Description of dataset
print(boston.DESCR)
304/6:
#Create dataframe
boston_df = pd.DataFrame(boston.data)
boston_df.columns = columns
boston_df_o = boston_df
boston_df.shape
304/7:
#Oulier detection - Univarite - Boxplot
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
sns.boxplot(x=boston_df['DIS'])
304/8:
#Check the correlation between features before multivariate outlier analysis
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline

plt.figure(figsize= (10,10), dpi=100)
sns.heatmap(boston_df.corr())
304/9:
#Multivariate outlier analysis
fig, ax = plt.subplots(figsize=(16,8))
ax.scatter(boston_df['INDUS'], boston_df['TAX'])
ax.set_xlabel('Proportion of non-retail business acres per town')
ax.set_ylabel('Full-value property-tax rate per $10,000')
plt.show()
304/10:
from scipy import stats
import numpy as np
z = np.abs(stats.zscore(boston_df))
print(z)
304/11: z.shape
304/12:
threshold = 3
print(np.where(z > 3))
304/13:
#print(boston_df[np.where(z > 3)])
print(z[55][1])
304/14: boston_df_o = boston_df_o[(z < 3).all(axis=1)]
304/15: boston_df.shape
304/16: boston_df_o.shape
304/17: boston_df_o1 = boston_df
304/18:
Q1 = boston_df_o1.quantile(0.25)
Q3 = boston_df_o1.quantile(0.75)
IQR = Q3 - Q1
print(IQR)

boston_df_out = boston_df_o1[~((boston_df_o1 < (Q1 - 1.5 * IQR)) |(boston_df_o1 > (Q3 + 1.5 * IQR))).any(axis=1)]
304/19: boston_df_out.shape
306/1:
fig, ax = plt.subplots(figsize=(16,8))
ax.scatter(boston_df_o['INDUS'], boston_df_o['TAX'])
ax.set_xlabel('Proportion of non-retail business acres per town')
ax.set_ylabel('Full-value property-tax rate per $10,000')
306/2:
#Import the libraries
import numpy as np
import pandas as pd
from sklearn.datasets import load_boston,load_iris
306/3:
#Load the data
boston = load_boston()

#Find features and target
x = boston.data
y = boston.target

#Find the dic keys
print(boston.keys())
306/4:
#find features name
columns = boston.feature_names
columns
306/5:
#Description of dataset
print(boston.DESCR)
306/6:
#Create dataframe
boston_df = pd.DataFrame(boston.data)
boston_df.columns = columns
boston_df_o = boston_df
boston_df.shape
306/7:
#Oulier detection - Univarite - Boxplot
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
sns.boxplot(x=boston_df['DIS'])
306/8:
#Check the correlation between features before multivariate outlier analysis
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline

plt.figure(figsize= (10,10), dpi=100)
sns.heatmap(boston_df.corr())
306/9:
#Multivariate outlier analysis
fig, ax = plt.subplots(figsize=(16,8))
ax.scatter(boston_df['INDUS'], boston_df['TAX'])
ax.set_xlabel('Proportion of non-retail business acres per town')
ax.set_ylabel('Full-value property-tax rate per $10,000')
plt.show()
306/10:
from scipy import stats
import numpy as np
z = np.abs(stats.zscore(boston_df))
print(z)
306/11: z.shape
306/12:
threshold = 3
print(np.where(z > 3))
306/13:
#print(boston_df[np.where(z > 3)])
print(z[55][1])
306/14: boston_df_o = boston_df_o[(z < 3).all(axis=1)]
306/15: boston_df.shape
306/16: boston_df_o.shape
306/17:
fig, ax = plt.subplots(figsize=(16,8))
ax.scatter(boston_df_o['INDUS'], boston_df_o['TAX'])
ax.set_xlabel('Proportion of non-retail business acres per town')
ax.set_ylabel('Full-value property-tax rate per $10,000')
306/18: boston_df_o1 = boston_df
306/19:
Q1 = boston_df_o1.quantile(0.25)
Q3 = boston_df_o1.quantile(0.75)
IQR = Q3 - Q1
print(IQR)

boston_df_out = boston_df_o1[~((boston_df_o1 < (Q1 - 1.5 * IQR)) |(boston_df_o1 > (Q3 + 1.5 * IQR))).any(axis=1)]
306/20: boston_df_out.shape
306/21:
boston_df_o = boston_df_o[(z < 3).all(axis=1)]
boston_df_o
306/22: boston_df_o = boston_df_o[(z < 3).all(axis=1)]
306/23:
#Import the libraries
import numpy as np
import pandas as pd
from sklearn.datasets import load_boston,load_iris
306/24:
#Load the data
boston = load_boston()

#Find features and target
x = boston.data
y = boston.target

#Find the dic keys
print(boston.keys())
306/25:
#find features name
columns = boston.feature_names
columns
306/26:
#Description of dataset
print(boston.DESCR)
306/27:
#Create dataframe
boston_df = pd.DataFrame(boston.data)
boston_df.columns = columns
boston_df_o = boston_df
boston_df.shape
306/28:
#Oulier detection - Univarite - Boxplot
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
sns.boxplot(x=boston_df['DIS'])
306/29:
#Check the correlation between features before multivariate outlier analysis
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline

plt.figure(figsize= (10,10), dpi=100)
sns.heatmap(boston_df.corr())
306/30:
#Multivariate outlier analysis
fig, ax = plt.subplots(figsize=(16,8))
ax.scatter(boston_df['INDUS'], boston_df['TAX'])
ax.set_xlabel('Proportion of non-retail business acres per town')
ax.set_ylabel('Full-value property-tax rate per $10,000')
plt.show()
306/31:
from scipy import stats
import numpy as np
z = np.abs(stats.zscore(boston_df))
print(z)
306/32: z.shape
306/33:
threshold = 3
print(np.where(z > 3))
306/34:
#print(boston_df[np.where(z > 3)])
print(z[55][1])
306/35: boston_df_o = boston_df_o[(z < 3).all(axis=1)]
306/36: boston_df.shape
306/37: boston_df_o.shape
306/38:
fig, ax = plt.subplots(figsize=(16,8))
ax.scatter(boston_df_o['INDUS'], boston_df_o['TAX'])
ax.set_xlabel('Proportion of non-retail business acres per town')
ax.set_ylabel('Full-value property-tax rate per $10,000')
306/39: boston_df_o1 = boston_df
306/40:
Q1 = boston_df_o1.quantile(0.25)
Q3 = boston_df_o1.quantile(0.75)
IQR = Q3 - Q1
print(IQR)

boston_df_out = boston_df_o1[~((boston_df_o1 < (Q1 - 1.5 * IQR)) |(boston_df_o1 > (Q3 + 1.5 * IQR))).any(axis=1)]
306/41: boston_df_out.shape
306/42:
boston_df_o = boston_df_o[(z < 3).all(axis=1)]
boston_df_o
306/43:
boston_df_o = boston_df_o[(z < 3).all(axis=1)]
boston_df_o
306/44: boston_df_o = boston_df_o[(z < 3).all(axis=1)]
306/45: boston_df_o = boston_df_o[(z < 3).all(axis=1)]
306/46: boston_df_o = boston_df_o[(z < 3).all(axis=1)]
306/47: boston_df_o = boston_df_o[(z < 3).all(axis=1)]
306/48:
#Import the libraries
import numpy as np
import pandas as pd
from sklearn.datasets import load_boston,load_iris
306/49:
#Load the data
boston = load_boston()

#Find features and target
x = boston.data
y = boston.target

#Find the dic keys
print(boston.keys())
306/50:
#find features name
columns = boston.feature_names
columns
306/51:
#Description of dataset
print(boston.DESCR)
306/52:
#Create dataframe
boston_df = pd.DataFrame(boston.data)
boston_df.columns = columns
boston_df_o = boston_df
boston_df.shape
306/53:
#Oulier detection - Univarite - Boxplot
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
sns.boxplot(x=boston_df['DIS'])
306/54:
#Check the correlation between features before multivariate outlier analysis
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline

plt.figure(figsize= (10,10), dpi=100)
sns.heatmap(boston_df.corr())
306/55:
#Multivariate outlier analysis
fig, ax = plt.subplots(figsize=(16,8))
ax.scatter(boston_df['INDUS'], boston_df['TAX'])
ax.set_xlabel('Proportion of non-retail business acres per town')
ax.set_ylabel('Full-value property-tax rate per $10,000')
plt.show()
306/56:
from scipy import stats
import numpy as np
z = np.abs(stats.zscore(boston_df))
print(z)
306/57: z.shape
306/58:
threshold = 3
print(np.where(z > 3))
306/59:
#print(boston_df[np.where(z > 3)])
print(z[55][1])
306/60:
boston_df_o = boston_df_o[(z < 3).all(axis=1)]
boston_df_o
306/61: boston_df.shape
306/62: boston_df_o.shape
306/63:
fig, ax = plt.subplots(figsize=(16,8))
ax.scatter(boston_df_o['INDUS'], boston_df_o['TAX'])
ax.set_xlabel('Proportion of non-retail business acres per town')
ax.set_ylabel('Full-value property-tax rate per $10,000')
306/64: boston_df_o1 = boston_df
306/65:
Q1 = boston_df_o1.quantile(0.25)
Q3 = boston_df_o1.quantile(0.75)
IQR = Q3 - Q1
print(IQR)

boston_df_out = boston_df_o1[~((boston_df_o1 < (Q1 - 1.5 * IQR)) |(boston_df_o1 > (Q3 + 1.5 * IQR))).any(axis=1)]
306/66: boston_df_out.shape
306/67:
plt.figure(figsize= (4,4), dpi=100)
sns.heatmap(boston_df.corr())
306/68:
plt.figure(figsize= (6,6), dpi=100)
sns.heatmap(boston_df.corr())
306/69:
plt.figure(figsize= (4,4), dpi=100)
sns.heatmap(boston_df.corr())
306/70:
plt.figure(figsize= (4,4), dpi=200)
sns.heatmap(boston_df.corr())
306/71:
plt.figure(figsize= (4,4), dpi=100)
sns.heatmap(boston_df.corr())
306/72:
#Multivariate outlier analysis
fig, ax = plt.subplots(figsize=(11,8.5))
ax.scatter(boston_df['INDUS'], boston_df['TAX'])
ax.set_xlabel('Proportion of non-retail business acres per town')
ax.set_ylabel('Full-value property-tax rate per $10,000')
plt.show()
306/73:
#Multivariate outlier analysis
fig, ax = plt.subplots(figsize=(8.5,8.5))
ax.scatter(boston_df['INDUS'], boston_df['TAX'])
ax.set_xlabel('Proportion of non-retail business acres per town')
ax.set_ylabel('Full-value property-tax rate per $10,000')
plt.show()
306/74:
#Multivariate outlier analysis
fig, ax = plt.subplots(figsize=(11,8.5))
ax.scatter(boston_df['INDUS'], boston_df['TAX'])
ax.set_xlabel('Proportion of non-retail business acres per town')
ax.set_ylabel('Full-value property-tax rate per $10,000')
plt.show()
306/75:
#Multivariate outlier analysis
fig, ax = plt.subplots(figsize=(11,8.5))
ax.scatter(boston_df['INDUS'], boston_df['TAX'])
ax.set_xlabel('Proportion of non-retail business acres per town')
ax.set_ylabel('Full-value property-tax rate per $10,000')
306/76:
#Import the libraries
import os
import numpy as np
import pandas as pd
from sklearn.datasets import load_boston,load_iris, preprocessing
import matplotlib.pyplot as plt

import seaborn as sns
sns.set(color_codes=True)
306/77:
#Import the libraries
import os
import numpy as np
import pandas as pd
from sklearn.datasets import load_boston,load_iris
from sklearn import preprocessing
import matplotlib.pyplot as plt

import seaborn as sns
sns.set(color_codes=True)
306/78:
#Load the data
boston = load_boston()

#Find features and target
x = boston.data
y = boston.target

#Find the dic keys
print(boston.keys())
boston
306/79:
#Load the data
boston = load_boston()

#Find features and target
x = boston.data
y = boston.target

#Find the dic keys
print(boston.keys())
306/80:
#Create dataframe
boston_df = pd.DataFrame(boston.data)
boston_df.columns = columns
boston_df_o = boston_df
print(boston_df.shape)
boston_df
306/81:
#Create dataframe
boston_df = pd.DataFrame(boston.data)
boston_df.columns = columns
boston_df = boston_df[['Indus'],['Tax']]
print(boston_df.shape)
boston_df
306/82:
#Create dataframe
boston_df = pd.DataFrame(boston.data)
boston_df.columns = columns
boston_df = boston_df[['INDUS'],['TAX']]
print(boston_df.shape)
boston_df
306/83:
#Create dataframe
boston_df = pd.DataFrame(boston.data)
boston_df.columns = columns
boston_df = boston_df[['INDUS'],['TAX']]
print(boston_df.shape)
boston_df
306/84:
#Create dataframe
boston_df = pd.DataFrame(boston.data)
boston_df.columns = columns
boston = boston_df[['INDUS'],['TAX']]
print(boston_df.shape)
boston_df
306/85:
#Create dataframe
boston_df = pd.DataFrame(boston.data)
boston_df.columns = columns
boston_df = boston_df[['TAX']]
print(boston_df.shape)
boston_df
306/86:
#Create dataframe
boston_df = pd.DataFrame(boston.data)
boston_df.columns = columns
boston_df = boston_df[['TAX'],['INDUS']]
print(boston_df.shape)
boston_df
307/1:
#Import the libraries
import os
import numpy as np
import pandas as pd
from sklearn.datasets import load_boston,load_iris
from sklearn import preprocessing
import matplotlib.pyplot as plt

import seaborn as sns
sns.set(color_codes=True)
307/2:
#Load the data
boston = load_boston()

#Find features and target
x = boston.data
y = boston.target

#Find the dic keys
print(boston.keys())
307/3:
#find features name
columns = boston.feature_names
columns
307/4:
#Create dataframe
boston_df = pd.DataFrame(boston.data)
boston_df.columns = columns
boston_df = boston_df[['TAX'],['INDUS']]
print(boston_df.shape)
boston_df
307/5:
#Create dataframe
boston_df = pd.DataFrame(boston.data)
boston_df.columns = columns
print(boston_df.shape)
boston_df
boston_df[['TAX'],['INDUS']]
307/6:
#Create dataframe
boston_df = pd.DataFrame(boston.data)
boston_df.columns = columns
print(boston_df.shape)
boston_df = boston_df[['TAX','INDUS']]
boston_df
307/7:
plt.figure(figsize= (4,4), dpi=100)
sns.heatmap(boston_df.corr())
307/8:
plt.figure(figsize= (4,4), dpi=100)
sns.heatmap(boston_df.corr())
307/9:
plt.figure(figsize= (4,4), dpi=100)
sns.heatmap(boston_df.corr())
307/10:
#Create dataframe
boston_df = pd.DataFrame(boston.data)
boston_df.columns = columns
print(boston_df.shape)
307/11:
plt.figure(figsize= (4,4), dpi=100)
sns.heatmap(boston_df.corr())
307/12:
#Multivariate outlier analysis
boston_df = boston_df[['TAX','INDUS']]
fig, ax = plt.subplots(figsize=(11,8.5))
ax.scatter(boston_df[0], boston_df[1])
ax.set_xlabel('Proportion of non-retail business acres per town')
ax.set_ylabel('Full-value property-tax rate per $10,000')
307/13:
#Multivariate outlier analysis
boston_df = boston_df[['TAX','INDUS']]
fig, ax = plt.subplots(figsize=(11,8.5))
ax.scatter(boston_df[[0]], boston_df[[1]])
ax.set_xlabel('Proportion of non-retail business acres per town')
ax.set_ylabel('Full-value property-tax rate per $10,000')
307/14:
#Multivariate outlier analysis
boston_df = boston_df[['TAX','INDUS']]
fig, ax = plt.subplots(figsize=(11,8.5))
ax.scatter(boston_df['INDUS'], boston_df['TAX'])
ax.set_xlabel('Proportion of non-retail business acres per town')
ax.set_ylabel('Full-value property-tax rate per $10,000')
307/15:


from sklearn.decomposition import PCA
307/16:

pca = PCA(n_components=boston_df.shape[1], svd_solver= 'full')
boston_df.shape[1]
307/17:
pca = PCA(n_components=boston_df.shape[1], svd_solver= 'full')
df = pd.DataFrame(pca.fit_transform(boston_df),index=boston_df.index )
307/18:
pca = PCA(n_components=boston_df.shape[1], svd_solver= 'full')
df = pd.DataFrame(pca.fit_transform(boston_df),index=boston_df.index )
df
307/19:
#Create dataframe
boston_df = pd.DataFrame(boston.data)
boston_df.columns = columns
print(boston_df.shape)
boston_df
307/20:
#Create dataframe
boston_df = pd.DataFrame(boston.data)
boston_df.columns = columns
print(boston_df.shape)
307/21:
#Multivariate outlier analysis
boston_df = boston_df[['TAX','INDUS']]
fig, ax = plt.subplots(figsize=(11,8.5))
ax.scatter(boston_df['INDUS'], boston_df['TAX'])
ax.set_xlabel('Proportion of non-retail business acres per town')
ax.set_ylabel('Full-value property-tax rate per $10,000')
boston_df
307/22:
boston_df = boston_df[['TAX','INDUS']]
boston_df
307/23:
#Multivariate outlier analysis
fig, ax = plt.subplots(figsize=(11,8.5))
ax.scatter(boston_df['INDUS'], boston_df['TAX'])
ax.set_xlabel('Proportion of non-retail business acres per town')
ax.set_ylabel('Full-value property-tax rate per $10,000')
307/24:
pca = PCA(n_components=boston_df.shape[1], svd_solver= 'full')
df = pd.DataFrame(pca.fit_transform(boston_df),index=boston_df.index )
df
307/25:
def cov_matrix(data, verbose=False):
    covariance_matrix = np.cov(data, rowvar=False)
    if is_pos_def(covariance_matrix):
        inv_covariance_matrix = np.linalg.inv(covariance_matrix)
        if is_pos_def(inv_covariance_matrix):
            return covariance_matrix, inv_covariance_matrix
        else:
            print("Error: Inverse of Covariance Matrix is not positive definite!")
    else:
        print("Error: Covariance Matrix is not positive definite!")

def MahalanobisDist(inv_cov_matrix, mean_distr, data, verbose=False):
    inv_covariance_matrix = inv_cov_matrix
    vars_mean = mean_distr
    diff = data - vars_mean
    md = []
    for i in range(len(diff)):
        md.append(np.sqrt(diff[i].dot(inv_covariance_matrix).dot(diff[i])))
    return md

def MD_detectOutliers(dist, extreme=False, verbose=False):
    k = 3. if extreme else 2.
    threshold = np.mean(dist) * k
    outliers = []
    for i in range(len(dist)):
        if dist[i] >= threshold:
            outliers.append(i)  # index of the outlier
    return np.array(outliers)

def MD_threshold(dist, extreme=False, verbose=False):
    k = 3. if extreme else 2.
    threshold = np.mean(dist) * k
    return threshold

def is_pos_def(A):
    if np.allclose(A, A.T):
        try:
            np.linalg.cholesky(A)
            return True
        except np.linalg.LinAlgError:
            return False
    else:
        return False
307/26: df.values
307/27: df.values.shape
307/28: type(df.values)
307/29: cov_matrix, inv_cov_matrix  = cov_matrix(df.values)
307/30:
cov_matrix, inv_cov_matrix  = cov_matrix(df.values)
cov_matrix
307/31: cov_matrix, inv_cov_matrix  = cov_matrix(df.values)
307/32: cov_matrixs, inv_cov_matrix  = cov_matrix(df.values)
307/33:
def cov_matrix(data, verbose=False):
    covariance_matrix = np.cov(data, rowvar=False)
    if is_pos_def(covariance_matrix):
        inv_covariance_matrix = np.linalg.inv(covariance_matrix)
        if is_pos_def(inv_covariance_matrix):
            return covariance_matrix, inv_covariance_matrix
        else:
            print("Error: Inverse of Covariance Matrix is not positive definite!")
    else:
        print("Error: Covariance Matrix is not positive definite!")

def MahalanobisDist(inv_cov_matrix, mean_distr, data, verbose=False):
    inv_covariance_matrix = inv_cov_matrix
    vars_mean = mean_distr
    diff = data - vars_mean
    md = []
    for i in range(len(diff)):
        md.append(np.sqrt(diff[i].dot(inv_covariance_matrix).dot(diff[i])))
    return md

def MD_detectOutliers(dist, extreme=False, verbose=False):
    k = 3. if extreme else 2.
    threshold = np.mean(dist) * k
    outliers = []
    for i in range(len(dist)):
        if dist[i] >= threshold:
            outliers.append(i)  # index of the outlier
    return np.array(outliers)

def MD_threshold(dist, extreme=False, verbose=False):
    k = 3. if extreme else 2.
    threshold = np.mean(dist) * k
    return threshold

def is_pos_def(A):
    if np.allclose(A, A.T):
        try:
            np.linalg.cholesky(A)
            return True
        except np.linalg.LinAlgError:
            return False
    else:
        return False
307/34:
def cov_matrix(data, verbose=False):
    covariance_matrix = np.cov(data, rowvar=False)
    if is_pos_def(covariance_matrix):
        inv_covariance_matrix = np.linalg.inv(covariance_matrix)
        if is_pos_def(inv_covariance_matrix):
            return covariance_matrix, inv_covariance_matrix
        else:
            print("Error: Inverse of Covariance Matrix is not positive definite!")
    else:
        print("Error: Covariance Matrix is not positive definite!")

def MahalanobisDist(inv_cov_matrix, mean_distr, data, verbose=False):
    inv_covariance_matrix = inv_cov_matrix
    vars_mean = mean_distr
    diff = data - vars_mean
    md = []
    for i in range(len(diff)):
        md.append(np.sqrt(diff[i].dot(inv_covariance_matrix).dot(diff[i])))
    return md

def MD_detectOutliers(dist, extreme=False, verbose=False):
    k = 3. if extreme else 2.
    threshold = np.mean(dist) * k
    outliers = []
    for i in range(len(dist)):
        if dist[i] >= threshold:
            outliers.append(i)  # index of the outlier
    return np.array(outliers)

def MD_threshold(dist, extreme=False, verbose=False):
    k = 3. if extreme else 2.
    threshold = np.mean(dist) * k
    return threshold

def is_pos_def(A):
    if np.allclose(A, A.T):
        try:
            np.linalg.cholesky(A)
            return True
        except np.linalg.LinAlgError:
            return False
    else:
        return False
307/35: cov_matrixs, inv_cov_matrix  = cov_matrix(df.values)
307/36:
cov_matrixs, inv_cov_matrix  = cov_matrix(df.values)
cov_matrixs
307/37:
cov_matrixs, inv_cov_matrix  = cov_matrix(df.values)
mean = df.values.mean
mean
307/38:
cov_matrixs, inv_cov_matrix  = cov_matrix(df.values)
mean = df.values.mean()
mean
307/39:
cov_matrixs, inv_cov_matrix  = cov_matrix(df.values)
mean = df.values.mean(axis=0)
mean
307/40:
cov_matrixs, inv_cov_matrix  = cov_matrix(df.values)
mean = df.values.mean(axis=0)
df.sum
307/41:
cov_matrixs, inv_cov_matrix  = cov_matrix(df.values)
mean = df.values.mean(axis=0)
df.sammury()
307/42:
cov_matrixs, inv_cov_matrix  = cov_matrix(df.values)
mean = df.values.mean(axis=0)
df
307/43:
cov_matrixs, inv_cov_matrix  = cov_matrix(df.values)
mean = df.values.mean(axis=0)
df[100:150]
307/44:
cov_matrixs, inv_cov_matrix  = cov_matrix(df.values)
mean = df.values.mean(axis=0)
np.equal(np.cov(df.values), cov_matrixs)
307/45:
cov_matrixs, inv_cov_matrix  = cov_matrix(df.values)
mean = df.values.mean(axis=0)
# np.equal(np.cov(df.values), cov_matrixs)
307/46:
cov_matrixs, inv_cov_matrix  = cov_matrix(df.values)
mean = df.values.mean(axis=0)
# np.equal(np.cov(df.values), cov_matrixs)
cov_matrixs
307/47:
cov_matrixs, inv_cov_matrix  = cov_matrix(df.values)
mean = df.values.mean(axis=0)
np.equal(np.cov(df.values,rowvar=False), cov_matrixs)
307/48:
cov_matrix = np.cov(df.values,rowvar=False)
inv_cov = np.linalg.inv(cov_matrix)
mean = df.values.mean(axis=0)

assert is_pos_def(cov_matrix) and is_pos_def(inv_cov)
np.linalg.matmul(cov_matrix,inv_cov)
307/49:
cov_matrix = np.cov(df.values,rowvar=False)
inv_cov = np.linalg.inv(cov_matrix)
mean = df.values.mean(axis=0)

assert is_pos_def(cov_matrix) and is_pos_def(inv_cov)
np.matmul(cov_matrix,inv_cov)
307/50:
cov_matrix = np.cov(df.values,rowvar=False)
inv_cov = np.linalg.inv(cov_matrix)
mean = df.values.mean(axis=0)

assert is_pos_def(cov_matrix) and is_pos_def(inv_cov)
np.matmul(cov_matrix,inv_cov).astype(np.int32)
307/51:
cov_matrix = np.cov(df.values,rowvar=False)
inv_cov = np.linalg.inv(cov_matrix)
mean = df.values.mean(axis=0)

assert is_pos_def(cov_matrix) and is_pos_def(inv_cov)
np.matmul(cov_matrix,inv_cov).astype(np.int64)
307/52:
cov_matrix = np.cov(df.values,rowvar=False)
inv_cov = np.linalg.inv(cov_matrix)
mean = df.values.mean(axis=0)

assert is_pos_def(cov_matrix) and is_pos_def(inv_cov)
np.matmul(cov_matrix,inv_cov).astype(np.float)
307/53:
cov_matrix = np.cov(df.values,rowvar=False)
inv_cov = np.linalg.inv(cov_matrix)
mean = df.values.mean(axis=0)

assert is_pos_def(cov_matrix) and is_pos_def(inv_cov)
np.matmul(cov_matrix,inv_cov).astype(np.float16)
307/54:
cov_matrix = np.cov(df.values,rowvar=False)
inv_cov = np.linalg.inv(cov_matrix)
mean = df.values.mean(axis=0)

assert is_pos_def(cov_matrix) and is_pos_def(inv_cov)
np.matmul(cov_matrix,inv_cov).astype(np.float32)
307/55:
cov_matrix = np.cov(df.values,rowvar=False)
inv_cov = np.linalg.inv(cov_matrix)
mean = df.values.mean(axis=0)

assert is_pos_def(cov_matrix) and is_pos_def(inv_cov)
np.matmul(cov_matrix,inv_cov).astype(np.float16)
307/56:
md = MahalanobisDist(inv_cov, mean, df.values, verbose=False)
threshold = MD_threshold(df.values, extreme = True)
307/57:
md = MahalanobisDist(inv_cov, mean, df.values, verbose=False)
threshold = MD_threshold(df.values, extreme = True)
threshold
307/58:
md = MahalanobisDist(inv_cov, mean, df.values, verbose=False)
threshold = MD_threshold(df.values, extreme = True)
md
307/59:
md = MahalanobisDist(inv_cov, mean, df.values, verbose=False)
threshold = MD_threshold(df.values, extreme = True)
threshold
307/60:
md = MahalanobisDist(inv_cov, mean, df.values, verbose=False)
threshold = MD_threshold(df.values, extreme = True)
threshold
307/61:
plt.figure()
sns.distplot(np.square(md),bins = 10, kde= False)
plt.xlim([0.0,15])
307/62:
plt.figure()
sns.distplot(np.square(md),bins = 10, kde= False)
307/63:
plt.figure()
sns.distplot(np.square(md),bins = 10, kde= False)
plt.xlim([0.0,15])
307/64:
plt.figure()
sns.distplot(np.square(md),bins = 10, kde= False)
plt.xlim([0.0,15])

plt.figure()
sns.distplot(dist_train,
             bins = 10, 
             kde= True, 
            color = 'green');
plt.xlim([0.0,5])
plt.xlabel('Mahalanobis dist')
307/65:
plt.figure()
sns.distplot(np.square(md),bins = 10, kde= False)
plt.xlim([0.0,15])

plt.figure()
sns.distplot(md,
             bins = 10, 
             kde= True, 
            color = 'green');
plt.xlim([0.0,5])
plt.xlabel('Mahalanobis dist')
307/66:
plt.figure()
sns.distplot(np.square(md),bins = 10, kde= False)
plt.xlim([0.0,15])

plt.figure()
sns.distplot(md,
             bins = 1, 
             kde= True, 
            color = 'green');
plt.xlim([0.0,5])
plt.xlabel('Mahalanobis dist')
307/67:
plt.figure()
sns.distplot(np.square(md),bins = 10, kde= False)
plt.xlim([0.0,15])

plt.figure()
sns.distplot(md,
             bins = 20, 
             kde= True, 
            color = 'green');
plt.xlim([0.0,5])
plt.xlabel('Mahalanobis dist')
307/68:
plt.figure()
sns.distplot(np.square(md),bins = 20, kde= False)
plt.xlim([0.0,15])

plt.figure()
sns.distplot(md,
             bins = 20, 
             kde= True, 
            color = 'green');
plt.xlim([0.0,5])
plt.xlabel('Mahalanobis dist')
307/69:
plt.figure()
sns.distplot(np.square(md),bins = 20, kde= False)
plt.xlim([0.0,15])

plt.figure()
sns.distplot(md,
             bins = 20, 
             kde= False, 
            color = 'green');
plt.xlim([0.0,5])
plt.xlabel('Mahalanobis dist')
307/70:
plt.figure()
sns.distplot(np.square(md),bins = 20, kde= False)
plt.xlim([0.0,15])

plt.figure()
sns.distplot(md,
             bins = 20, 
             kde= True, 
            color = 'green');
plt.xlim([0.0,5])
plt.xlabel('Mahalanobis dist')
307/71:
md = MahalanobisDist(inv_cov, mean, df.values, verbose=False)
threshold = MD_threshold(df.values, extreme = False)
threshold
307/72:
md = MahalanobisDist(inv_cov, mean, df.values, verbose=False)
threshold = MD_threshold(df.values, extreme = True)
threshold
307/73:
md = MahalanobisDist(inv_cov, mean, df.values, verbose=False)
threshold = MD_threshold(md, extreme = True)
threshold
307/74:
plt.figure()
sns.distplot(np.square(md),bins = 20, kde= False)
plt.xlim([0.0,15])

plt.figure()
sns.distplot(md,
             bins = 20, 
             kde= True, 
            color = 'green');
plt.xlim([0.0,5])
plt.xlabel('Mahalanobis dist')
307/75:
df['mob_dist'] = md
df['thresh'] = threshold
307/76:
df['mob_dist'] = md
df['thresh'] = threshold
df['anomaly'] = df.mob_dist > df.thresh
df
307/77:
df['mob_dist'] = md
df['thresh'] = threshold
df['anomaly'] = df.mob_dist > df.thresh
df[df.anomaly]
307/78:
md = MahalanobisDist(inv_cov, mean, df.values, verbose=False)
threshold = MD_threshold(md, extreme = False)
threshold
307/79:
#Import the libraries
import os
import numpy as np
import pandas as pd
from sklearn import preprocessing
from sklearn.datasets import load_boston,load_iris
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

import seaborn as sns
sns.set(color_codes=True)
307/80:
#Load the data
boston = load_boston()

#Find features and target
x = boston.data
y = boston.target

#Find the dic keys
print(boston.keys())
307/81:
#find features name
columns = boston.feature_names
columns
307/82:
#Create dataframe
boston_df = pd.DataFrame(boston.data)
boston_df.columns = columns
print(boston_df.shape)
307/83:
plt.figure(figsize= (4,4), dpi=100)
sns.heatmap(boston_df.corr())
307/84:
boston_df = boston_df[['TAX','INDUS']]
boston_df
307/85:
#Multivariate outlier analysis
fig, ax = plt.subplots(figsize=(11,8.5))
ax.scatter(boston_df['INDUS'], boston_df['TAX'])
ax.set_xlabel('Proportion of non-retail business acres per town')
ax.set_ylabel('Full-value property-tax rate per $10,000')
307/86:
pca = PCA(n_components=boston_df.shape[1], svd_solver= 'full')
df = pd.DataFrame(pca.fit_transform(boston_df),index=boston_df.index )
df
307/87:
def cov_matrix(data, verbose=False):
    covariance_matrix = np.cov(data, rowvar=False)
    if is_pos_def(covariance_matrix):
        inv_covariance_matrix = np.linalg.inv(covariance_matrix)
        if is_pos_def(inv_covariance_matrix):
            return covariance_matrix, inv_covariance_matrix
        else:
            print("Error: Inverse of Covariance Matrix is not positive definite!")
    else:
        print("Error: Covariance Matrix is not positive definite!")

def MahalanobisDist(inv_cov_matrix, mean_distr, data, verbose=False):
    inv_covariance_matrix = inv_cov_matrix
    vars_mean = mean_distr
    diff = data - vars_mean
    md = []
    for i in range(len(diff)):
        md.append(np.sqrt(diff[i].dot(inv_covariance_matrix).dot(diff[i])))
    return md

def MD_detectOutliers(dist, extreme=False, verbose=False):
    k = 3. if extreme else 2.
    threshold = np.mean(dist) * k
    outliers = []
    for i in range(len(dist)):
        if dist[i] >= threshold:
            outliers.append(i)  # index of the outlier
    return np.array(outliers)

def MD_threshold(dist, extreme=False, verbose=False):
    k = 3. if extreme else 2.
    threshold = np.mean(dist) * k
    return threshold

# Check that matrix is positive definite
def is_pos_def(A):
    if np.allclose(A, A.T):
        try:
            np.linalg.cholesky(A)
            return True
        except np.linalg.LinAlgError:
            return False
    else:
        return False
307/88:
cov_matrix = np.cov(df.values,rowvar=False)
inv_cov = np.linalg.inv(cov_matrix)
mean = df.values.mean(axis=0)

# Check matrices are positive definite:https://en.wikipedia.org/wiki/Definiteness_of_a_matrix 
assert is_pos_def(cov_matrix) and is_pos_def(inv_cov)
# Check matrices are invereses
np.matmul(cov_matrix,inv_cov).astype(np.float16)
307/89:
md = MahalanobisDist(inv_cov, mean, df.values, verbose=False)
threshold = MD_threshold(md, extreme = False)
threshold
307/90:
plt.figure()
sns.distplot(np.square(md),bins = 20, kde= False)
plt.xlim([0.0,15])

plt.figure()
sns.distplot(md,
             bins = 20, 
             kde= True, 
            color = 'green');
plt.xlim([0.0,5])
plt.xlabel('Mahalanobis dist')
307/91:
df_out = df.copy()
df_out['mob_dist'] = md
df_out['thresh'] = threshold
df_out['anomaly'] = df_out.mob_dist > df_out.thresh
df_out[df_out.anomaly]
307/92:
boston_df_o = boston_df_o[(z < 3).all(axis=1)]
boston_df_o
307/93: boston_df.shape
307/94: boston_df_o.shape
307/95:
fig, ax = plt.subplots(figsize=(16,8))
ax.scatter(boston_df_o['INDUS'], boston_df_o['TAX'])
ax.set_xlabel('Proportion of non-retail business acres per town')
ax.set_ylabel('Full-value property-tax rate per $10,000')
307/96:
md = MahalanobisDist(inv_cov, mean, df.values, verbose=False)
threshold = MD_threshold(md, extreme = True)
threshold
307/97:
plt.figure()
sns.distplot(np.square(md),bins = 20, kde= False)
plt.xlim([0.0,15])

plt.figure()
sns.distplot(md,
             bins = 20, 
             kde= True, 
            color = 'green');
plt.xlim([0.0,5])
plt.xlabel('Mahalanobis dist')
307/98:
df_out = df.copy()
df_out['mob_dist'] = md
df_out['thresh'] = threshold
df_out['anomaly'] = df_out.mob_dist > df_out.thresh
df_out[df_out.anomaly]
307/99:
boston_df_o = boston_df_o[(z < 3).all(axis=1)]
boston_df_o
307/100: boston_df.shape
307/101: boston_df_o.shape
307/102:
fig, ax = plt.subplots(figsize=(16,8))
ax.scatter(boston_df_o['INDUS'], boston_df_o['TAX'])
ax.set_xlabel('Proportion of non-retail business acres per town')
ax.set_ylabel('Full-value property-tax rate per $10,000')
307/103:
df_out = df.copy()
df_out['mob_dist'] = md
df_out['thresh'] = threshold
df_out['anomaly'] = df_out.mob_dist > df_out.thresh
df_out[df_out.anomaly]
307/104:
pca = PCA(n_components=boston_df.shape[1], svd_solver= 'full')
df = pd.DataFrame(pca.fit_transform(boston_df),index=boston_df.index,columns=boston_df.columns)
df
307/105:
pca = PCA(n_components=boston_df.shape[1], svd_solver= 'full')
df = pd.DataFrame(pca.fit_transform(boston_df),index=boston_df.index,columns=[x+'_pca' for x in boston_df.columns])
df
307/106:
pca = PCA(n_components=boston_df.shape[1], svd_solver= 'full')
df = pd.DataFrame(pca.fit_transform(boston_df),index=boston_df.index,columns=[x.lower()+'_pca' for x in boston_df.columns])
df
307/107:
pca = PCA(n_components=boston_df.shape[1], svd_solver= 'full')
df = pd.DataFrame(pca.fit_transform(boston_df),index=boston_df.index,columns=[x.lower()+'_pca' for x in boston_df.columns])
df
307/108:
def cov_matrix(data, verbose=False):
    covariance_matrix = np.cov(data, rowvar=False)
    if is_pos_def(covariance_matrix):
        inv_covariance_matrix = np.linalg.inv(covariance_matrix)
        if is_pos_def(inv_covariance_matrix):
            return covariance_matrix, inv_covariance_matrix
        else:
            print("Error: Inverse of Covariance Matrix is not positive definite!")
    else:
        print("Error: Covariance Matrix is not positive definite!")

def MahalanobisDist(inv_cov_matrix, mean_distr, data, verbose=False):
    inv_covariance_matrix = inv_cov_matrix
    vars_mean = mean_distr
    diff = data - vars_mean
    md = []
    for i in range(len(diff)):
        md.append(np.sqrt(diff[i].dot(inv_covariance_matrix).dot(diff[i])))
    return md

def MD_detectOutliers(dist, extreme=False, verbose=False):
    k = 3. if extreme else 2.
    threshold = np.mean(dist) * k
    outliers = []
    for i in range(len(dist)):
        if dist[i] >= threshold:
            outliers.append(i)  # index of the outlier
    return np.array(outliers)

def MD_threshold(dist, extreme=False, verbose=False):
    k = 3. if extreme else 2.
    threshold = np.mean(dist) * k
    return threshold

# Check that matrix is positive definite
def is_pos_def(A):
    if np.allclose(A, A.T):
        try:
            np.linalg.cholesky(A)
            return True
        except np.linalg.LinAlgError:
            return False
    else:
        return False
307/109:
cov_matrix = np.cov(df.values,rowvar=False)
inv_cov = np.linalg.inv(cov_matrix)
mean = df.values.mean(axis=0)

# Check matrices are positive definite:https://en.wikipedia.org/wiki/Definiteness_of_a_matrix 
assert is_pos_def(cov_matrix) and is_pos_def(inv_cov)
# Check matrices are invereses
np.matmul(cov_matrix,inv_cov).astype(np.float16)
307/110:
md = MahalanobisDist(inv_cov, mean, df.values, verbose=False)
threshold = MD_threshold(md, extreme = True)
threshold
307/111:
plt.figure()
sns.distplot(np.square(md),bins = 20, kde= False)
plt.xlim([0.0,15])

plt.figure()
sns.distplot(md,
             bins = 20, 
             kde= True, 
            color = 'green');
plt.xlim([0.0,5])
plt.xlabel('Mahalanobis dist')
307/112:
df_out = df.copy()
df_out['mob_dist'] = md
df_out['thresh'] = threshold
df_out['anomaly'] = df_out.mob_dist > df_out.thresh
df_out[df_out.anomaly]
307/113:
boston_df_o = boston_df_o[(z < 3).all(axis=1)]
boston_df_o
307/114: boston_df.shape
307/115: boston_df_o.shape
307/116:
fig, ax = plt.subplots(figsize=(16,8))
ax.scatter(boston_df_o['INDUS'], boston_df_o['TAX'])
ax.set_xlabel('Proportion of non-retail business acres per town')
ax.set_ylabel('Full-value property-tax rate per $10,000')
307/117:
# df_out = df.copy()
# df_out['mob_dist'] = md
# df_out['thresh'] = threshold
# df_out['anomaly'] = df_out.mob_dist > df_out.thresh
# df_out[df_out.anomaly]
md>threshold
307/118:
# df_out = df.copy()
# df_out['mob_dist'] = md
# df_out['thresh'] = threshold
# df_out['anomaly'] = df_out.mob_dist > df_out.thresh
# df_out[df_out.anomaly]
df[md>threshold]
307/119:
# df_out = df.copy()
# df_out['mob_dist'] = md
# df_out['thresh'] = threshold
# df_out['anomaly'] = df_out.mob_dist > df_out.thresh
# df_out[df_out.anomaly]
df['anomaly'] = md>threshold
307/120:
# df_out = df.copy()
# df_out['mob_dist'] = md
# df_out['thresh'] = threshold
# df_out['anomaly'] = df_out.mob_dist > df_out.thresh
# df_out[df_out.anomaly]
df['anomaly'] = md>threshold
df
307/121:
# df_out = df.copy()
# df_out['mob_dist'] = md
# df_out['thresh'] = threshold
# df_out['anomaly'] = df_out.mob_dist > df_out.thresh
# df_out[df_out.anomaly]
df['anomaly'] = md>threshold
df[df.anomaly]
307/122:
df['anomaly'] = md>threshold
df[df.anomaly]
307/123:
# classify what data is an outlier  
df['anomaly'] = md>threshold
df[df.anomaly]
307/124: df.plot(figsize = (6,6))
307/125:
pca = PCA(n_components=boston_df.shape[1], svd_solver= 'full')
df = pd.DataFrame(pca.fit_transform(boston_df),index=boston_df.index,columns=[x.lower()+'_pca' for x in boston_df.columns])
df.plot()
307/126:
pca = PCA(n_components=boston_df.shape[1], svd_solver= 'full')
df = pd.DataFrame(pca.fit_transform(boston_df),index=boston_df.index,columns=[x.lower()+'_pca' for x in boston_df.columns])
df.plot(x=df.indus_pca,y=df.tax_pca)
307/127:
pca = PCA(n_components=boston_df.shape[1], svd_solver= 'full')
df = pd.DataFrame(pca.fit_transform(boston_df),index=boston_df.index,columns=[x.lower()+'_pca' for x in boston_df.columns])
plt.plot(x=df.indus_pca,y=df.tax_pca)
307/128:
pca = PCA(n_components=boston_df.shape[1], svd_solver= 'full')
df = pd.DataFrame(pca.fit_transform(boston_df),index=boston_df.index,columns=[x.lower()+'_pca' for x in boston_df.columns])
print(df.indus_pca)
plt.plot(x=df.indus_pca,y=df.tax_pca)
307/129:
pca = PCA(n_components=boston_df.shape[1], svd_solver= 'full')
df = pd.DataFrame(pca.fit_transform(boston_df),index=boston_df.index,columns=[x.lower()+'_pca' for x in boston_df.columns])
plt.plot(x=df.indus_pca.values,y=df.tax_pca.values)
307/130:
pca = PCA(n_components=boston_df.shape[1], svd_solver= 'full')
df = pd.DataFrame(pca.fit_transform(boston_df),index=boston_df.index,columns=[x.lower()+'_pca' for x in boston_df.columns])
plt.scatter(x=df.indus_pca.values,y=df.tax_pca.values)
307/131:
pca = PCA(n_components=boston_df.shape[1], svd_solver= 'full')
df = pd.DataFrame(pca.fit_transform(boston_df),index=boston_df.index,columns=[x.lower()+'_pca' for x in boston_df.columns])
plt.scatter(x=df.indus_pca,y=df.tax_pca)
307/132:
pca = PCA(n_components=boston_df.shape[1], svd_solver= 'full')
df = pd.DataFrame(pca.fit_transform(boston_df),index=boston_df.index,columns=[x.lower()+'_pca' for x in boston_df.columns])
plt.line(x=df.indus_pca,y=df.tax_pca)
307/133:
pca = PCA(n_components=boston_df.shape[1], svd_solver= 'full')
df = pd.DataFrame(pca.fit_transform(boston_df),index=boston_df.index,columns=[x.lower()+'_pca' for x in boston_df.columns])
plt.scatter(x=df.indus_pca,y=df.tax_pca)
307/134:
pca = PCA(n_components=boston_df.shape[1], svd_solver= 'full')
df = pd.DataFrame(pca.fit_transform(boston_df),index=boston_df.index,columns=[x.lower()+'_pca' for x in boston_df.columns])
plt.scatter(x=df.indus_pca,y=df.tax_pca)
plt.tite('PCA Graph')
307/135:
pca = PCA(n_components=boston_df.shape[1], svd_solver= 'full')
df = pd.DataFrame(pca.fit_transform(boston_df),index=boston_df.index,columns=[x.lower()+'_pca' for x in boston_df.columns])
plt.scatter(x=df.indus_pca,y=df.tax_pca)
plt.tite='PCA Graph'
307/136:
pca = PCA(n_components=boston_df.shape[1], svd_solver= 'full')
df = pd.DataFrame(pca.fit_transform(boston_df),index=boston_df.index,columns=[x.lower()+'_pca' for x in boston_df.columns])
plt.scatter(x=df.indus_pca,y=df.tax_pca)
plt.title('PCA Graph')
307/137:
boston_df = boston_df[['TAX','INDUS']]
boston_df.corr()
307/138:
pca = PCA(n_components=boston_df.shape[1], svd_solver= 'full')
df = pd.DataFrame(pca.fit_transform(boston_df),index=boston_df.index,columns=[x.lower()+'_pca' for x in boston_df.columns])
plt.scatter(x=df.indus_pca,y=df.tax_pca)
plt.title('PCA Graph')
df.corr()
307/139:
pca = PCA(n_components=boston_df.shape[1], svd_solver= 'full')
df = pd.DataFrame(pca.fit_transform(boston_df),index=boston_df.index,columns=[x.lower()+'_pca' for x in boston_df.columns])
plt.scatter(x=df.indus_pca,y=df.tax_pca)
plt.title('PCA Graph')
df.corr().astype(np.float16)
307/140:
pca = PCA(n_components=boston_df.shape[1], svd_solver= 'full')
df = pd.DataFrame(pca.fit_transform(boston_df),index=boston_df.index,columns=[x.lower()+'_pca' for x in boston_df.columns])
plt.scatter(x=df.indus_pca,y=df.tax_pca)
plt.title('PCA Graph')
df.corr().astype(np.float32)
307/141:
boston_df = boston_df[['TAX','INDUS']]
boston_df
307/142:
boston_df = boston_df[['TAX','INDUS']]
print(boston_df.corr())
boston_df
307/143:
boston_df = boston_df[['TAX','INDUS']]
boston_df
print(boston_df.corr())
307/144:
boston_df = boston_df[['TAX','INDUS']]
boston_df
boston_df.corr()
307/145:
boston_df = boston_df[['TAX','INDUS']]
boston_df
307/146: boston_df.corr()
307/147: sns.heatmap(boston_df.corr())
307/148: sns.heatmap(boston_df.corr(),cmap='hot')
307/149: boston_df.corr()
307/150: plt.plot(threshold)
307/151: plt.plot(df.indus,threshold)
307/152: plt.plot(df.indus_pca,threshold)
307/153: plt.plot(x=df.indus_pca,y=threshold)
307/154: plt.plot(x=df.indus_pca,[threshold]*len(df.indus_pca))
307/155: plt.plot([threshold]*len(df.indus_pca),df.indus_pca)
307/156:
# classify what data is an outlier  
boston_df['anomaly'] = df['anomaly'] = md>threshold
boston_df[df.anomaly]
307/157:
boston_df = boston_df[['INDUS','TAX']]
boston_df
307/158:
#Import the libraries
import os
import numpy as np
import pandas as pd
from sklearn import preprocessing
from sklearn.datasets import load_boston,load_iris
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

import seaborn as sns
sns.set(color_codes=True)
307/159:
#Load the data
boston = load_boston()

#Find features and target
x = boston.data
y = boston.target

#Find the dic keys
print(boston.keys())
307/160:
#find features name
columns = boston.feature_names
columns
307/161:
#Create dataframe
boston_df = pd.DataFrame(boston.data)
boston_df.columns = columns
print(boston_df.shape)
307/162:
plt.figure(figsize= (4,4), dpi=100)
sns.heatmap(boston_df.corr())
307/163:
boston_df = boston_df[['INDUS','TAX']]
boston_df
307/164: boston_df.corr()
307/165:
#Multivariate outlier analysis
fig, ax = plt.subplots(figsize=(11,8.5))
ax.scatter(boston_df['INDUS'], boston_df['TAX'])
ax.set_xlabel('Proportion of non-retail business acres per town')
ax.set_ylabel('Full-value property-tax rate per $10,000')
307/166:
pca = PCA(n_components=boston_df.shape[1], svd_solver= 'full')
df = pd.DataFrame(pca.fit_transform(boston_df),index=boston_df.index,columns=[x.lower()+'_pca' for x in boston_df.columns])
plt.scatter(x=df.indus_pca,y=df.tax_pca)
plt.title('PCA Graph')
307/167:
def cov_matrix(data, verbose=False):
    covariance_matrix = np.cov(data, rowvar=False)
    if is_pos_def(covariance_matrix):
        inv_covariance_matrix = np.linalg.inv(covariance_matrix)
        if is_pos_def(inv_covariance_matrix):
            return covariance_matrix, inv_covariance_matrix
        else:
            print("Error: Inverse of Covariance Matrix is not positive definite!")
    else:
        print("Error: Covariance Matrix is not positive definite!")

def MahalanobisDist(inv_cov_matrix, mean_distr, data, verbose=False):
    inv_covariance_matrix = inv_cov_matrix
    vars_mean = mean_distr
    diff = data - vars_mean
    md = []
    for i in range(len(diff)):
        md.append(np.sqrt(diff[i].dot(inv_covariance_matrix).dot(diff[i])))
    return md

def MD_detectOutliers(dist, extreme=False, verbose=False):
    k = 3. if extreme else 2.
    threshold = np.mean(dist) * k
    outliers = []
    for i in range(len(dist)):
        if dist[i] >= threshold:
            outliers.append(i)  # index of the outlier
    return np.array(outliers)

def MD_threshold(dist, extreme=False, verbose=False):
    k = 3. if extreme else 2.
    threshold = np.mean(dist) * k
    return threshold

# Check that matrix is positive definite
def is_pos_def(A):
    if np.allclose(A, A.T):
        try:
            np.linalg.cholesky(A)
            return True
        except np.linalg.LinAlgError:
            return False
    else:
        return False
307/168:
cov_matrix = np.cov(df.values,rowvar=False)
inv_cov = np.linalg.inv(cov_matrix)
mean = df.values.mean(axis=0)

# Check matrices are positive definite:https://en.wikipedia.org/wiki/Definiteness_of_a_matrix 
assert is_pos_def(cov_matrix) and is_pos_def(inv_cov)
# Check matrices are invereses
np.matmul(cov_matrix,inv_cov).astype(np.float16)
307/169:
md = MahalanobisDist(inv_cov, mean, df.values, verbose=False)
threshold = MD_threshold(md, extreme = True)
threshold
307/170:
plt.figure()
sns.distplot(np.square(md),bins = 20, kde= False)
plt.xlim([0.0,15])

plt.figure()
sns.distplot(md,
             bins = 20, 
             kde= True, 
            color = 'green');
plt.xlim([0.0,5])
plt.xlabel('Mahalanobis dist')
307/171:
# classify what data is an outlier  
boston_df['anomaly'] = df['anomaly'] = md>threshold
boston_df[df.anomaly]
307/172:
plt.plot(df.indus_pca df.tax)
plt.plot([threshold]*len(df.indus_pca),df.indus_pca,colour)
307/173:
boston_df_o = boston_df_o[(z < 3).all(axis=1)]
boston_df_o
307/174: boston_df.shape
307/175: boston_df_o.shape
307/176:
fig, ax = plt.subplots(figsize=(16,8))
ax.scatter(boston_df_o['INDUS'], boston_df_o['TAX'])
ax.set_xlabel('Proportion of non-retail business acres per town')
ax.set_ylabel('Full-value property-tax rate per $10,000')
307/177:
#Load the data
boston = load_boston()

#Find features and target
x = boston.data
y = boston.target

#Find the dic keys
print(boston.keys())
boston.target
307/178:
#Load the data
boston = load_boston()

#Find features and target
x = boston.data
y = boston.target

#Find the dic keys
print(boston.keys())
boston.target.shape
307/179:
boston_df = boston_df[['INDUS','TAX','Target']]
boston_df
307/180:
boston_df = boston_df[['INDUS','TAX','target']]
boston_df
307/181:
boston_df = boston_df[['INDUS','TAX','target']]
boston_df
307/182:
#Load the data
boston = load_boston()

#Find features and target
x = boston.data
y = boston.target

#Find the dic keys
print(boston.keys())
boston
307/183:
#Load the data
boston = load_boston()

#Find features and target
x = boston.data
y = boston.target

#Find the dic keys
print(boston.keys())
boston.data
307/184:
#Load the data
boston = load_boston()

#Find features and target
x = boston.data
y = boston.target

#Find the dic keys
print(boston.keys())
307/185:
#Create dataframe
boston_df = pd.DataFrame(boston.data)
boston_df.columns = columns
print(boston_df.shape)
boston_df.target = boston.target
307/186:
#Create dataframe
boston_df = pd.DataFrame(boston.data)
boston_df.columns = columns
print(boston_df.shape)
boston_df.target = boston.target
boston_df
307/187:
#Create dataframe
boston_df = pd.DataFrame(boston.data)
boston_df.columns = columns
print(boston_df.shape)
# boston_df.target = boston.target
boston_df
307/188:
#Create dataframe
boston_df = pd.DataFrame(boston.data)
boston_df.columns = columns
print(boston_df.shape)
boston_df.target = boston.target
boston_df
307/189:
#Create dataframe
boston_df = pd.DataFrame(boston.data)
boston_df.columns = columns
print(boston_df.shape)
boston_df['target'] = boston.target
boston_df
307/190:
plt.figure(figsize= (4,4), dpi=100)
sns.heatmap(boston_df.corr())
307/191:
boston_df = boston_df[['INDUS','TAX','target']]
boston_df
307/192: boston_df.corr()
307/193:
#Multivariate outlier analysis
fig, ax = plt.subplots(figsize=(11,8.5))
ax.scatter(boston_df['INDUS'], boston_df['target'])
ax.set_xlabel('Proportion of non-retail business acres per town')
ax.set_ylabel('Full-value property-tax rate per $10,000')
307/194:
#Multivariate outlier analysis
fig, ax = plt.subplots(figsize=(11,8.5))
ax.scatter(boston_df['INDUS'], boston_df['TAX'])
ax.set_xlabel('Proportion of non-retail business acres per town')
ax.set_ylabel('Full-value property-tax rate per $10,000')
307/195:
cov_matrix = np.cov(df.values,rowvar=False)
inv_cov = np.linalg.inv(cov_matrix)
mean = df.values.mean(axis=0)

# Check matrices are positive definite:https://en.wikipedia.org/wiki/Definiteness_of_a_matrix 
assert is_pos_def(cov_matrix) and is_pos_def(inv_cov)
# Check matrices are invereses
np.matmul(cov_matrix,inv_cov).astype(np.float16)
307/196:
boston_df = boston_df[['INDUS','TAX']]
boston_df
307/197:
boston_df = boston_df[['INDUS','TAX']]
boston_df
307/198: boston_df.corr()
307/199:
#Multivariate outlier analysis
fig, ax = plt.subplots(figsize=(11,8.5))
ax.scatter(boston_df['INDUS'], boston_df['TAX'])
ax.set_xlabel('Proportion of non-retail business acres per town')
ax.set_ylabel('Full-value property-tax rate per $10,000')
307/200:
pca = PCA(n_components=boston_df.shape[1], svd_solver= 'full')
df = pd.DataFrame(pca.fit_transform(boston_df),index=boston_df.index,columns=[x.lower()+'_pca' for x in boston_df.columns])
plt.scatter(x=df.indus_pca,y=df.tax_pca)
plt.title('PCA Graph')
307/201:
def cov_matrix(data, verbose=False):
    covariance_matrix = np.cov(data, rowvar=False)
    if is_pos_def(covariance_matrix):
        inv_covariance_matrix = np.linalg.inv(covariance_matrix)
        if is_pos_def(inv_covariance_matrix):
            return covariance_matrix, inv_covariance_matrix
        else:
            print("Error: Inverse of Covariance Matrix is not positive definite!")
    else:
        print("Error: Covariance Matrix is not positive definite!")

def MahalanobisDist(inv_cov_matrix, mean_distr, data, verbose=False):
    inv_covariance_matrix = inv_cov_matrix
    vars_mean = mean_distr
    diff = data - vars_mean
    md = []
    for i in range(len(diff)):
        md.append(np.sqrt(diff[i].dot(inv_covariance_matrix).dot(diff[i])))
    return md

def MD_detectOutliers(dist, extreme=False, verbose=False):
    k = 3. if extreme else 2.
    threshold = np.mean(dist) * k
    outliers = []
    for i in range(len(dist)):
        if dist[i] >= threshold:
            outliers.append(i)  # index of the outlier
    return np.array(outliers)

def MD_threshold(dist, extreme=False, verbose=False):
    k = 3. if extreme else 2.
    threshold = np.mean(dist) * k
    return threshold

# Check that matrix is positive definite
def is_pos_def(A):
    if np.allclose(A, A.T):
        try:
            np.linalg.cholesky(A)
            return True
        except np.linalg.LinAlgError:
            return False
    else:
        return False
307/202:
cov_matrix = np.cov(df.values,rowvar=False)
inv_cov = np.linalg.inv(cov_matrix)
mean = df.values.mean(axis=0)

# Check matrices are positive definite:https://en.wikipedia.org/wiki/Definiteness_of_a_matrix 
assert is_pos_def(cov_matrix) and is_pos_def(inv_cov)
# Check matrices are invereses
np.matmul(cov_matrix,inv_cov).astype(np.float16)
307/203:
md = MahalanobisDist(inv_cov, mean, df.values, verbose=False)
threshold = MD_threshold(md, extreme = True)
threshold
307/204:
md = MahalanobisDist(inv_cov, mean, df.values, verbose=False)
threshold = MD_threshold(md, extreme = False)
threshold
307/205:
md = MahalanobisDist(inv_cov, mean, df.values, verbose=False)
threshold = MD_threshold(md, extreme = False)
threshold
307/206:
plt.figure()
sns.distplot(np.square(md),bins = 20, kde= False)
plt.xlim([0.0,15])

plt.figure()
sns.distplot(md,
             bins = 20, 
             kde= True, 
            color = 'green');
plt.xlim([0.0,5])
plt.xlabel('Mahalanobis dist')
307/207:
plt.figure()
sns.distplot(np.square(md),bins = 20, kde= False)
plt.xlim([0.0,15])

plt.figure()
sns.distplot(md,
             bins = 30, 
             kde= True, 
            color = 'green');
plt.xlim([0.0,5])
plt.xlabel('Mahalanobis dist')
307/208:
plt.figure()
sns.distplot(np.square(md),bins = 20, kde= False)
plt.xlim([0.0,15])

plt.figure()
sns.distplot(md,
             bins = 10, 
             kde= True, 
            color = 'green');
plt.xlim([0.0,5])
plt.xlabel('Mahalanobis dist')
307/209:
plt.figure()
sns.distplot(np.square(md),bins = 30, kde= False)
plt.xlim([0.0,15])

plt.figure()
sns.distplot(md,
             bins = 20, 
             kde= True, 
            color = 'green');
plt.xlim([0.0,5])
plt.xlabel('Mahalanobis dist')
307/210:
plt.figure()
sns.distplot(np.square(md),bins = 40, kde= False)
plt.xlim([0.0,15])

plt.figure()
sns.distplot(md,
             bins = 20, 
             kde= True, 
            color = 'green');
plt.xlim([0.0,5])
plt.xlabel('Mahalanobis dist')
307/211:
plt.figure()
sns.distplot(np.square(md),bins = 40, kde= False)
# plt.xlim([0.0,15])

plt.figure()
sns.distplot(md,
             bins = 20, 
             kde= True, 
            color = 'green');
plt.xlim([0.0,5])
plt.xlabel('Mahalanobis dist')
307/212:
plt.figure()
sns.distplot(np.square(md) kde= False)
# plt.xlim([0.0,15])

plt.figure()
sns.distplot(md,
             bins = 20, 
             kde= True, 
            color = 'green');
plt.xlim([0.0,5])
plt.xlabel('Mahalanobis dist')
307/213:
plt.figure()
sns.distplot(np.square(md), kde= False)
# plt.xlim([0.0,15])

plt.figure()
sns.distplot(md,
             bins = 20, 
             kde= True, 
            color = 'green');
plt.xlim([0.0,5])
plt.xlabel('Mahalanobis dist')
307/214:
plt.figure()
sns.distplot(np.square(md), kde= False)
# plt.xlim([0.0,15])

plt.figure()
sns.distplot(md,
             bins = 20, 
             kde= True, 
            color = 'green');
# plt.xlim([0.0,5])
plt.xlabel('Mahalanobis dist')
307/215:
plt.figure()
sns.distplot(np.square(md), kde= False)
# plt.xlim([0.0,15])

plt.figure()
sns.distplot(md,
            #  bins = 20, 
             kde= True, 
            color = 'green');
# plt.xlim([0.0,5])
plt.xlabel('Mahalanobis dist')
307/216:
plt.figure()
sns.distplot(np.square(md), kde= False)
plt.xlim([0.0,15])

plt.figure()
sns.distplot(md,
            #  bins = 20, 
             kde= True, 
            color = 'green');
# plt.xlim([0.0,5])
plt.xlabel('Mahalanobis dist')
307/217:
plt.figure()
sns.distplot(np.square(md), kde= False)
plt.xlim([0.0,15])

plt.figure()
sns.distplot(md,
             bins = 20, 
             kde= True, 
            color = 'green');
# plt.xlim([0.0,5])
plt.xlabel('Mahalanobis dist')
307/218:
plt.figure()
sns.distplot(np.square(md), bines = 20,kde= False)
plt.xlim([0.0,15])

plt.figure()
sns.distplot(md,
             bins = 20, 
             kde= True, 
            color = 'green');
# plt.xlim([0.0,5])
plt.xlabel('Mahalanobis dist')
307/219:
plt.figure()
sns.distplot(np.square(md), bins = 20, kde= False)
plt.xlim([0.0,15])

plt.figure()
sns.distplot(md,
             bins = 20, 
             kde= True, 
            color = 'green');
# plt.xlim([0.0,5])
plt.xlabel('Mahalanobis dist')
307/220:
plt.figure()
sns.distplot(np.square(md), bins = 40, kde= False)
plt.xlim([0.0,15])

plt.figure()
sns.distplot(md,
             bins = 20, 
             kde= True, 
            color = 'green');
# plt.xlim([0.0,5])
plt.xlabel('Mahalanobis dist')
307/221:
plt.figure()
sns.distplot(np.square(md), bins = 40, kde= False)
plt.xlim([0.0,15])

plt.figure()
sns.distplot(md,
             bins = 40, 
             kde= True, 
            color = 'green');
# plt.xlim([0.0,5])
plt.xlabel('Mahalanobis dist')
307/222:
# classify what data is an outlier  
 md>threshold
boston_df[df.anomaly]
307/223:
# classify what data is an outlier  
md>threshold
boston_df[df.anomaly]
307/224:
# classify what data is an outlier  
# boston_df['anomaly'] = df['anomaly'] = md>threshold
# boston_df[df.anomaly]
boston_df[md>threshold]
307/225:
# classify what data is an outlier  
# boston_df['anomaly'] = df['anomaly'] = md>threshold
# boston_df[df.anomaly]
boston_df[md>threshold]
md
307/226:
# classify what data is an outlier  
# boston_df['anomaly'] = df['anomaly'] = md>threshold
# boston_df[df.anomaly]
boston_df[md>threshold]
307/227:
md = MahalanobisDist(inv_cov, mean, df.values, verbose=False)
threshold = MD_threshold(md, extreme = False)
print(threshold)
md
307/228:
md = MahalanobisDist(inv_cov, mean, df.values, verbose=False)
threshold = MD_threshold(md, extreme = False)
print(threshold)
md.summary()
307/229:
md = MahalanobisDist(inv_cov, mean, df.values, verbose=False)
threshold = MD_threshold(md, extreme = False)
print(threshold)
md.shape
307/230:
md = MahalanobisDist(inv_cov, mean, df.values, verbose=False)
threshold = MD_threshold(md, extreme = False)
print(threshold)
md
307/231:
md = MahalanobisDist(inv_cov, mean, df.values, verbose=False)
threshold = MD_threshold(md, extreme = False)
print(threshold)
md
307/232:
plt.figure()
sns.distplot(np.square(md), bins = 40, kde= False)
plt.xlim([0.0,15])

plt.figure()
sns.distplot(md,
             bins = 40, 
             kde= True, 
            color = 'green');
# plt.xlim([0.0,5])
plt.xlabel('Mahalanobis dist')
307/233:
# classify what data is an outlier  
# boston_df['anomaly'] = df['anomaly'] = md>threshold
# boston_df[df.anomaly]
boston_df[md>threshold]
307/234:
# classify what data is an outlier  
# boston_df['anomaly'] = df['anomaly'] = md>threshold
# boston_df[df.anomaly]
boston_df[md>threshold]
boston_df.loc(120:130)
307/235:
# classify what data is an outlier  
# boston_df['anomaly'] = df['anomaly'] = md>threshold
# boston_df[df.anomaly]
boston_df[md>threshold]
boston_df.loc[120:130]
307/236:
# classify what data is an outlier  
# boston_df['anomaly'] = df['anomaly'] = md>threshold
# boston_df[df.anomaly]
boston_df[md>threshold]
boston.loc[120:130]
307/237:
# classify what data is an outlier  
# boston_df['anomaly'] = df['anomaly'] = md>threshold
# boston_df[df.anomaly]
boston_df[md>threshold]
boston.iloc[120:130]
307/238:
# classify what data is an outlier  
# boston_df['anomaly'] = df['anomaly'] = md>threshold
# boston_df[df.anomaly]
boston_df[md>threshold]
boston.loc[120:130]
307/239:
# classify what data is an outlier  
# boston_df['anomaly'] = df['anomaly'] = md>threshold
# boston_df[df.anomaly]
boston_df[md>threshold]
boston.data.loc[120:130]
307/240:
# classify what data is an outlier  
# boston_df['anomaly'] = df['anomaly'] = md>threshold
# boston_df[df.anomaly]
boston_df[md>threshold]
pd.DataFrame(boston.data).loc[120:130]
307/241:
# classify what data is an outlier  
# boston_df['anomaly'] = df['anomaly'] = md>threshold
# boston_df[df.anomaly]
boston_df[md>threshold]
307/242:
plt.plot(df.indus_pca, df.tax)
plt.plot([threshold]*len(df.indus_pca),df.indus_pca,colour)
307/243:
classify what data is an outlier  
boston_df['anomaly'] = df['anomaly'] = md>threshold
boston_df[df.anomaly]
307/244:
classify what data is an outlier  
boston_df['anomaly'] = df['anomaly'] = md>threshold
boston_df[boston_df.anomaly]
307/245:
classify what data is an outlier  
boston_df['anomaly'] = df['anomaly'] = md>threshold
boston_df[boston_df.anomaly]
307/246:
# classify what data is an outlier  
boston_df['anomaly'] = df['anomaly'] = md>threshold
boston_df[boston_df.anomaly]
307/247:
plt.plot(boston_df.INDUS[not boston_df.anomaly], boston_df.TAX)
plt.plot(boston_df.INDUS[boston_df.anomaly], boston_df.TAX)
plt.plot([threshold]*len(df.indus_pca),df.indus_pca,colour)
307/248:
plt.scatter(boston_df.INDUS[not boston_df.anomaly], boston_df.TAX)
plt.scatter(boston_df.INDUS[boston_df.anomaly], boston_df.TAX)
plt.plot([threshold]*len(df.indus_pca),df.indus_pca,colour)
307/249:
plt.scatter(boston_df.indus[not boston_df.anomaly], boston_df.tax)
plt.scatter(boston_df.INDUS[boston_df.anomaly], boston_df.TAX)
plt.plot([threshold]*len(df.indus_pca),df.indus_pca,colour)
307/250:
plt.scatter(boston_df['INDUS'][not boston_df.anomaly], boston_df['TAX'])
plt.scatter(boston_df.INDUS[boston_df.anomaly], boston_df.TAX)
plt.plot([threshold]*len(df.indus_pca),df.indus_pca,colour)
307/251:
plt.scatter(boston_df[['INDUS']][not boston_df.anomaly], boston_df[['TAX']])
plt.scatter(boston_df.INDUS[boston_df.anomaly], boston_df.TAX)
plt.plot([threshold]*len(df.indus_pca),df.indus_pca,colour)
307/252:
# classify what data is an outlier  
boston_df['anomaly'] = df['anomaly'] = md>threshold
boston_df[boston_df.anomaly]
boston_df.INDUS
307/253:
# classify what data is an outlier  
boston_df['anomaly'] = df['anomaly'] = md>threshold
boston_df[boston_df.anomaly]
not boston_df.anomaly
307/254:
# classify what data is an outlier  
boston_df['anomaly'] = df['anomaly'] = md>threshold
boston_df[boston_df.anomaly]
boston_df.anomaly
307/255:
# classify what data is an outlier  
boston_df['anomaly'] = df['anomaly'] = md>threshold
boston_df[boston_df.anomaly]
boston_df.INDUS[not boston_df.anomaly]
307/256:
# classify what data is an outlier  
boston_df['anomaly'] = df['anomaly'] = md>threshold
boston_df[boston_df.anomaly]
boston_df[not boston_df.anomaly]
307/257:
# classify what data is an outlier  
boston_df['anomaly'] = df['anomaly'] = md>threshold
boston_df[boston_df.anomaly]
boston_df[boston_df.anomaly]
307/258:
# classify what data is an outlier  
boston_df['anomaly'] = df['anomaly'] = md>threshold
boston_df[boston_df.anomaly]
boston_df[boston_df.anomaly==False]
307/259:
# classify what data is an outlier  
boston_df['anomaly'] = df['anomaly'] = md>threshold
boston_df[boston_df.anomaly]
boston_df.INDUS[boston_df.anomaly==False]
307/260:
# classify what data is an outlier  
boston_df['anomaly'] = df['anomaly'] = md>threshold
boston_df[boston_df.anomaly]
boston_df.indus[boston_df.anomaly==False]
307/261:
# classify what data is an outlier  
boston_df['anomaly'] = df['anomaly'] = md>threshold
boston_df[boston_df.anomaly]
boston_df.INDUS[boston_df.anomaly==False]
307/262:
plt.scatter(boston_df.INDUS[boston_df.anomaly==False], boston_df.TAX)
plt.scatter(boston_df.INDUS[boston_df.anomaly], boston_df.TAX)
plt.plot([threshold]*len(df.indus_pca),df.indus_pca,colour)
307/263:
plt.scatter(boston_df.INDUS[boston_df.anomaly==False], boston_df.TAX[boston_df.anomaly==False])
plt.scatter(boston_df.INDUS[boston_df.anomaly], boston_df.TAX[boston_df.anomaly])
plt.plot([threshold]*len(df.indus_pca),df.indus_pca,colour)
307/264:
plt.scatter(boston_df.INDUS[boston_df.anomaly==False], boston_df.TAX[boston_df.anomaly==False])
plt.scatter(boston_df.INDUS[boston_df.anomaly], boston_df.TAX[boston_df.anomaly])
# plt.plot([threshold]*len(df.indus_pca),df.indus_pca,colour)
307/265:
md = MahalanobisDist(inv_cov, mean, df.values, verbose=False)
threshold = MD_threshold(md, extreme = False)
print("Threshold: "+threshold)
plt.bar(range(md),md)
307/266:
md = MahalanobisDist(inv_cov, mean, df.values, verbose=False)
threshold = MD_threshold(md, extreme = False)
print("Threshold: "+threshold)
plt.bar(range(md),md)
307/267:
plt.scatter(boston_df.INDUS[md<=threshold], boston_df.TAX[md<=threshold])
plt.scatter(boston_df.INDUS[md>threshold], boston_df.TAX[md>threshold])
# plt.plot([threshold]*len(df.indus_pca),df.indus_pca,colour)
307/268:
plt.scatter(boston_df.INDUS[md<=threshold], boston_df.TAX[md<=threshold])
plt.scatter(boston_df.INDUS[md>threshold], boston_df.TAX[md>threshold])
plt.show()

plt.scatter(df.INDUS[md<=threshold], df.TAX[md<=threshold])
plt.scatter(df.INDUS[md>threshold], df.TAX[md>threshold])
# plt.plot([threshold]*len(df.indus_pca),df.indus_pca,colour)
307/269:
plt.scatter(boston_df.INDUS[md<=threshold], boston_df.TAX[md<=threshold])
plt.scatter(boston_df.INDUS[md>threshold], boston_df.TAX[md>threshold])
plt.show()

plt.scatter(df.indus_pca[md<=threshold], df.tax_pca[md<=threshold])
plt.scatter(df.indus_pca[md>threshold], df.tax_pca[md>threshold])
# plt.plot([threshold]*len(df.indus_pca),df.indus_pca,colour)
307/270:
#Import the libraries
import os
import numpy as np
import pandas as pd
from sklearn import preprocessing
from sklearn.datasets import load_boston,load_iris
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

import seaborn as sns
sns.set(color_codes=True)
307/271:
#Load the data
boston = load_boston()

#Find features and target
x = boston.data
y = boston.target

#Find the dic keys
print(boston.keys())
307/272:
#find features name
columns = boston.feature_names
columns
307/273:
#Create dataframe
boston_df = pd.DataFrame(boston.data)
boston_df.columns = columns
print(boston_df.shape)
boston_df['target'] = boston.target
boston_df
307/274:
plt.figure(figsize= (4,4), dpi=100)
sns.heatmap(boston_df.corr())
307/275:
boston_df = boston_df[['INDUS','TAX']]
boston_df
307/276: boston_df.corr()
307/277:
#Multivariate outlier analysis
fig, ax = plt.subplots(figsize=(11,8.5))
ax.scatter(boston_df['INDUS'], boston_df['TAX'])
ax.set_xlabel('Proportion of non-retail business acres per town')
ax.set_ylabel('Full-value property-tax rate per $10,000')
307/278:
pca = PCA(n_components=boston_df.shape[1], svd_solver= 'full')
df = pd.DataFrame(pca.fit_transform(boston_df),index=boston_df.index,columns=[x.lower()+'_pca' for x in boston_df.columns])
plt.scatter(x=df.indus_pca,y=df.tax_pca)
plt.title('PCA Graph')
307/279:
def cov_matrix(data, verbose=False):
    covariance_matrix = np.cov(data, rowvar=False)
    if is_pos_def(covariance_matrix):
        inv_covariance_matrix = np.linalg.inv(covariance_matrix)
        if is_pos_def(inv_covariance_matrix):
            return covariance_matrix, inv_covariance_matrix
        else:
            print("Error: Inverse of Covariance Matrix is not positive definite!")
    else:
        print("Error: Covariance Matrix is not positive definite!")

def MahalanobisDist(inv_cov_matrix, mean_distr, data, verbose=False):
    inv_covariance_matrix = inv_cov_matrix
    vars_mean = mean_distr
    diff = data - vars_mean
    md = []
    for i in range(len(diff)):
        md.append(np.sqrt(diff[i].dot(inv_covariance_matrix).dot(diff[i])))
    return md

def MD_detectOutliers(dist, extreme=False, verbose=False):
    k = 3. if extreme else 2.
    threshold = np.mean(dist) * k
    outliers = []
    for i in range(len(dist)):
        if dist[i] >= threshold:
            outliers.append(i)  # index of the outlier
    return np.array(outliers)

def MD_threshold(dist, extreme=False, verbose=False):
    k = 3. if extreme else 2.
    threshold = np.mean(dist) * k
    return threshold

# Check that matrix is positive definite
def is_pos_def(A):
    if np.allclose(A, A.T):
        try:
            np.linalg.cholesky(A)
            return True
        except np.linalg.LinAlgError:
            return False
    else:
        return False
307/280:
cov_matrix = np.cov(df.values,rowvar=False)
inv_cov = np.linalg.inv(cov_matrix)
mean = df.values.mean(axis=0)

# Check matrices are positive definite:https://en.wikipedia.org/wiki/Definiteness_of_a_matrix 
assert is_pos_def(cov_matrix) and is_pos_def(inv_cov)
# Check matrices are invereses
np.matmul(cov_matrix,inv_cov).astype(np.float16)
307/281:
md = MahalanobisDist(inv_cov, mean, df.values, verbose=False)
threshold = MD_threshold(md, extreme = False)
print("Threshold: "+threshold)
plt.bar(range(md),md)
307/282:
plt.figure()
sns.distplot(np.square(md), bins = 40, kde= False)
plt.xlim([0.0,15])

plt.figure()
sns.distplot(md,
             bins = 40, 
             kde= True, 
            color = 'green');
# plt.xlim([0.0,5])
plt.xlabel('Mahalanobis dist')
307/283:
# classify what data is an outlier  
boston_df['anomaly'] = df['anomaly'] = md>threshold
boston_df[boston_df.anomaly]
boston_df
307/284:
plt.scatter(boston_df.INDUS[md<=threshold], boston_df.TAX[md<=threshold])
plt.scatter(boston_df.INDUS[md>threshold], boston_df.TAX[md>threshold])
plt.show()

plt.scatter(df.indus_pca[md<=threshold], df.tax_pca[md<=threshold])
plt.scatter(df.indus_pca[md>threshold], df.tax_pca[md>threshold])
# plt.plot([threshold]*len(df.indus_pca),df.indus_pca,colour)
307/285:
md = MahalanobisDist(inv_cov, mean, df.values, verbose=False)
threshold = MD_threshold(md, extreme = False)
# print("Threshold: "+threshold)
plt.bar(range(md),md)
threshold
307/286:
df
md = MahalanobisDist(inv_cov, mean, df.values, verbose=False)
threshold = MD_threshold(md, extreme = False)
# print("Threshold: "+threshold)
plt.bar(range(md),md)
threshold
307/287:
print(df)
md = MahalanobisDist(inv_cov, mean, df.values, verbose=False)
threshold = MD_threshold(md, extreme = False)
# print("Threshold: "+threshold)
plt.bar(range(md),md)
threshold
307/288:
# classify what data is an outlier  
# boston_df['anomaly'] = df['anomaly'] = md>threshold
# boston_df[boston_df.anomaly]
boston_df[md>threshold]
308/1:
#Import the libraries
import os
import numpy as np
import pandas as pd
from sklearn import preprocessing
from sklearn.datasets import load_boston,load_iris
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

import seaborn as sns
sns.set(color_codes=True)
308/2:
#Load the data
boston = load_boston()

#Find features and target
x = boston.data
y = boston.target

#Find the dic keys
print(boston.keys())
308/3:
#find features name
columns = boston.feature_names
columns
308/4:
#Create dataframe
boston_df = pd.DataFrame(boston.data)
boston_df.columns = columns
print(boston_df.shape)
boston_df['target'] = boston.target
boston_df
308/5:
plt.figure(figsize= (4,4), dpi=100)
sns.heatmap(boston_df.corr())
308/6:
boston_df = boston_df[['INDUS','TAX']]
boston_df
308/7: boston_df.corr()
308/8:
#Multivariate outlier analysis
fig, ax = plt.subplots(figsize=(11,8.5))
ax.scatter(boston_df['INDUS'], boston_df['TAX'])
ax.set_xlabel('Proportion of non-retail business acres per town')
ax.set_ylabel('Full-value property-tax rate per $10,000')
308/9:
pca = PCA(n_components=boston_df.shape[1], svd_solver= 'full')
df = pd.DataFrame(pca.fit_transform(boston_df),index=boston_df.index,columns=[x.lower()+'_pca' for x in boston_df.columns])
plt.scatter(x=df.indus_pca,y=df.tax_pca)
plt.title('PCA Graph')
308/10:
def cov_matrix(data, verbose=False):
    covariance_matrix = np.cov(data, rowvar=False)
    if is_pos_def(covariance_matrix):
        inv_covariance_matrix = np.linalg.inv(covariance_matrix)
        if is_pos_def(inv_covariance_matrix):
            return covariance_matrix, inv_covariance_matrix
        else:
            print("Error: Inverse of Covariance Matrix is not positive definite!")
    else:
        print("Error: Covariance Matrix is not positive definite!")

def MahalanobisDist(inv_cov_matrix, mean_distr, data, verbose=False):
    inv_covariance_matrix = inv_cov_matrix
    vars_mean = mean_distr
    diff = data - vars_mean
    md = []
    for i in range(len(diff)):
        md.append(np.sqrt(diff[i].dot(inv_covariance_matrix).dot(diff[i])))
    return md

def MD_detectOutliers(dist, extreme=False, verbose=False):
    k = 3. if extreme else 2.
    threshold = np.mean(dist) * k
    outliers = []
    for i in range(len(dist)):
        if dist[i] >= threshold:
            outliers.append(i)  # index of the outlier
    return np.array(outliers)

def MD_threshold(dist, extreme=False, verbose=False):
    k = 3. if extreme else 2.
    threshold = np.mean(dist) * k
    return threshold

# Check that matrix is positive definite
def is_pos_def(A):
    if np.allclose(A, A.T):
        try:
            np.linalg.cholesky(A)
            return True
        except np.linalg.LinAlgError:
            return False
    else:
        return False
308/11:
cov_matrix = np.cov(df.values,rowvar=False)
inv_cov = np.linalg.inv(cov_matrix)
mean = df.values.mean(axis=0)

# Check matrices are positive definite:https://en.wikipedia.org/wiki/Definiteness_of_a_matrix 
assert is_pos_def(cov_matrix) and is_pos_def(inv_cov)
# Check matrices are invereses
np.matmul(cov_matrix,inv_cov).astype(np.float16)
308/12:
md = MahalanobisDist(inv_cov, mean, df.values, verbose=False)
threshold = MD_threshold(md, extreme = False)
# print("Threshold: "+threshold)
plt.bar(range(md),md)
threshold
308/13:
plt.figure()
sns.distplot(np.square(md), bins = 40, kde= False)
plt.xlim([0.0,15])

plt.figure()
sns.distplot(md,
             bins = 40, 
             kde= True, 
            color = 'green');
# plt.xlim([0.0,5])
plt.xlabel('Mahalanobis dist')
308/14:
# classify what data is an outlier  
# boston_df['anomaly'] = df['anomaly'] = md>threshold
# boston_df[boston_df.anomaly]
boston_df[md>threshold]
308/15:
plt.scatter(boston_df.INDUS[md<=threshold], boston_df.TAX[md<=threshold])
plt.scatter(boston_df.INDUS[md>threshold], boston_df.TAX[md>threshold])
plt.show()

plt.scatter(df.indus_pca[md<=threshold], df.tax_pca[md<=threshold])
plt.scatter(df.indus_pca[md>threshold], df.tax_pca[md>threshold])
# plt.plot([threshold]*len(df.indus_pca),df.indus_pca,colour)
308/16:
md = MahalanobisDist(inv_cov, mean, df.values, verbose=False)
threshold = MD_threshold(md, extreme = False)
# print("Threshold: "+threshold)
plt.hist(range(md),md)
threshold
308/17:
md = MahalanobisDist(inv_cov, mean, df.values, verbose=False)
threshold = MD_threshold(md, extreme = False)
# print("Threshold: "+threshold)
plt.hist(len(md),md)
threshold
308/18:
md = MahalanobisDist(inv_cov, mean, df.values, verbose=False)
threshold = MD_threshold(md, extreme = False)
# print("Threshold: "+threshold)
plt.hist(len(md),md,bins=20)
threshold
308/19:
md = MahalanobisDist(inv_cov, mean, df.values, verbose=False)
threshold = MD_threshold(md, extreme = False)
# print("Threshold: "+threshold)
plt.hist(md)
threshold
308/20:
md = MahalanobisDist(inv_cov, mean, df.values, verbose=False)
threshold = MD_threshold(md, extreme = False)
# print("Threshold: "+threshold)
plt.hist(md,bins=40)
threshold
308/21:
md = MahalanobisDist(inv_cov, mean, df.values, verbose=False)
threshold = MD_threshold(md, extreme = False)
# print("Threshold: "+threshold)
plt.hist(md,bins=40)
threshold = 2
308/22:
md = MahalanobisDist(inv_cov, mean, df.values, verbose=False)
threshold = MD_threshold(md, extreme = False)
# print("Threshold: "+threshold)
plt.hist(md,bins=40)
threshold = 2
308/23:
plt.figure()
sns.distplot(np.square(md), bins = 40, kde= False)
plt.xlim([0.0,15])

plt.figure()
sns.distplot(md,
             bins = 40, 
             kde= True, 
            color = 'green');
# plt.xlim([0.0,5])
plt.xlabel('Mahalanobis dist')
308/24:
# classify what data is an outlier  
# boston_df['anomaly'] = df['anomaly'] = md>threshold
# boston_df[boston_df.anomaly]
boston_df[md>threshold]
308/25:
plt.scatter(boston_df.INDUS[md<=threshold], boston_df.TAX[md<=threshold])
plt.scatter(boston_df.INDUS[md>threshold], boston_df.TAX[md>threshold])
plt.show()

plt.scatter(df.indus_pca[md<=threshold], df.tax_pca[md<=threshold])
plt.scatter(df.indus_pca[md>threshold], df.tax_pca[md>threshold])
# plt.plot([threshold]*len(df.indus_pca),df.indus_pca,colour)
308/26:
# classify what data is an outlier  
# boston_df['anomaly'] = df['anomaly'] = md>threshold
# boston_df[boston_df.anomaly]
boston_df[md>threshold]
308/27:
def cov_matrix(data, verbose=False):
    covariance_matrix = np.cov(data, rowvar=False)
    if is_pos_def(covariance_matrix):
        inv_covariance_matrix = np.linalg.inv(covariance_matrix)
        if is_pos_def(inv_covariance_matrix):
            return covariance_matrix, inv_covariance_matrix
        else:
            print("Error: Inverse of Covariance Matrix is not positive definite!")
    else:
        print("Error: Covariance Matrix is not positive definite!")

def MahalanobisDist(inv_cov_matrix, mean_distr, data, verbose=False):
    inv_covariance_matrix = inv_cov_matrix
    vars_mean = mean_distr
    diff = data - vars_mean
    md = []
    for i in range(len(diff)):
        md.append(np.sqrt(diff[i].dot(inv_covariance_matrix).dot(diff[i])))
    return np.array(md)

def MD_detectOutliers(dist, extreme=False, verbose=False):
    k = 3. if extreme else 2.
    threshold = np.mean(dist) * k
    outliers = []
    for i in range(len(dist)):
        if dist[i] >= threshold:
            outliers.append(i)  # index of the outlier
    return np.array(outliers)

def MD_threshold(dist, extreme=False, verbose=False):
    k = 3. if extreme else 2.
    threshold = np.mean(dist) * k
    return threshold

# Check that matrix is positive definite
def is_pos_def(A):
    if np.allclose(A, A.T):
        try:
            np.linalg.cholesky(A)
            return True
        except np.linalg.LinAlgError:
            return False
    else:
        return False
308/28:
cov_matrix = np.cov(df.values,rowvar=False)
inv_cov = np.linalg.inv(cov_matrix)
mean = df.values.mean(axis=0)

# Check matrices are positive definite:https://en.wikipedia.org/wiki/Definiteness_of_a_matrix 
assert is_pos_def(cov_matrix) and is_pos_def(inv_cov)
# Check matrices are invereses
np.matmul(cov_matrix,inv_cov).astype(np.float16)
308/29:
md = MahalanobisDist(inv_cov, mean, df.values, verbose=False)
threshold = MD_threshold(md, extreme = False)
# print("Threshold: "+threshold)
plt.hist(md,bins=40)
threshold = 2
308/30:
plt.figure()
sns.distplot(np.square(md), bins = 40, kde= False)
plt.xlim([0.0,15])

plt.figure()
sns.distplot(md,
             bins = 40, 
             kde= True, 
            color = 'green');
# plt.xlim([0.0,5])
plt.xlabel('Mahalanobis dist')
308/31:
# classify what data is an outlier  
# boston_df['anomaly'] = df['anomaly'] = md>threshold
# boston_df[boston_df.anomaly]
boston_df[md>threshold]
308/32:
plt.scatter(boston_df.INDUS[md<=threshold], boston_df.TAX[md<=threshold])
plt.scatter(boston_df.INDUS[md>threshold], boston_df.TAX[md>threshold])
plt.show()

plt.scatter(df.indus_pca[md<=threshold], df.tax_pca[md<=threshold])
plt.scatter(df.indus_pca[md>threshold], df.tax_pca[md>threshold])
# plt.plot([threshold]*len(df.indus_pca),df.indus_pca,colour)
308/33:
# classify what data is an outlier  
# boston_df['anomaly'] = df['anomaly'] = md>threshold
# boston_df[boston_df.anomaly]
boston_df[md>threshold].shape
308/34:
# classify what data is an outlier  
# boston_df['anomaly'] = df['anomaly'] = md>threshold
# boston_df[boston_df.anomaly]
len(boston_df[md>threshold])
308/35:
md = MahalanobisDist(inv_cov, mean, df.values, verbose=False)
threshold = MD_threshold(md, extreme = False)
# print("Threshold: "+threshold)
plt.hist(md,bins=40)
threshold = 1.5
308/36:
plt.figure()
sns.distplot(np.square(md), bins = 40, kde= False)
plt.xlim([0.0,15])

plt.figure()
sns.distplot(md,
             bins = 40, 
             kde= True, 
            color = 'green');
# plt.xlim([0.0,5])
plt.xlabel('Mahalanobis dist')
308/37:
# classify what data is an outlier  
# boston_df['anomaly'] = df['anomaly'] = md>threshold
# boston_df[boston_df.anomaly]
len(boston_df[md>threshold])
308/38:
plt.scatter(boston_df.INDUS[md<=threshold], boston_df.TAX[md<=threshold])
plt.scatter(boston_df.INDUS[md>threshold], boston_df.TAX[md>threshold])
plt.show()

plt.scatter(df.indus_pca[md<=threshold], df.tax_pca[md<=threshold])
plt.scatter(df.indus_pca[md>threshold], df.tax_pca[md>threshold])
# plt.plot([threshold]*len(df.indus_pca),df.indus_pca,colour)
308/39:
md = MahalanobisDist(inv_cov, mean, df.values, verbose=False)
threshold = MD_threshold(md, extreme = False)
# print("Threshold: "+threshold)
plt.hist(md,bins=40)
threshold
308/40:
md = MahalanobisDist(inv_cov, mean, df.values, verbose=False)
threshold = MD_threshold(md, extreme = False)
# print("Threshold: "+threshold)
plt.hist(md,bins=40)
int(threshold)
308/41:
md = MahalanobisDist(inv_cov, mean, df.values, verbose=False)
threshold = MD_threshold(md, extreme = False)
# print("Threshold: "+threshold)
plt.hist(md,bins=40)
float(threshold)
308/42:
md = MahalanobisDist(inv_cov, mean, df.values, verbose=False)
threshold = MD_threshold(md, extreme = False)
print("Threshold: "+float(threshold))
plt.hist(md,bins=40)
308/43:
md = MahalanobisDist(inv_cov, mean, df.values, verbose=False)
threshold = MD_threshold(md, extreme = False)
print("Threshold: "+str(threshold))
plt.hist(md,bins=40)
308/44:
md = MahalanobisDist(inv_cov, mean, df.values, verbose=False)
threshold = MD_threshold(md, extreme = False)
print("Threshold: "+str(threshold))
plt.hist(md,bins=40)
308/45:
md = MahalanobisDist(inv_cov, mean, df.values, verbose=False)
threshold = MD_threshold(md, extreme = False)
print("Threshold: "+str(threshold))
# plt.hist(md,bins=40)
308/46:
md = MahalanobisDist(inv_cov, mean, df.values, verbose=False)
threshold = MD_threshold(md, extreme = False)
print("Threshold: "+str(threshold))
plt.hist(md,bins=40)
308/47:
md = MahalanobisDist(inv_cov, mean, df.values, verbose=False)
threshold = MD_threshold(md, extreme = False)
print("Threshold: "+str(threshold))
plt.hist(md.tolist(),bins=40)
308/48:
md = MahalanobisDist(inv_cov, mean, df.values, verbose=False)
threshold = MD_threshold(md, extreme = False)
print("Threshold: "+str(threshold))
# plt.hist(md.tolist(),bins=40)
308/49:
md = MahalanobisDist(inv_cov, mean, df.values, verbose=False)
threshold = MD_threshold(md, extreme = False)
print("Threshold: "+str(threshold))
plt.hist(md.tolist(),bins=40)
308/50:
md = MahalanobisDist(inv_cov, mean, df.values, verbose=False)
threshold = MD_threshold(md, extreme = False)
print("Threshold: "+str(threshold))
plt.hist(list(md),bins=40)
308/51:
md = MahalanobisDist(inv_cov, mean, df.values, verbose=False)
threshold = MD_threshold(md, extreme = False)
# print("Threshold: "+str(threshold))
plt.hist(list(md),bins=40)
308/52:
md = MahalanobisDist(inv_cov, mean, df.values, verbose=False)
threshold = MD_threshold(md, extreme = False)
# print("Threshold: "+str(threshold))
# plt.hist(list(md),bins=40)
308/53:
md = MahalanobisDist(inv_cov, mean, df.values, verbose=False)
threshold = MD_threshold(md, extreme = False)
print("Threshold: "+str(threshold))
plt.hist(list(md),bins=40)
plt.show()
308/54:
md = MahalanobisDist(inv_cov, mean, df.values, verbose=False)
threshold = MD_threshold(md, extreme = True)
print("Threshold: "+str(threshold))
plt.hist(list(md),bins=40)
plt.show()
308/55:
md = MahalanobisDist(inv_cov, mean, df.values, verbose=False)
threshold = MD_threshold(md, extreme = False)
print("Threshold: "+str(threshold))
plt.hist(list(md),bins=40)
plt.show()
308/56:
md = MahalanobisDist(inv_cov, mean, df.values, verbose=False)
threshold = MD_threshold(md, extreme = False)
print("Threshold: "+str(threshold))
plt.hist(list(md),bins=40)
plt.show()
MD_detectOutliers(md)
308/57:
md = MahalanobisDist(inv_cov, mean, df.values, verbose=False)
threshold = MD_threshold(md, extreme = False)
print("Threshold: "+str(threshold))
plt.hist(list(md),bins=40)
plt.show()
308/58:
#Import the libraries
import os
import numpy as np
import pandas as pd
from sklearn import preprocessing
from sklearn.datasets import load_boston,load_iris
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

import seaborn as sns
sns.set(color_codes=True)
308/59:
#Load the data
boston = load_boston()

#Find features and target
x = boston.data
y = boston.target

#Find the dic keys
print(boston.keys())
308/60:
#find features name
columns = boston.feature_names
columns
308/61:
#Create dataframe
boston_df = pd.DataFrame(boston.data)
boston_df.columns = columns
print(boston_df.shape)
boston_df['target'] = boston.target
boston_df
308/62:
plt.figure(figsize= (4,4), dpi=100)
sns.heatmap(boston_df.corr())
308/63:
boston_df = boston_df[['INDUS','DIS']]
boston_df
308/64:
boston_df.corr()
boston_df = pd.DataFrame
308/65:
#Multivariate outlier analysis
fig, ax = plt.subplots(figsize=(11,8.5))
ax.scatter(boston_df['INDUS'], boston_df['DIS'])
ax.set_xlabel('Proportion of non-retail business acres per town')
ax.set_ylabel('Full-value property-tax rate per $10,000')
308/66:
boston_df.corr()
boston_df = pd.DataFrame
308/67: boston_df.corr()
308/68:
boston_df = boston_df[['INDUS','DIS']]
boston_df
308/69:
#Import the libraries
import os
import numpy as np
import pandas as pd
from sklearn import preprocessing
from sklearn.datasets import load_boston,load_iris
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

import seaborn as sns
sns.set(color_codes=True)
308/70:
#Load the data
boston = load_boston()

#Find features and target
x = boston.data
y = boston.target

#Find the dic keys
print(boston.keys())
308/71:
#find features name
columns = boston.feature_names
columns
308/72:
#Create dataframe
boston_df = pd.DataFrame(boston.data)
boston_df.columns = columns
print(boston_df.shape)
boston_df['target'] = boston.target
boston_df
308/73:
plt.figure(figsize= (4,4), dpi=100)
sns.heatmap(boston_df.corr())
308/74:
boston_df = boston_df[['INDUS','DIS']]
boston_df
308/75:
boston_df = boston_df[['INDUS','DIS']]
boston_df
308/76: boston_df.corr()
308/77: boston_df.corr()
308/78:
#Multivariate outlier analysis
fig, ax = plt.subplots(figsize=(11,8.5))
ax.scatter(boston_df['INDUS'], boston_df['DIS'])
ax.set_xlabel('Proportion of non-retail business acres per town')
ax.set_ylabel('Full-value property-tax rate per $10,000')
308/79:
pca = PCA(n_components=boston_df.shape[1], svd_solver= 'full')
df = pd.DataFrame(pca.fit_transform(boston_df),index=boston_df.index,columns=[x.lower()+'_pca' for x in boston_df.columns])
plt.scatter(x=df.indus_pca,y=df.tax_pca)
plt.title('PCA Graph')
308/80:
pca = PCA(n_components=boston_df.shape[1], svd_solver= 'full')
df = pd.DataFrame(pca.fit_transform(boston_df),index=boston_df.index,columns=[x.lower()+'_pca' for x in boston_df.columns])
plt.scatter(x=df.indus_pca,y=df.dis_pca)
plt.title('PCA Graph')
308/81:
cov_matrix = np.cov(df.values,rowvar=False)
inv_cov = np.linalg.inv(cov_matrix)
mean = df.values.mean(axis=0)

# Check matrices are positive definite:https://en.wikipedia.org/wiki/Definiteness_of_a_matrix 
assert is_pos_def(cov_matrix) and is_pos_def(inv_cov)
# Check matrices are invereses
np.matmul(cov_matrix,inv_cov).astype(np.float16)
308/82:
md = MahalanobisDist(inv_cov, mean, df.values, verbose=False)
threshold = MD_threshold(md, extreme = False)
print("Threshold: "+str(threshold))
plt.hist(list(md),bins=40)
plt.show()
308/83:
plt.figure()
sns.distplot(np.square(md), bins = 40, kde= False)
plt.xlim([0.0,15])

plt.figure()
sns.distplot(md,
             bins = 40, 
             kde= True, 
            color = 'green');
# plt.xlim([0.0,5])
plt.xlabel('Mahalanobis dist')
308/84:
# classify what data is an outlier  
# boston_df['anomaly'] = df['anomaly'] = md>threshold
# boston_df[boston_df.anomaly]
len(boston_df[md>threshold])
308/85:
plt.scatter(boston_df.INDUS[md<=threshold], boston_df.TAX[md<=threshold])
plt.scatter(boston_df.INDUS[md>threshold], boston_df.TAX[md>threshold])
plt.show()

plt.scatter(df.indus_pca[md<=threshold], df.tax_pca[md<=threshold])
plt.scatter(df.indus_pca[md>threshold], df.tax_pca[md>threshold])
# plt.plot([threshold]*len(df.indus_pca),df.indus_pca,colour)
308/86:
plt.scatter(boston_df.INDUS[md<=threshold], boston_df.DIS[md<=threshold])
plt.scatter(boston_df.INDUS[md>threshold], boston_df.DIS[md>threshold])
plt.show()

plt.scatter(df.indus_pca[md<=threshold], df.dis_pca[md<=threshold])
plt.scatter(df.indus_pca[md>threshold], df.dis_pca[md>threshold])
# plt.plot([threshold]*len(df.indus_pca),df.indus_pca,colour)
308/87:
boston_df = boston_df[['DIS','INDUS']]
boston_df
308/88: boston_df.corr()
308/89:
#Multivariate outlier analysis
fig, ax = plt.subplots(figsize=(11,8.5))
ax.scatter(boston_df['INDUS'], boston_df['DIS'])
ax.set_xlabel('Proportion of non-retail business acres per town')
ax.set_ylabel('Full-value property-tax rate per $10,000')
308/90:
pca = PCA(n_components=boston_df.shape[1], svd_solver= 'full')
df = pd.DataFrame(pca.fit_transform(boston_df),index=boston_df.index,columns=[x.lower()+'_pca' for x in boston_df.columns])
plt.scatter(x=df.indus_pca,y=df.dis_pca)
plt.title('PCA Graph')
308/91:
def cov_matrix(data, verbose=False):
    covariance_matrix = np.cov(data, rowvar=False)
    if is_pos_def(covariance_matrix):
        inv_covariance_matrix = np.linalg.inv(covariance_matrix)
        if is_pos_def(inv_covariance_matrix):
            return covariance_matrix, inv_covariance_matrix
        else:
            print("Error: Inverse of Covariance Matrix is not positive definite!")
    else:
        print("Error: Covariance Matrix is not positive definite!")

def MahalanobisDist(inv_cov_matrix, mean_distr, data, verbose=False):
    inv_covariance_matrix = inv_cov_matrix
    vars_mean = mean_distr
    diff = data - vars_mean
    md = []
    for i in range(len(diff)):
        md.append(np.sqrt(diff[i].dot(inv_covariance_matrix).dot(diff[i])))
    return np.array(md)

def MD_detectOutliers(dist, extreme=False, verbose=False):
    k = 3. if extreme else 2.
    threshold = np.mean(dist) * k
    outliers = []
    for i in range(len(dist)):
        if dist[i] >= threshold:
            outliers.append(i)  # index of the outlier
    return np.array(outliers)

def MD_threshold(dist, extreme=False, verbose=False):
    k = 3. if extreme else 2.
    threshold = np.mean(dist) * k
    return threshold

# Check that matrix is positive definite
def is_pos_def(A):
    if np.allclose(A, A.T):
        try:
            np.linalg.cholesky(A)
            return True
        except np.linalg.LinAlgError:
            return False
    else:
        return False
308/92:
cov_matrix = np.cov(df.values,rowvar=False)
inv_cov = np.linalg.inv(cov_matrix)
mean = df.values.mean(axis=0)

# Check matrices are positive definite:https://en.wikipedia.org/wiki/Definiteness_of_a_matrix 
assert is_pos_def(cov_matrix) and is_pos_def(inv_cov)
# Check matrices are invereses
np.matmul(cov_matrix,inv_cov).astype(np.float16)
308/93:
md = MahalanobisDist(inv_cov, mean, df.values, verbose=False)
threshold = MD_threshold(md, extreme = False)
print("Threshold: "+str(threshold))
plt.hist(list(md),bins=40)
plt.show()
308/94:
plt.figure()
sns.distplot(np.square(md), bins = 40, kde= False)
plt.xlim([0.0,15])

plt.figure()
sns.distplot(md,
             bins = 40, 
             kde= True, 
            color = 'green');
# plt.xlim([0.0,5])
plt.xlabel('Mahalanobis dist')
308/95:
# classify what data is an outlier  
# boston_df['anomaly'] = df['anomaly'] = md>threshold
# boston_df[boston_df.anomaly]
len(boston_df[md>threshold])
308/96:
plt.scatter(boston_df.INDUS[md<=threshold], boston_df.DIS[md<=threshold])
plt.scatter(boston_df.INDUS[md>threshold], boston_df.DIS[md>threshold])
plt.show()

plt.scatter(df.indus_pca[md<=threshold], df.dis_pca[md<=threshold])
plt.scatter(df.indus_pca[md>threshold], df.dis_pca[md>threshold])
# plt.plot([threshold]*len(df.indus_pca),df.indus_pca,colour)
308/97:
x='INDUS'
y='DIS'
boston_df = boston_df[[x,y]]
boston_df
308/98:
pca = PCA(n_components=boston_df.shape[1], svd_solver= 'full')
df = pd.DataFrame(pca.fit_transform(boston_df),index=boston_df.index,columns=boston_df.columns) #[i.lower()+'_pca' for i in boston_df.columns]
plt.scatter(x=df.x,y=df.y)
plt.title('PCA Graph')
308/99:
#Import the libraries
import os
import numpy as np
import pandas as pd
from sklearn import preprocessing
from sklearn.datasets import load_boston,load_iris
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

import seaborn as sns
sns.set(color_codes=True)
308/100:
#Load the data
boston = load_boston()

#Find features and target
x = boston.data
y = boston.target

#Find the dic keys
print(boston.keys())
308/101:
#find features name
columns = boston.feature_names
columns
308/102:
#Create dataframe
boston_df = pd.DataFrame(boston.data)
boston_df.columns = columns
print(boston_df.shape)
boston_df['target'] = boston.target
boston_df
308/103:
plt.figure(figsize= (4,4), dpi=100)
sns.heatmap(boston_df.corr())
308/104:
x='INDUS'
y='DIS'
boston_df = boston_df[[x,y]]
boston_df
308/105: boston_df.corr()
308/106:
#Multivariate outlier analysis
fig, ax = plt.subplots(figsize=(11,8.5))
ax.scatter(boston_df[x], boston_df[y])
ax.set_xlabel('Proportion of non-retail business acres per town')
ax.set_ylabel('Full-value property-tax rate per $10,000')
308/107:
pca = PCA(n_components=boston_df.shape[1], svd_solver= 'full')
df = pd.DataFrame(pca.fit_transform(boston_df),index=boston_df.index,columns=boston_df.columns) #[i.lower()+'_pca' for i in boston_df.columns]
plt.scatter(x=df.x,y=df.y)
plt.title('PCA Graph')
308/108:
pca = PCA(n_components=boston_df.shape[1], svd_solver= 'full')
df = pd.DataFrame(pca.fit_transform(boston_df),index=boston_df.index,columns=boston_df.columns) #[i.lower()+'_pca' for i in boston_df.columns]
plt.scatter(x=df[x],y=df[y])
plt.title('PCA Graph')
308/109:
plt.scatter(boston_df[x][md<=threshold], boston_df[y][md<=threshold])
plt.scatter(boston_df[x][md>threshold], boston_df[y][md>threshold])
plt.show()

plt.scatter(df[x][md<=threshold], df[y][md<=threshold])
plt.scatter(df[x][md>threshold], df[y][md>threshold])
# plt.plot([threshold]*len(df[x]_pca),df.indus_pca,colour)
308/110:
#Import the libraries
import os
import numpy as np
import pandas as pd
from sklearn import preprocessing
from sklearn.datasets import load_boston,load_iris
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

import seaborn as sns
sns.set(color_codes=True)
308/111:
#Load the data
boston = load_boston()

#Find features and target
x = boston.data
y = boston.target

#Find the dic keys
print(boston.keys())
308/112:
#find features name
columns = boston.feature_names
columns
308/113:
#Create dataframe
boston_df = pd.DataFrame(boston.data)
boston_df.columns = columns
print(boston_df.shape)
boston_df['target'] = boston.target
boston_df
308/114:
plt.figure(figsize= (4,4), dpi=100)
sns.heatmap(boston_df.corr())
308/115:
x='INDUS'
y='DIS'
boston_df = boston_df[[x,y]]
boston_df
308/116: boston_df.corr()
308/117:
#Multivariate outlier analysis
fig, ax = plt.subplots(figsize=(11,8.5))
ax.scatter(boston_df[x], boston_df[y])
ax.set_xlabel('Proportion of non-retail business acres per town')
ax.set_ylabel('Full-value property-tax rate per $10,000')
308/118:
pca = PCA(n_components=boston_df.shape[1], svd_solver= 'full')
df = pd.DataFrame(pca.fit_transform(boston_df),index=boston_df.index,columns=boston_df.columns) #[i.lower()+'_pca' for i in boston_df.columns]
plt.scatter(x=df[x],y=df[y])
plt.title('PCA Graph')
308/119:
def cov_matrix(data, verbose=False):
    covariance_matrix = np.cov(data, rowvar=False)
    if is_pos_def(covariance_matrix):
        inv_covariance_matrix = np.linalg.inv(covariance_matrix)
        if is_pos_def(inv_covariance_matrix):
            return covariance_matrix, inv_covariance_matrix
        else:
            print("Error: Inverse of Covariance Matrix is not positive definite!")
    else:
        print("Error: Covariance Matrix is not positive definite!")

def MahalanobisDist(inv_cov_matrix, mean_distr, data, verbose=False):
    inv_covariance_matrix = inv_cov_matrix
    vars_mean = mean_distr
    diff = data - vars_mean
    md = []
    for i in range(len(diff)):
        md.append(np.sqrt(diff[i].dot(inv_covariance_matrix).dot(diff[i])))
    return np.array(md)

def MD_detectOutliers(dist, extreme=False, verbose=False):
    k = 3. if extreme else 2.
    threshold = np.mean(dist) * k
    outliers = []
    for i in range(len(dist)):
        if dist[i] >= threshold:
            outliers.append(i)  # index of the outlier
    return np.array(outliers)

def MD_threshold(dist, extreme=False, verbose=False):
    k = 3. if extreme else 2.
    threshold = np.mean(dist) * k
    return threshold

# Check that matrix is positive definite
def is_pos_def(A):
    if np.allclose(A, A.T):
        try:
            np.linalg.cholesky(A)
            return True
        except np.linalg.LinAlgError:
            return False
    else:
        return False
308/120:
cov_matrix = np.cov(df.values,rowvar=False)
inv_cov = np.linalg.inv(cov_matrix)
mean = df.values.mean(axis=0)

# Check matrices are positive definite:https://en.wikipedia.org/wiki/Definiteness_of_a_matrix 
assert is_pos_def(cov_matrix) and is_pos_def(inv_cov)
# Check matrices are invereses
np.matmul(cov_matrix,inv_cov).astype(np.float16)
308/121:
md = MahalanobisDist(inv_cov, mean, df.values, verbose=False)
threshold = MD_threshold(md, extreme = False)
print("Threshold: "+str(threshold))
plt.hist(list(md),bins=40)
plt.show()
308/122:
plt.figure()
sns.distplot(np.square(md), bins = 40, kde= False)
plt.xlim([0.0,15])

plt.figure()
sns.distplot(md,
             bins = 40, 
             kde= True, 
            color = 'green');
# plt.xlim([0.0,5])
plt.xlabel('Mahalanobis dist')
308/123:
# classify what data is an outlier  
# boston_df['anomaly'] = df['anomaly'] = md>threshold
# boston_df[boston_df.anomaly]
len(boston_df[md>threshold])
308/124:
x='LSTAT'
y='target'
boston_df = boston_df[[x,y]]
boston_df
308/125: boston_df.corr()
308/126:
#Multivariate outlier analysis
fig, ax = plt.subplots(figsize=(11,8.5))
ax.scatter(boston_df[x], boston_df[y])
ax.set_xlabel('Proportion of non-retail business acres per town')
ax.set_ylabel('Full-value property-tax rate per $10,000')
308/127:
pca = PCA(n_components=boston_df.shape[1], svd_solver= 'full')
df = pd.DataFrame(pca.fit_transform(boston_df),index=boston_df.index,columns=boston_df.columns) #[i.lower()+'_pca' for i in boston_df.columns]
plt.scatter(x=df[x],y=df[y])
plt.title('PCA Graph')
308/128:
def cov_matrix(data, verbose=False):
    covariance_matrix = np.cov(data, rowvar=False)
    if is_pos_def(covariance_matrix):
        inv_covariance_matrix = np.linalg.inv(covariance_matrix)
        if is_pos_def(inv_covariance_matrix):
            return covariance_matrix, inv_covariance_matrix
        else:
            print("Error: Inverse of Covariance Matrix is not positive definite!")
    else:
        print("Error: Covariance Matrix is not positive definite!")

def MahalanobisDist(inv_cov_matrix, mean_distr, data, verbose=False):
    inv_covariance_matrix = inv_cov_matrix
    vars_mean = mean_distr
    diff = data - vars_mean
    md = []
    for i in range(len(diff)):
        md.append(np.sqrt(diff[i].dot(inv_covariance_matrix).dot(diff[i])))
    return np.array(md)

def MD_detectOutliers(dist, extreme=False, verbose=False):
    k = 3. if extreme else 2.
    threshold = np.mean(dist) * k
    outliers = []
    for i in range(len(dist)):
        if dist[i] >= threshold:
            outliers.append(i)  # index of the outlier
    return np.array(outliers)

def MD_threshold(dist, extreme=False, verbose=False):
    k = 3. if extreme else 2.
    threshold = np.mean(dist) * k
    return threshold

# Check that matrix is positive definite
def is_pos_def(A):
    if np.allclose(A, A.T):
        try:
            np.linalg.cholesky(A)
            return True
        except np.linalg.LinAlgError:
            return False
    else:
        return False
308/129:
cov_matrix = np.cov(df.values,rowvar=False)
inv_cov = np.linalg.inv(cov_matrix)
mean = df.values.mean(axis=0)

# Check matrices are positive definite:https://en.wikipedia.org/wiki/Definiteness_of_a_matrix 
assert is_pos_def(cov_matrix) and is_pos_def(inv_cov)
# Check matrices are invereses
np.matmul(cov_matrix,inv_cov).astype(np.float16)
308/130:
md = MahalanobisDist(inv_cov, mean, df.values, verbose=False)
threshold = MD_threshold(md, extreme = False)
print("Threshold: "+str(threshold))
plt.hist(list(md),bins=40)
plt.show()
308/131:
plt.figure()
sns.distplot(np.square(md), bins = 40, kde= False)
plt.xlim([0.0,15])

plt.figure()
sns.distplot(md,
             bins = 40, 
             kde= True, 
            color = 'green');
# plt.xlim([0.0,5])
plt.xlabel('Mahalanobis dist')
308/132:
# classify what data is an outlier  
# boston_df['anomaly'] = df['anomaly'] = md>threshold
# boston_df[boston_df.anomaly]
len(boston_df[md>threshold])
308/133:
plt.scatter(boston_df[x][md<=threshold], boston_df[y][md<=threshold])
plt.scatter(boston_df[x][md>threshold], boston_df[y][md>threshold])
plt.show()

plt.scatter(df[x][md<=threshold], df[y][md<=threshold])
plt.scatter(df[x][md>threshold], df[y][md>threshold])
# plt.plot([threshold]*len(df[x]_pca),df.indus_pca,colour)
308/134:
x='LSTAT'
y='target'
boston_df = boston_df[x,y]
boston_df
308/135:
x='LSTAT'
y='target'
boston_df = boston_df[[x,y]]
boston_df
308/136:
#Create dataframe
boston_df = pd.DataFrame(boston.data)
boston_df.columns = columns
print(boston_df.shape)
boston_df['target'] = boston.target
boston_df
308/137:
#Import the libraries
import os
import numpy as np
import pandas as pd
from sklearn import preprocessing
from sklearn.datasets import load_boston,load_iris
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

import seaborn as sns
sns.set(color_codes=True)
308/138:
#Load the data
boston = load_boston()

#Find features and target
x = boston.data
y = boston.target

#Find the dic keys
print(boston.keys())
308/139:
#find features name
columns = boston.feature_names
columns
308/140:
#Create dataframe
boston_df = pd.DataFrame(boston.data)
boston_df.columns = columns
print(boston_df.shape)
boston_df['target'] = boston.target
boston_df
308/141:
plt.figure(figsize= (4,4), dpi=100)
sns.heatmap(boston_df.corr())
308/142:
x='LSTAT'
y='target'
boston_df = boston_df[[x,y]]
boston_df
308/143: boston_df.corr()
308/144:
#Multivariate outlier analysis
fig, ax = plt.subplots(figsize=(11,8.5))
ax.scatter(boston_df[x], boston_df[y])
ax.set_xlabel('Proportion of non-retail business acres per town')
ax.set_ylabel('Full-value property-tax rate per $10,000')
308/145:
pca = PCA(n_components=boston_df.shape[1], svd_solver= 'full')
df = pd.DataFrame(pca.fit_transform(boston_df),index=boston_df.index,columns=boston_df.columns) #[i.lower()+'_pca' for i in boston_df.columns]
plt.scatter(x=df[x],y=df[y])
plt.title('PCA Graph')
308/146:
def cov_matrix(data, verbose=False):
    covariance_matrix = np.cov(data, rowvar=False)
    if is_pos_def(covariance_matrix):
        inv_covariance_matrix = np.linalg.inv(covariance_matrix)
        if is_pos_def(inv_covariance_matrix):
            return covariance_matrix, inv_covariance_matrix
        else:
            print("Error: Inverse of Covariance Matrix is not positive definite!")
    else:
        print("Error: Covariance Matrix is not positive definite!")

def MahalanobisDist(inv_cov_matrix, mean_distr, data, verbose=False):
    inv_covariance_matrix = inv_cov_matrix
    vars_mean = mean_distr
    diff = data - vars_mean
    md = []
    for i in range(len(diff)):
        md.append(np.sqrt(diff[i].dot(inv_covariance_matrix).dot(diff[i])))
    return np.array(md)

def MD_detectOutliers(dist, extreme=False, verbose=False):
    k = 3. if extreme else 2.
    threshold = np.mean(dist) * k
    outliers = []
    for i in range(len(dist)):
        if dist[i] >= threshold:
            outliers.append(i)  # index of the outlier
    return np.array(outliers)

def MD_threshold(dist, extreme=False, verbose=False):
    k = 3. if extreme else 2.
    threshold = np.mean(dist) * k
    return threshold

# Check that matrix is positive definite
def is_pos_def(A):
    if np.allclose(A, A.T):
        try:
            np.linalg.cholesky(A)
            return True
        except np.linalg.LinAlgError:
            return False
    else:
        return False
308/147:
cov_matrix = np.cov(df.values,rowvar=False)
inv_cov = np.linalg.inv(cov_matrix)
mean = df.values.mean(axis=0)

# Check matrices are positive definite:https://en.wikipedia.org/wiki/Definiteness_of_a_matrix 
assert is_pos_def(cov_matrix) and is_pos_def(inv_cov)
# Check matrices are invereses
np.matmul(cov_matrix,inv_cov).astype(np.float16)
308/148:
md = MahalanobisDist(inv_cov, mean, df.values, verbose=False)
threshold = MD_threshold(md, extreme = False)
print("Threshold: "+str(threshold))
plt.hist(list(md),bins=40)
plt.show()
308/149:
plt.figure()
sns.distplot(np.square(md), bins = 40, kde= False)
plt.xlim([0.0,15])

plt.figure()
sns.distplot(md,
             bins = 40, 
             kde= True, 
            color = 'green');
# plt.xlim([0.0,5])
plt.xlabel('Mahalanobis dist')
308/150:
# classify what data is an outlier  
# boston_df['anomaly'] = df['anomaly'] = md>threshold
# boston_df[boston_df.anomaly]
len(boston_df[md>threshold])
308/151:
plt.scatter(boston_df[x][md<=threshold], boston_df[y][md<=threshold])
plt.scatter(boston_df[x][md>threshold], boston_df[y][md>threshold])
plt.show()

plt.scatter(df[x][md<=threshold], df[y][md<=threshold])
plt.scatter(df[x][md>threshold], df[y][md>threshold])
# plt.plot([threshold]*len(df[x]_pca),df.indus_pca,colour)
308/152:
md = MahalanobisDist(inv_cov, mean, df.values, verbose=False)
threshold = MD_threshold(md, extreme = True)
print("Threshold: "+str(threshold))
plt.hist(list(md),bins=40)
plt.show()
308/153:
plt.figure()
sns.distplot(np.square(md), bins = 40, kde= False)
plt.xlim([0.0,15])

plt.figure()
sns.distplot(md,
             bins = 40, 
             kde= True, 
            color = 'green');
# plt.xlim([0.0,5])
plt.xlabel('Mahalanobis dist')
308/154:
# classify what data is an outlier  
# boston_df['anomaly'] = df['anomaly'] = md>threshold
# boston_df[boston_df.anomaly]
len(boston_df[md>threshold])
308/155:
plt.scatter(boston_df[x][md<=threshold], boston_df[y][md<=threshold])
plt.scatter(boston_df[x][md>threshold], boston_df[y][md>threshold])
plt.show()

plt.scatter(df[x][md<=threshold], df[y][md<=threshold])
plt.scatter(df[x][md>threshold], df[y][md>threshold])
# plt.plot([threshold]*len(df[x]_pca),df.indus_pca,colour)
308/156:
def corrdot(*args, **kwargs):
    corr_r = args[0].corr(args[1], 'pearson')
    corr_text = f"{corr_r:2.2f}".replace("0.", ".")
    ax = plt.gca()
    ax.set_axis_off()
    marker_size = abs(corr_r) * 10000
    ax.scatter([.5], [.5], marker_size, [corr_r], alpha=0.6, cmap="coolwarm",
               vmin=-1, vmax=1, transform=ax.transAxes)
    font_size = abs(corr_r) * 40 + 5
    ax.annotate(corr_text, [.5, .5,],  xycoords="axes fraction",
                ha='center', va='center', fontsize=font_size)

g = sns.PairGrid(boston, aspect=1.4, diag_sharey=False)
g.map_lower(sns.regplot, lowess=True, ci=False, line_kws={'color': 'black'})
g.map_diag(sns.distplot, kde_kws={'color': 'black'})
g.map_upper(corrdot)
308/157:
def corrdot(*args, **kwargs):
    corr_r = args[0].corr(args[1], 'pearson')
    corr_text = f"{corr_r:2.2f}".replace("0.", ".")
    ax = plt.gca()
    ax.set_axis_off()
    marker_size = abs(corr_r) * 10000
    ax.scatter([.5], [.5], marker_size, [corr_r], alpha=0.6, cmap="coolwarm",
               vmin=-1, vmax=1, transform=ax.transAxes)
    font_size = abs(corr_r) * 40 + 5
    ax.annotate(corr_text, [.5, .5,],  xycoords="axes fraction",
                ha='center', va='center', fontsize=font_size)

g = sns.PairGrid(boston_df, aspect=1.4, diag_sharey=False)
g.map_lower(sns.regplot, lowess=True, ci=False, line_kws={'color': 'black'})
g.map_diag(sns.distplot, kde_kws={'color': 'black'})
g.map_upper(corrdot)
308/158:
#Multivariate outlier analysis
fig, ax = plt.subplots(figsize=(11,8.5))
ax.scatter(boston_df[x], boston_df[y])
ax.set_xlabel(x)
ax.set_ylabel(y)
308/159:
#Load the data
boston = load_boston()

#Find features and target
x = boston.data
y = boston.target

#Find the dic keys
print(boston.keys())
308/160:
#find features name
columns = boston_data.feature_names
columns
308/161:
#Create dataframe
boston_data = pd.DataFrame(boston.data)
boston_data.columns = columns
print(boston_data.shape)
boston_data['target'] = boston.target
boston_data
308/162:
plt.figure(figsize= (4,4), dpi=100)
sns.heatmap(boston_data.corr())
308/163:
def corrdot(*args, **kwargs):
    corr_r = args[0].corr(args[1], 'pearson')
    corr_text = f"{corr_r:2.2f}".replace("0.", ".")
    ax = plt.gca()
    ax.set_axis_off()
    marker_size = abs(corr_r) * 10000
    ax.scatter([.5], [.5], marker_size, [corr_r], alpha=0.6, cmap="coolwarm",
               vmin=-1, vmax=1, transform=ax.transAxes)
    font_size = abs(corr_r) * 40 + 5
    ax.annotate(corr_text, [.5, .5,],  xycoords="axes fraction",
                ha='center', va='center', fontsize=font_size)

g = sns.PairGrid(boston_data, aspect=1.4, diag_sharey=False)
g.map_lower(sns.regplot, lowess=True, ci=False, line_kws={'color': 'black'})
g.map_diag(sns.distplot, kde_kws={'color': 'black'})
g.map_upper(corrdot)
309/1:
#Import the libraries
import os
import numpy as np
import pandas as pd
from sklearn import preprocessing
from sklearn.datasets import load_boston,load_iris
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

import seaborn as sns
sns.set(color_codes=True)
309/2:
#Load the data
boston = load_boston()

#Find features and target
x = boston.data
y = boston.target

#Find the dic keys
print(boston.keys())
309/3:
#find features name
columns = boston.feature_names
columns
309/4:
#Create dataframe
boston_data = pd.DataFrame(boston.data)
boston_data.columns = columns
print(boston_data.shape)
boston_data['target'] = boston.target
boston_data
309/5:
plt.figure(figsize= (4,4), dpi=100)
sns.heatmap(boston_data.corr())
309/6:
def corrdot(*args, **kwargs):
    corr_r = args[0].corr(args[1], 'pearson')
    corr_text = f"{corr_r:2.2f}".replace("0.", ".")
    ax = plt.gca()
    ax.set_axis_off()
    marker_size = abs(corr_r) * 10000
    ax.scatter([.5], [.5], marker_size, [corr_r], alpha=0.6, cmap="coolwarm",
               vmin=-1, vmax=1, transform=ax.transAxes)
    font_size = abs(corr_r) * 40 + 5
    ax.annotate(corr_text, [.5, .5,],  xycoords="axes fraction",
                ha='center', va='center', fontsize=font_size)

g = sns.PairGrid(boston_data, aspect=1.4, diag_sharey=False)
g.map_lower(sns.regplot, lowess=True, ci=False, line_kws={'color': 'black'})
g.map_diag(sns.distplot, kde_kws={'color': 'black'})
g.map_upper(corrdot)
309/7:
x='LSTAT'
y='target'
boston_df = boston_df[[x,y]]
boston_df
309/8:
def corrdot(*args, **kwargs):
    corr_r = args[0].corr(args[1], 'pearson')
    corr_text = f"{corr_r:2.2f}".replace("0.", ".")
    ax = plt.gca()
    ax.set_axis_off()
    marker_size = abs(corr_r) * 10000
    ax.scatter([.5], [.5], marker_size, [corr_r], alpha=0.6, cmap="coolwarm",
               vmin=-1, vmax=1, transform=ax.transAxes)
    font_size = abs(corr_r) * 40 + 5
    ax.annotate(corr_text, [.5, .5,],  xycoords="axes fraction",
                ha='center', va='center', fontsize=font_size)

g = sns.PairGrid(boston_df, aspect=1.4, diag_sharey=False)
g.map_lower(sns.regplot, lowess=True, ci=False, line_kws={'color': 'black'})
g.map_diag(sns.distplot, kde_kws={'color': 'black'})
g.map_upper(corrdot)
309/9: boston_df.corr()
309/10:
#Multivariate outlier analysis
fig, ax = plt.subplots(figsize=(11,8.5))
ax.scatter(boston_df[x], boston_df[y])
ax.set_xlabel(x)
ax.set_ylabel(y)
309/11:
pca = PCA(n_components=boston_df.shape[1], svd_solver= 'full')
df = pd.DataFrame(pca.fit_transform(boston_df),index=boston_df.index,columns=boston_df.columns) #[i.lower()+'_pca' for i in boston_df.columns]
plt.scatter(x=df[x],y=df[y])
plt.title('PCA Graph')
309/12:
def cov_matrix(data, verbose=False):
    covariance_matrix = np.cov(data, rowvar=False)
    if is_pos_def(covariance_matrix):
        inv_covariance_matrix = np.linalg.inv(covariance_matrix)
        if is_pos_def(inv_covariance_matrix):
            return covariance_matrix, inv_covariance_matrix
        else:
            print("Error: Inverse of Covariance Matrix is not positive definite!")
    else:
        print("Error: Covariance Matrix is not positive definite!")

def MahalanobisDist(inv_cov_matrix, mean_distr, data, verbose=False):
    inv_covariance_matrix = inv_cov_matrix
    vars_mean = mean_distr
    diff = data - vars_mean
    md = []
    for i in range(len(diff)):
        md.append(np.sqrt(diff[i].dot(inv_covariance_matrix).dot(diff[i])))
    return np.array(md)

def MD_detectOutliers(dist, extreme=False, verbose=False):
    k = 3. if extreme else 2.
    threshold = np.mean(dist) * k
    outliers = []
    for i in range(len(dist)):
        if dist[i] >= threshold:
            outliers.append(i)  # index of the outlier
    return np.array(outliers)

def MD_threshold(dist, extreme=False, verbose=False):
    k = 3. if extreme else 2.
    threshold = np.mean(dist) * k
    return threshold

# Check that matrix is positive definite
def is_pos_def(A):
    if np.allclose(A, A.T):
        try:
            np.linalg.cholesky(A)
            return True
        except np.linalg.LinAlgError:
            return False
    else:
        return False
309/13:
cov_matrix = np.cov(df.values,rowvar=False)
inv_cov = np.linalg.inv(cov_matrix)
mean = df.values.mean(axis=0)

# Check matrices are positive definite:https://en.wikipedia.org/wiki/Definiteness_of_a_matrix 
assert is_pos_def(cov_matrix) and is_pos_def(inv_cov)
# Check matrices are invereses
np.matmul(cov_matrix,inv_cov).astype(np.float16)
309/14:
md = MahalanobisDist(inv_cov, mean, df.values, verbose=False)
threshold = MD_threshold(md, extreme = True)
print("Threshold: "+str(threshold))
plt.hist(list(md),bins=40)
plt.show()
309/15:
plt.figure()
sns.distplot(np.square(md), bins = 40, kde= False)
plt.xlim([0.0,15])

plt.figure()
sns.distplot(md,
             bins = 40, 
             kde= True, 
            color = 'green');
# plt.xlim([0.0,5])
plt.xlabel('Mahalanobis dist')
309/16:
# classify what data is an outlier  
# boston_df['anomaly'] = df['anomaly'] = md>threshold
# boston_df[boston_df.anomaly]
len(boston_df[md>threshold])
309/17:
plt.scatter(boston_df[x][md<=threshold], boston_df[y][md<=threshold])
plt.scatter(boston_df[x][md>threshold], boston_df[y][md>threshold])
plt.show()

plt.scatter(df[x][md<=threshold], df[y][md<=threshold])
plt.scatter(df[x][md>threshold], df[y][md>threshold])
# plt.plot([threshold]*len(df[x]_pca),df.indus_pca,colour)
309/18:
def corrdot(*args, **kwargs):
    corr_r = args[0].corr(args[1], 'pearson')
    corr_text = f"{corr_r:2.2f}".replace("0.", ".")
    ax = plt.gca()
    ax.set_axis_off()
    marker_size = abs(corr_r) * 10000
    ax.scatter([.5], [.5], marker_size, [corr_r], alpha=0.6, cmap="coolwarm",
               vmin=-1, vmax=1, transform=ax.transAxes)
    font_size = abs(corr_r) * 40 + 5
    ax.annotate(corr_text, [.5, .5,],  xycoords="axes fraction",
                ha='center', va='center', fontsize=font_size)

g = sns.PairGrid(boston_df, aspect=1.4, diag_sharey=False)
g.map_lower(sns.regplot, lowess=True, ci=False, line_kws={'color': 'black'})
g.map_diag(sns.distplot, kde_kws={'color': 'black'})
g.map_upper(corrdot)
310/1:
#Import the libraries
import os
import numpy as np
import pandas as pd
from sklearn import preprocessing
from sklearn.datasets import load_boston,load_iris
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

import seaborn as sns
sns.set(color_codes=True)
310/2:
def corrdot(*args, **kwargs):
    corr_r = args[0].corr(args[1], 'pearson')
    corr_text = f"{corr_r:2.2f}".replace("0.", ".")
    ax = plt.gca()
    ax.set_axis_off()
    marker_size = abs(corr_r) * 10000
    ax.scatter([.5], [.5], marker_size, [corr_r], alpha=0.6, cmap="coolwarm",
               vmin=-1, vmax=1, transform=ax.transAxes)
    font_size = abs(corr_r) * 40 + 5
    ax.annotate(corr_text, [.5, .5,],  xycoords="axes fraction",
                ha='center', va='center', fontsize=font_size)

g = sns.PairGrid(boston_data[:10], aspect=1.4, diag_sharey=False)
g.map_lower(sns.regplot, lowess=True, ci=False, line_kws={'color': 'black'})
g.map_diag(sns.distplot, kde_kws={'color': 'black'})
g.map_upper(corrdot)
310/3:
#Load the data
boston = load_boston()

#Find features and target
x = boston.data
y = boston.target

#Find the dic keys
print(boston.keys())
310/4:
#find features name
columns = boston.feature_names
columns
310/5:
#Create dataframe
boston_data = pd.DataFrame(boston.data)
boston_data.columns = columns
print(boston_data.shape)
boston_data['target'] = boston.target
boston_data
310/6:
plt.figure(figsize= (4,4), dpi=100)
sns.heatmap(boston_data.corr())
310/7:
def corrdot(*args, **kwargs):
    corr_r = args[0].corr(args[1], 'pearson')
    corr_text = f"{corr_r:2.2f}".replace("0.", ".")
    ax = plt.gca()
    ax.set_axis_off()
    marker_size = abs(corr_r) * 10000
    ax.scatter([.5], [.5], marker_size, [corr_r], alpha=0.6, cmap="coolwarm",
               vmin=-1, vmax=1, transform=ax.transAxes)
    font_size = abs(corr_r) * 40 + 5
    ax.annotate(corr_text, [.5, .5,],  xycoords="axes fraction",
                ha='center', va='center', fontsize=font_size)

g = sns.PairGrid(boston_data[:10], aspect=1.4, diag_sharey=False)
g.map_lower(sns.regplot, lowess=True, ci=False, line_kws={'color': 'black'})
g.map_diag(sns.distplot, kde_kws={'color': 'black'})
g.map_upper(corrdot)
310/8:
def corrdot(*args, **kwargs):
    corr_r = args[0].corr(args[1], 'pearson')
    corr_text = f"{corr_r:2.2f}".replace("0.", ".")
    ax = plt.gca()
    ax.set_axis_off()
    marker_size = abs(corr_r) * 10000
    ax.scatter([.5], [.5], marker_size, [corr_r], alpha=0.6, cmap="coolwarm",
               vmin=-1, vmax=1, transform=ax.transAxes)
    font_size = abs(corr_r) * 40 + 5
    ax.annotate(corr_text, [.5, .5,],  xycoords="axes fraction",
                ha='center', va='center', fontsize=font_size)

g = sns.PairGrid(boston_data[:10], aspect=1.4, diag_sharey=False)
g.map_lower(sns.regplot, lowess=True, ci=False, line_kws={'color': 'black'})
g.map_diag(sns.distplot, kde_kws={'color': 'black'})
g.map_upper(corrdot)
boston_data
310/9: boston_data
310/10: boston_data
310/11: boston_data
310/12:
#Import the libraries
import os
import numpy as np
import pandas as pd
from sklearn import preprocessing
from sklearn.datasets import load_boston,load_iris
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

import seaborn as sns
sns.set(color_codes=True)
310/13:
#Load the data
boston = load_boston()

#Find features and target
x = boston.data
y = boston.target

#Find the dic keys
print(boston.keys())
310/14:
#find features name
columns = boston.feature_names
columns
310/15:
#Create dataframe
boston_data = pd.DataFrame(boston.data)
boston_data.columns = columns
print(boston_data.shape)
boston_data['target'] = boston.target
boston_data
310/16:
plt.figure(figsize= (4,4), dpi=100)
sns.heatmap(boston_data.corr())
310/17: boston_data
310/18:
def corrdot(*args, **kwargs):
    corr_r = args[0].corr(args[1], 'pearson')
    corr_text = f"{corr_r:2.2f}".replace("0.", ".")
    ax = plt.gca()
    ax.set_axis_off()
    marker_size = abs(corr_r) * 10000
    ax.scatter([.5], [.5], marker_size, [corr_r], alpha=0.6, cmap="coolwarm",
               vmin=-1, vmax=1, transform=ax.transAxes)
    font_size = abs(corr_r) * 40 + 5
    ax.annotate(corr_text, [.5, .5,],  xycoords="axes fraction",
                ha='center', va='center', fontsize=font_size)

g = sns.PairGrid(boston_data[:2], aspect=1.4, diag_sharey=False)
g.map_lower(sns.regplot, lowess=True, ci=False, line_kws={'color': 'black'})
g.map_diag(sns.distplot, kde_kws={'color': 'black'})
g.map_upper(corrdot)
310/19: boston_data[:2]
310/20: boston_data[:2]
310/21: boston_data.iloc[:2]
310/22: boston_data.iloc[,:2]
310/23: boston_data.iloc[,2]
310/24: boston_data.iloc[:2]
310/25: boston_data.loc[:,2]
310/26: boston_data.iloc[:,2]
310/27: boston_data.iloc[:,:2]
310/28: boston_data.iloc[:,:5]
310/29:
def corrdot(*args, **kwargs):
    corr_r = args[0].corr(args[1], 'pearson')
    corr_text = f"{corr_r:2.2f}".replace("0.", ".")
    ax = plt.gca()
    ax.set_axis_off()
    marker_size = abs(corr_r) * 10000
    ax.scatter([.5], [.5], marker_size, [corr_r], alpha=0.6, cmap="coolwarm",
               vmin=-1, vmax=1, transform=ax.transAxes)
    font_size = abs(corr_r) * 40 + 5
    ax.annotate(corr_text, [.5, .5,],  xycoords="axes fraction",
                ha='center', va='center', fontsize=font_size)

g = sns.PairGrid(boston_data.iloc[:,:5], aspect=1.4, diag_sharey=False)
g.map_lower(sns.regplot, lowess=True, ci=False, line_kws={'color': 'black'})
g.map_diag(sns.distplot, kde_kws={'color': 'black'})
g.map_upper(corrdot)
310/30:
def corrdot(*args, **kwargs):
    corr_r = args[0].corr(args[1], 'pearson')
    corr_text = f"{corr_r:2.2f}".replace("0.", ".")
    ax = plt.gca()
    ax.set_axis_off()
    marker_size = abs(corr_r) * 10000
    ax.scatter([.5], [.5], marker_size, [corr_r], alpha=0.6, cmap="coolwarm",
               vmin=-1, vmax=1, transform=ax.transAxes)
    font_size = abs(corr_r) * 40 + 5
    ax.annotate(corr_text, [.5, .5,],  xycoords="axes fraction",
                ha='center', va='center', fontsize=font_size)

g = sns.PairGrid(boston_data.iloc[:,:2], aspect=1.4, diag_sharey=False)
g.map_lower(sns.regplot, lowess=True, ci=False, line_kws={'color': 'black'})
g.map_diag(sns.distplot, kde_kws={'color': 'black'})
g.map_upper(corrdot)
310/31:
def corrdot(*args, **kwargs):
    corr_r = args[0].corr(args[1], 'pearson')
    corr_text = f"{corr_r:2.2f}".replace("0.", ".")
    ax = plt.gca()
    ax.set_axis_off()
    marker_size = abs(corr_r) * 10000
    ax.scatter([.5], [.5], marker_size, [corr_r], alpha=0.6, cmap="coolwarm",
               vmin=-1, vmax=1, transform=ax.transAxes)
    font_size = abs(corr_r) * 40 + 5
    ax.annotate(corr_text, [.5, .5,],  xycoords="axes fraction",
                ha='center', va='center', fontsize=font_size)

g = sns.PairGrid(boston_data.iloc[:,:3], aspect=1.4, diag_sharey=False)
g.map_lower(sns.regplot, lowess=True, ci=False, line_kws={'color': 'black'})
g.map_diag(sns.distplot, kde_kws={'color': 'black'})
g.map_upper(corrdot)
310/32:
def corrdot(*args, **kwargs):
    corr_r = args[0].corr(args[1], 'pearson')
    corr_text = f"{corr_r:2.2f}".replace("0.", ".")
    ax = plt.gca()
    ax.set_axis_off()
    marker_size = abs(corr_r) * 10000
    ax.scatter([.5], [.5], marker_size, [corr_r], alpha=0.6, cmap="coolwarm",
               vmin=-1, vmax=1, transform=ax.transAxes)
    font_size = abs(corr_r) * 40 + 5
    ax.annotate(corr_text, [.5, .5,],  xycoords="axes fraction",
                ha='center', va='center', fontsize=font_size)

g = sns.PairGrid(boston_data.iloc[:,:4], aspect=1.4, diag_sharey=False)
g.map_lower(sns.regplot, lowess=True, ci=False, line_kws={'color': 'black'})
g.map_diag(sns.distplot, kde_kws={'color': 'black'})
g.map_upper(corrdot)
310/33:
def corrdot(*args, **kwargs):
    corr_r = args[0].corr(args[1], 'pearson')
    corr_text = f"{corr_r:2.2f}".replace("0.", ".")
    ax = plt.gca()
    ax.set_axis_off()
    marker_size = abs(corr_r) * 10000
    ax.scatter([.5], [.5], marker_size, [corr_r], alpha=0.6, cmap="coolwarm",
               vmin=-1, vmax=1, transform=ax.transAxes)
    font_size = abs(corr_r) * 40 + 5
    ax.annotate(corr_text, [.5, .5,],  xycoords="axes fraction",
                ha='center', va='center', fontsize=font_size)

g = sns.PairGrid(boston_data.iloc[:,:3], aspect=1.4, diag_sharey=False)
g.map_lower(sns.regplot, lowess=True, ci=False, line_kws={'color': 'black'})
g.map_diag(sns.distplot, kde_kws={'color': 'black'})
g.map_upper(corrdot)
310/34:
def corrdot(*args, **kwargs):
    corr_r = args[0].corr(args[1], 'pearson')
    corr_text = f"{corr_r:2.2f}".replace("0.", ".")
    ax = plt.gca()
    ax.set_axis_off()
    marker_size = abs(corr_r) * 10000
    ax.scatter([.5], [.5], marker_size, [corr_r], alpha=0.6, cmap="coolwarm",
               vmin=-1, vmax=1, transform=ax.transAxes)
    font_size = abs(corr_r) * 40 + 5
    ax.annotate(corr_text, [.5, .5,],  xycoords="axes fraction",
                ha='center', va='center', fontsize=font_size)

g = sns.PairGrid(boston_data.iloc[:,:3], aspect=1.4, diag_sharey=False)
g.map_lower(sns.regplot, lowess=False, ci=False, line_kws={'color': 'black'})
g.map_diag(sns.distplot, kde_kws={'color': 'black'})
g.map_upper(corrdot)
310/35:
def corrdot(*args, **kwargs):
    corr_r = args[0].corr(args[1], 'pearson')
    corr_text = f"{corr_r:2.2f}".replace("0.", ".")
    ax = plt.gca()
    ax.set_axis_off()
    marker_size = abs(corr_r) * 10000
    ax.scatter([.5], [.5], marker_size, [corr_r], alpha=0.6, cmap="coolwarm",
               vmin=-1, vmax=1, transform=ax.transAxes)
    font_size = abs(corr_r) * 40 + 5
    ax.annotate(corr_text, [.5, .5,],  xycoords="axes fraction",
                ha='center', va='center', fontsize=font_size)

g = sns.PairGrid(boston_data.iloc[:,:3], aspect=1.4, diag_sharey=False)
g.map_lower(sns.regplot, lowess=True, ci=False, line_kws={'color': 'black'})
g.map_diag(sns.distplot, kde_kws={'color': 'black'})
# g.map_upper(corrdot)
310/36:
def corrdot(*args, **kwargs):
    corr_r = args[0].corr(args[1], 'pearson')
    corr_text = f"{corr_r:2.2f}".replace("0.", ".")
    ax = plt.gca()
    ax.set_axis_off()
    marker_size = abs(corr_r) * 10000
    ax.scatter([.5], [.5], marker_size, [corr_r], alpha=0.6, cmap="coolwarm",
               vmin=-1, vmax=1, transform=ax.transAxes)
    font_size = abs(corr_r) * 40 + 5
    ax.annotate(corr_text, [.5, .5,],  xycoords="axes fraction",
                ha='center', va='center', fontsize=font_size)

g = sns.PairGrid(boston_data.iloc[:,:3], aspect=1.4, diag_sharey=False)
g.map_lower(sns.regplot, lowess=True, ci=False, line_kws={'color': 'black'})
# g.map_diag(sns.distplot, kde_kws={'color': 'black'})
g.map_upper(corrdot)
310/37:
def corrdot(*args, **kwargs):
    corr_r = args[0].corr(args[1], 'pearson')
    corr_text = f"{corr_r:2.2f}".replace("0.", ".")
    ax = plt.gca()
    ax.set_axis_off()
    marker_size = abs(corr_r) * 10000
    ax.scatter([.5], [.5], marker_size, [corr_r], alpha=0.6, cmap="coolwarm",
               vmin=-1, vmax=1, transform=ax.transAxes)
    font_size = abs(corr_r) * 40 + 5
    ax.annotate(corr_text, [.5, .5,],  xycoords="axes fraction",
                ha='center', va='center', fontsize=font_size)

g = sns.PairGrid(boston_data.iloc[:,:3], aspect=1.4, diag_sharey=False)
# g.map_lower(sns.regplot, lowess=True, ci=False, line_kws={'color': 'black'})
g.map_diag(sns.distplot, kde_kws={'color': 'black'})
g.map_upper(corrdot)
310/38:
def corrdot(*args, **kwargs):
    corr_r = args[0].corr(args[1], 'pearson')
    corr_text = f"{corr_r:2.2f}".replace("0.", ".")
    ax = plt.gca()
    ax.set_axis_off()
    marker_size = abs(corr_r) * 10000
    ax.scatter([.5], [.5], marker_size, [corr_r], alpha=0.6, cmap="coolwarm",
               vmin=-1, vmax=1, transform=ax.transAxes)
    font_size = abs(corr_r) * 40 + 5
    ax.annotate(corr_text, [.5, .5,],  xycoords="axes fraction",
                ha='center', va='center', fontsize=font_size)

g = sns.PairGrid(boston_data.iloc[:,:3], aspect=1.4, diag_sharey=False)
g.map_lower(sns.regplot, lowess=False, ci=True, line_kws={'color': 'black'})
g.map_diag(sns.distplot, kde_kws={'color': 'black'})
g.map_upper(corrdot)
310/39:
def corrdot(*args, **kwargs):
    corr_r = args[0].corr(args[1], 'pearson')
    corr_text = f"{corr_r:2.2f}".replace("0.", ".")
    ax = plt.gca()
    ax.set_axis_off()
    marker_size = abs(corr_r) * 10000
    ax.scatter([.5], [.5], marker_size, [corr_r], alpha=0.6, cmap="coolwarm",
               vmin=-1, vmax=1, transform=ax.transAxes)
    font_size = abs(corr_r) * 40 + 5
    ax.annotate(corr_text, [.5, .5,],  xycoords="axes fraction",
                ha='center', va='center', fontsize=font_size)

g = sns.PairGrid(boston_data.iloc[:,:3], aspect=1.4, diag_sharey=False)
g.map_lower(sns.regplot,bins=15, lowess=True, ci=False, line_kws={'color': 'black'})
g.map_diag(sns.distplot, kde_kws={'color': 'black'})
g.map_upper(corrdot)
310/40:
def corrdot(*args, **kwargs):
    corr_r = args[0].corr(args[1], 'pearson')
    corr_text = f"{corr_r:2.2f}".replace("0.", ".")
    ax = plt.gca()
    ax.set_axis_off()
    marker_size = abs(corr_r) * 10000
    ax.scatter([.5], [.5], marker_size, [corr_r], alpha=0.6, cmap="coolwarm",
               vmin=-1, vmax=1, transform=ax.transAxes)
    font_size = abs(corr_r) * 40 + 5
    ax.annotate(corr_text, [.5, .5,],  xycoords="axes fraction",
                ha='center', va='center', fontsize=font_size)

g = sns.PairGrid(boston_data.iloc[:,:3], aspect=1.4, diag_sharey=False)
g.map_lower(sns.regplot, lowess=True, ci=False, line_kws={'color': 'black'})
g.map_diag(sns.distplot, kde_kws={'color': 'black'})
g.map_upper(corrdot)
310/41:
def corrdot(*args, **kwargs):
    corr_r = args[0].corr(args[1], 'pearson')
    corr_text = f"{corr_r:2.2f}".replace("0.", ".")
    ax = plt.gca()
    ax.set_axis_off()
    marker_size = abs(corr_r) * 10000
    ax.scatter([.5], [.5], marker_size, [corr_r], alpha=0.6, cmap="coolwarm",
               vmin=-1, vmax=1, transform=ax.transAxes)
    font_size = abs(corr_r) * 40 + 5
    ax.annotate(corr_text, [.5, .5,],  xycoords="axes fraction",
                ha='center', va='center', fontsize=font_size)

g = sns.PairGrid(boston_data.iloc[:,:3], aspect=1.4, diag_sharey=False)
g.map_lower(sns.regplot)
g.map_diag(sns.distplot, kde_kws={'color': 'black'})
g.map_upper(corrdot)
310/42:
def corrdot(*args, **kwargs):
    corr_r = args[0].corr(args[1], 'pearson')
    corr_text = f"{corr_r:2.2f}".replace("0.", ".")
    ax = plt.gca()
    ax.set_axis_off()
    marker_size = abs(corr_r) * 10000
    ax.scatter([.5], [.5], marker_size, [corr_r], alpha=0.6, cmap="coolwarm",
               vmin=-1, vmax=1, transform=ax.transAxes)
    font_size = abs(corr_r) * 40 + 5
    ax.annotate(corr_text, [.5, .5,],  xycoords="axes fraction",
                ha='center', va='center', fontsize=font_size)

g = sns.PairGrid(boston_data.iloc[:,:3], aspect=1.4, diag_sharey=False)
g.map_lower(sns.regplot, lowess=True, ci=False, line_kws={'color': 'black'})
g.map_diag(sns.distplot, kde_kws={'color': 'black'})
g.map_upper(corrdot)
310/43:
def corrdot(*args, **kwargs):
    corr_r = args[0].corr(args[1], 'pearson')
    corr_text = f"{corr_r:2.2f}".replace("0.", ".")
    ax = plt.gca()
    ax.set_axis_off()
    marker_size = abs(corr_r) * 10000
    ax.scatter([.5], [.5], marker_size, [corr_r], alpha=0.6, cmap="coolwarm",
               vmin=-1, vmax=1, transform=ax.transAxes)
    font_size = abs(corr_r) * 40 + 5
    ax.annotate(corr_text, [.5, .5,],  xycoords="axes fraction",
                ha='center', va='center', fontsize=font_size)

g = sns.PairGrid(boston_data.iloc[:,5:9], aspect=1.4, diag_sharey=False)
g.map_lower(sns.regplot, lowess=True, ci=False, line_kws={'color': 'black'})
g.map_diag(sns.distplot, kde_kws={'color': 'black'})
g.map_upper(corrdot)
310/44:
def corrdot(*args, **kwargs):
    corr_r = args[0].corr(args[1], 'pearson')
    corr_text = f"{corr_r:2.2f}".replace("0.", ".")
    ax = plt.gca()
    ax.set_axis_off()
    marker_size = abs(corr_r) * 10000
    ax.scatter([.5], [.5], marker_size, [corr_r], alpha=0.6, cmap="coolwarm",
               vmin=-1, vmax=1, transform=ax.transAxes)
    font_size = abs(corr_r) * 40 + 5
    ax.annotate(corr_text, [.5, .5,],  xycoords="axes fraction",
                ha='center', va='center', fontsize=font_size)

g = sns.PairGrid(boston_data.iloc[:,5:12], aspect=1.4, diag_sharey=False)
g.map_lower(sns.regplot, lowess=True, ci=False, line_kws={'color': 'black'})
g.map_diag(sns.distplot, kde_kws={'color': 'black'})
g.map_upper(corrdot)
310/45: boston_data
310/46:
plt.figure(figsize= (4,4), dpi=100)
sns.heatmap(boston_data.corr(),cmap='hot')
310/47:
x='RAD'
y='TAX'
boston_df = boston_df[[x,y]]
boston_df
310/48: boston_df.corr()
310/49:
#Multivariate outlier analysis
fig, ax = plt.subplots(figsize=(11,8.5))
ax.scatter(boston_df[x], boston_df[y])
ax.set_xlabel(x)
ax.set_ylabel(y)
310/50:
pca = PCA(n_components=boston_df.shape[1], svd_solver= 'full')
df = pd.DataFrame(pca.fit_transform(boston_df),index=boston_df.index,columns=boston_df.columns) #[i.lower()+'_pca' for i in boston_df.columns]
plt.scatter(x=df[x],y=df[y])
plt.title('PCA Graph')
310/51:
def cov_matrix(data, verbose=False):
    covariance_matrix = np.cov(data, rowvar=False)
    if is_pos_def(covariance_matrix):
        inv_covariance_matrix = np.linalg.inv(covariance_matrix)
        if is_pos_def(inv_covariance_matrix):
            return covariance_matrix, inv_covariance_matrix
        else:
            print("Error: Inverse of Covariance Matrix is not positive definite!")
    else:
        print("Error: Covariance Matrix is not positive definite!")

def MahalanobisDist(inv_cov_matrix, mean_distr, data, verbose=False):
    inv_covariance_matrix = inv_cov_matrix
    vars_mean = mean_distr
    diff = data - vars_mean
    md = []
    for i in range(len(diff)):
        md.append(np.sqrt(diff[i].dot(inv_covariance_matrix).dot(diff[i])))
    return np.array(md)

def MD_detectOutliers(dist, extreme=False, verbose=False):
    k = 3. if extreme else 2.
    threshold = np.mean(dist) * k
    outliers = []
    for i in range(len(dist)):
        if dist[i] >= threshold:
            outliers.append(i)  # index of the outlier
    return np.array(outliers)

def MD_threshold(dist, extreme=False, verbose=False):
    k = 3. if extreme else 2.
    threshold = np.mean(dist) * k
    return threshold

# Check that matrix is positive definite
def is_pos_def(A):
    if np.allclose(A, A.T):
        try:
            np.linalg.cholesky(A)
            return True
        except np.linalg.LinAlgError:
            return False
    else:
        return False
310/52:
cov_matrix = np.cov(df.values,rowvar=False)
inv_cov = np.linalg.inv(cov_matrix)
mean = df.values.mean(axis=0)

# Check matrices are positive definite:https://en.wikipedia.org/wiki/Definiteness_of_a_matrix 
assert is_pos_def(cov_matrix) and is_pos_def(inv_cov)
# Check matrices are invereses
np.matmul(cov_matrix,inv_cov).astype(np.float16)
310/53:
md = MahalanobisDist(inv_cov, mean, df.values, verbose=False)
threshold = MD_threshold(md, extreme = True)
print("Threshold: "+str(threshold))
plt.hist(list(md),bins=40)
plt.show()
310/54:
plt.figure()
sns.distplot(np.square(md), bins = 40, kde= False)
plt.xlim([0.0,15])

plt.figure()
sns.distplot(md,
             bins = 40, 
             kde= True, 
            color = 'green');
# plt.xlim([0.0,5])
plt.xlabel('Mahalanobis dist')
310/55:
# classify what data is an outlier  
# boston_df['anomaly'] = df['anomaly'] = md>threshold
# boston_df[boston_df.anomaly]
len(boston_df[md>threshold])
310/56:
plt.scatter(boston_df[x][md<=threshold], boston_df[y][md<=threshold])
plt.scatter(boston_df[x][md>threshold], boston_df[y][md>threshold])
plt.show()

plt.scatter(df[x][md<=threshold], df[y][md<=threshold])
plt.scatter(df[x][md>threshold], df[y][md>threshold])
# plt.plot([threshold]*len(df[x]_pca),df.indus_pca,colour)
310/57:
x='RAD'
y='TAX'
boston_df = boston_data[[x,y]]
boston_df
310/58:
x='RAD'
y='TAX'
boston_df = boston_data[[x,y]]
boston_df
310/59: boston_df.corr()
310/60:
x='RAD'
y='TAX'
boston_df = boston_data[[x,y]]
boston_df
310/61:
#Multivariate outlier analysis
fig, ax = plt.subplots(figsize=(11,8.5))
ax.scatter(boston_df[x], boston_df[y])
ax.set_xlabel(x)
ax.set_ylabel(y)
310/62: boston_df.corr()
310/63:
pca = PCA(n_components=boston_df.shape[1], svd_solver= 'full')
df = pd.DataFrame(pca.fit_transform(boston_df),index=boston_df.index,columns=boston_df.columns) #[i.lower()+'_pca' for i in boston_df.columns]
plt.scatter(x=df[x],y=df[y])
plt.title('PCA Graph')
310/64:
#Multivariate outlier analysis
fig, ax = plt.subplots(figsize=(11,8.5))
ax.scatter(boston_df[x], boston_df[y])
ax.set_xlabel(x)
ax.set_ylabel(y)
310/65:
def cov_matrix(data, verbose=False):
    covariance_matrix = np.cov(data, rowvar=False)
    if is_pos_def(covariance_matrix):
        inv_covariance_matrix = np.linalg.inv(covariance_matrix)
        if is_pos_def(inv_covariance_matrix):
            return covariance_matrix, inv_covariance_matrix
        else:
            print("Error: Inverse of Covariance Matrix is not positive definite!")
    else:
        print("Error: Covariance Matrix is not positive definite!")

def MahalanobisDist(inv_cov_matrix, mean_distr, data, verbose=False):
    inv_covariance_matrix = inv_cov_matrix
    vars_mean = mean_distr
    diff = data - vars_mean
    md = []
    for i in range(len(diff)):
        md.append(np.sqrt(diff[i].dot(inv_covariance_matrix).dot(diff[i])))
    return np.array(md)

def MD_detectOutliers(dist, extreme=False, verbose=False):
    k = 3. if extreme else 2.
    threshold = np.mean(dist) * k
    outliers = []
    for i in range(len(dist)):
        if dist[i] >= threshold:
            outliers.append(i)  # index of the outlier
    return np.array(outliers)

def MD_threshold(dist, extreme=False, verbose=False):
    k = 3. if extreme else 2.
    threshold = np.mean(dist) * k
    return threshold

# Check that matrix is positive definite
def is_pos_def(A):
    if np.allclose(A, A.T):
        try:
            np.linalg.cholesky(A)
            return True
        except np.linalg.LinAlgError:
            return False
    else:
        return False
310/66:
pca = PCA(n_components=boston_df.shape[1], svd_solver= 'full')
df = pd.DataFrame(pca.fit_transform(boston_df),index=boston_df.index,columns=boston_df.columns) #[i.lower()+'_pca' for i in boston_df.columns]
plt.scatter(x=df[x],y=df[y])
plt.title('PCA Graph')
310/67:
cov_matrix = np.cov(df.values,rowvar=False)
inv_cov = np.linalg.inv(cov_matrix)
mean = df.values.mean(axis=0)

# Check matrices are positive definite:https://en.wikipedia.org/wiki/Definiteness_of_a_matrix 
assert is_pos_def(cov_matrix) and is_pos_def(inv_cov)
# Check matrices are invereses
np.matmul(cov_matrix,inv_cov).astype(np.float16)
310/68:
def cov_matrix(data, verbose=False):
    covariance_matrix = np.cov(data, rowvar=False)
    if is_pos_def(covariance_matrix):
        inv_covariance_matrix = np.linalg.inv(covariance_matrix)
        if is_pos_def(inv_covariance_matrix):
            return covariance_matrix, inv_covariance_matrix
        else:
            print("Error: Inverse of Covariance Matrix is not positive definite!")
    else:
        print("Error: Covariance Matrix is not positive definite!")

def MahalanobisDist(inv_cov_matrix, mean_distr, data, verbose=False):
    inv_covariance_matrix = inv_cov_matrix
    vars_mean = mean_distr
    diff = data - vars_mean
    md = []
    for i in range(len(diff)):
        md.append(np.sqrt(diff[i].dot(inv_covariance_matrix).dot(diff[i])))
    return np.array(md)

def MD_detectOutliers(dist, extreme=False, verbose=False):
    k = 3. if extreme else 2.
    threshold = np.mean(dist) * k
    outliers = []
    for i in range(len(dist)):
        if dist[i] >= threshold:
            outliers.append(i)  # index of the outlier
    return np.array(outliers)

def MD_threshold(dist, extreme=False, verbose=False):
    k = 3. if extreme else 2.
    threshold = np.mean(dist) * k
    return threshold

# Check that matrix is positive definite
def is_pos_def(A):
    if np.allclose(A, A.T):
        try:
            np.linalg.cholesky(A)
            return True
        except np.linalg.LinAlgError:
            return False
    else:
        return False
310/69:
md = MahalanobisDist(inv_cov, mean, df.values, verbose=False)
threshold = MD_threshold(md, extreme = True)
print("Threshold: "+str(threshold))
plt.hist(list(md),bins=40)
plt.show()
310/70:
cov_matrix = np.cov(df.values,rowvar=False)
inv_cov = np.linalg.inv(cov_matrix)
mean = df.values.mean(axis=0)

# Check matrices are positive definite:https://en.wikipedia.org/wiki/Definiteness_of_a_matrix 
assert is_pos_def(cov_matrix) and is_pos_def(inv_cov)
# Check matrices are invereses
np.matmul(cov_matrix,inv_cov).astype(np.float16)
310/71:
plt.figure()
sns.distplot(np.square(md), bins = 40, kde= False)
plt.xlim([0.0,15])

plt.figure()
sns.distplot(md,
             bins = 40, 
             kde= True, 
            color = 'green');
# plt.xlim([0.0,5])
plt.xlabel('Mahalanobis dist')
310/72:
md = MahalanobisDist(inv_cov, mean, df.values, verbose=False)
threshold = MD_threshold(md, extreme = True)
print("Threshold: "+str(threshold))
plt.hist(list(md),bins=40)
plt.show()
310/73:
# classify what data is an outlier  
# boston_df['anomaly'] = df['anomaly'] = md>threshold
# boston_df[boston_df.anomaly]
len(boston_df[md>threshold])
310/74:
plt.figure()
sns.distplot(np.square(md), bins = 40, kde= False)
plt.xlim([0.0,15])

plt.figure()
sns.distplot(md,
             bins = 40, 
             kde= True, 
            color = 'green');
# plt.xlim([0.0,5])
plt.xlabel('Mahalanobis dist')
310/75:
plt.scatter(boston_df[x][md<=threshold], boston_df[y][md<=threshold])
plt.scatter(boston_df[x][md>threshold], boston_df[y][md>threshold])
plt.show()

plt.scatter(df[x][md<=threshold], df[y][md<=threshold])
plt.scatter(df[x][md>threshold], df[y][md>threshold])
# plt.plot([threshold]*len(df[x]_pca),df.indus_pca,colour)
310/76:
# classify what data is an outlier  
# boston_df['anomaly'] = df['anomaly'] = md>threshold
# boston_df[boston_df.anomaly]
len(boston_df[md>threshold])
310/77:
plt.scatter(boston_df[x][md<=threshold], boston_df[y][md<=threshold])
plt.scatter(boston_df[x][md>threshold], boston_df[y][md>threshold])
plt.show()

plt.scatter(df[x][md<=threshold], df[y][md<=threshold])
plt.scatter(df[x][md>threshold], df[y][md>threshold])
# plt.plot([threshold]*len(df[x]_pca),df.indus_pca,colour)
310/78:
md = MahalanobisDist(inv_cov, mean, df.values, verbose=False)
threshold = MD_threshold(md, extreme = False)
print("Threshold: "+str(threshold))
plt.hist(list(md),bins=40)
plt.show()
310/79:
plt.figure()
sns.distplot(np.square(md), bins = 40, kde= False)
plt.xlim([0.0,15])

plt.figure()
sns.distplot(md,
             bins = 40, 
             kde= True, 
            color = 'green');
# plt.xlim([0.0,5])
plt.xlabel('Mahalanobis dist')
310/80:
# classify what data is an outlier  
# boston_df['anomaly'] = df['anomaly'] = md>threshold
# boston_df[boston_df.anomaly]
len(boston_df[md>threshold])
310/81:
plt.scatter(boston_df[x][md<=threshold], boston_df[y][md<=threshold])
plt.scatter(boston_df[x][md>threshold], boston_df[y][md>threshold])
plt.show()

plt.scatter(df[x][md<=threshold], df[y][md<=threshold])
plt.scatter(df[x][md>threshold], df[y][md>threshold])
# plt.plot([threshold]*len(df[x]_pca),df.indus_pca,colour)
310/82:
plt.scatter(boston_df[x][md<=threshold], boston_df[y][md<=threshold])
plt.scatter(boston_df[x][md>threshold], boston_df[y][md>threshold])
plt.show()

plt.scatter(df[x][md<=threshold], df[y][md<=threshold])
plt.scatter(df[x][md>threshold], df[y][md>threshold],figsize=(6,6))
# plt.plot([threshold]*len(df[x]_pca),df.indus_pca,colour)
310/83:
plt.figure(figsize=(6,6))
plt.scatter(boston_df[x][md<=threshold], boston_df[y][md<=threshold])
plt.scatter(boston_df[x][md>threshold], boston_df[y][md>threshold])
plt.show()

plt.scatter(df[x][md<=threshold], df[y][md<=threshold])
plt.scatter(df[x][md>threshold], df[y][md>threshold])
# plt.plot([threshold]*len(df[x]_pca),df.indus_pca,colour)
310/84:
plt.figure(figsize=(10,8))
plt.scatter(boston_df[x][md<=threshold], boston_df[y][md<=threshold])
plt.scatter(boston_df[x][md>threshold], boston_df[y][md>threshold])
plt.show()

plt.scatter(df[x][md<=threshold], df[y][md<=threshold])
plt.scatter(df[x][md>threshold], df[y][md>threshold])
# plt.plot([threshold]*len(df[x]_pca),df.indus_pca,colour)
310/85:
plt.figure(figsize=(10,5))
plt.scatter(boston_df[x][md<=threshold], boston_df[y][md<=threshold])
plt.scatter(boston_df[x][md>threshold], boston_df[y][md>threshold])
plt.show()

plt.scatter(df[x][md<=threshold], df[y][md<=threshold])
plt.scatter(df[x][md>threshold], df[y][md>threshold])
# plt.plot([threshold]*len(df[x]_pca),df.indus_pca,colour)
310/86:
plt.figure(figsize=(10,8))
plt.scatter(boston_df[x][md<=threshold], boston_df[y][md<=threshold])
plt.scatter(boston_df[x][md>threshold], boston_df[y][md>threshold])
plt.show()

plt.figure(figsize=(10,8))
plt.scatter(df[x][md<=threshold], df[y][md<=threshold])
plt.scatter(df[x][md>threshold], df[y][md>threshold])
# plt.plot([threshold]*len(df[x]_pca),df.indus_pca,colour)
310/87:
x='INDUS'
y='TAX'
boston_df = boston_data[[x,y]]
boston_df
310/88: boston_df.corr()
310/89:
#Multivariate outlier analysis
fig, ax = plt.subplots(figsize=(11,8.5))
ax.scatter(boston_df[x], boston_df[y])
ax.set_xlabel(x)
ax.set_ylabel(y)
310/90:
pca = PCA(n_components=boston_df.shape[1], svd_solver= 'full')
df = pd.DataFrame(pca.fit_transform(boston_df),index=boston_df.index,columns=boston_df.columns) #[i.lower()+'_pca' for i in boston_df.columns]
plt.scatter(x=df[x],y=df[y])
plt.title('PCA Graph')
310/91:
def cov_matrix(data, verbose=False):
    covariance_matrix = np.cov(data, rowvar=False)
    if is_pos_def(covariance_matrix):
        inv_covariance_matrix = np.linalg.inv(covariance_matrix)
        if is_pos_def(inv_covariance_matrix):
            return covariance_matrix, inv_covariance_matrix
        else:
            print("Error: Inverse of Covariance Matrix is not positive definite!")
    else:
        print("Error: Covariance Matrix is not positive definite!")

def MahalanobisDist(inv_cov_matrix, mean_distr, data, verbose=False):
    inv_covariance_matrix = inv_cov_matrix
    vars_mean = mean_distr
    diff = data - vars_mean
    md = []
    for i in range(len(diff)):
        md.append(np.sqrt(diff[i].dot(inv_covariance_matrix).dot(diff[i])))
    return np.array(md)

def MD_detectOutliers(dist, extreme=False, verbose=False):
    k = 3. if extreme else 2.
    threshold = np.mean(dist) * k
    outliers = []
    for i in range(len(dist)):
        if dist[i] >= threshold:
            outliers.append(i)  # index of the outlier
    return np.array(outliers)

def MD_threshold(dist, extreme=False, verbose=False):
    k = 3. if extreme else 2.
    threshold = np.mean(dist) * k
    return threshold

# Check that matrix is positive definite
def is_pos_def(A):
    if np.allclose(A, A.T):
        try:
            np.linalg.cholesky(A)
            return True
        except np.linalg.LinAlgError:
            return False
    else:
        return False
310/92:
cov_matrix = np.cov(df.values,rowvar=False)
inv_cov = np.linalg.inv(cov_matrix)
mean = df.values.mean(axis=0)

# Check matrices are positive definite:https://en.wikipedia.org/wiki/Definiteness_of_a_matrix 
assert is_pos_def(cov_matrix) and is_pos_def(inv_cov)
# Check matrices are invereses
np.matmul(cov_matrix,inv_cov).astype(np.float16)
310/93:
md = MahalanobisDist(inv_cov, mean, df.values, verbose=False)
threshold = MD_threshold(md, extreme = False)
print("Threshold: "+str(threshold))
plt.hist(list(md),bins=40)
plt.show()
310/94:
plt.figure()
sns.distplot(np.square(md), bins = 40, kde= False)
plt.xlim([0.0,15])

plt.figure()
sns.distplot(md,
             bins = 40, 
             kde= True, 
            color = 'green');
# plt.xlim([0.0,5])
plt.xlabel('Mahalanobis dist')
310/95:
# classify what data is an outlier  
# boston_df['anomaly'] = df['anomaly'] = md>threshold
# boston_df[boston_df.anomaly]
len(boston_df[md>threshold])
310/96:
plt.figure(figsize=(10,8))
plt.scatter(boston_df[x][md<=threshold], boston_df[y][md<=threshold])
plt.scatter(boston_df[x][md>threshold], boston_df[y][md>threshold])
plt.show()

plt.figure(figsize=(10,8))
plt.scatter(df[x][md<=threshold], df[y][md<=threshold])
plt.scatter(df[x][md>threshold], df[y][md>threshold])
# plt.plot([threshold]*len(df[x]_pca),df.indus_pca,colour)
310/97:
md = MahalanobisDist(inv_cov, mean, df.values, verbose=False)
threshold = 2#MD_threshold(md, extreme = False)
print("Threshold: "+ str(threshold))
plt.hist(list(md),bins=40)
plt.show()
311/1:
plt.figure(figsize=(10,8))
plt.scatter(boston_df[x][md<=threshold], boston_df[y][md<=threshold])
plt.scatter(boston_df[x][md>threshold], boston_df[y][md>threshold])
plt.show()

plt.figure(figsize=(10,8))
plt.scatter(df[x][md<=threshold], df[y][md<=threshold])
plt.scatter(df[x][md>threshold], df[y][md>threshold])
# plt.plot([threshold]*len(df[x]_pca),df.indus_pca,colour)
311/2:
#Import the libraries
import os
import numpy as np
import pandas as pd
from sklearn import preprocessing
from sklearn.datasets import load_boston,load_iris
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

import seaborn as sns
sns.set(color_codes=True)
311/3:
#Load the data
boston = load_boston()

#Find features and target
x = boston.data
y = boston.target

#Find the dic keys
print(boston.keys())
311/4:
#find features name
columns = boston.feature_names
columns
311/5:
#Create dataframe
boston_data = pd.DataFrame(boston.data)
boston_data.columns = columns
print(boston_data.shape)
boston_data['target'] = boston.target
boston_data
311/6:
plt.figure(figsize= (4,4), dpi=100)
sns.heatmap(boston_data.corr(),cmap='hot')
311/7:
def corrdot(*args, **kwargs):
    corr_r = args[0].corr(args[1], 'pearson')
    corr_text = f"{corr_r:2.2f}".replace("0.", ".")
    ax = plt.gca()
    ax.set_axis_off()
    marker_size = abs(corr_r) * 10000
    ax.scatter([.5], [.5], marker_size, [corr_r], alpha=0.6, cmap="coolwarm",
               vmin=-1, vmax=1, transform=ax.transAxes)
    font_size = abs(corr_r) * 40 + 5
    ax.annotate(corr_text, [.5, .5,],  xycoords="axes fraction",
                ha='center', va='center', fontsize=font_size)

g = sns.PairGrid(boston_data.iloc[:,5:11], aspect=1.4, diag_sharey=False)
g.map_lower(sns.regplot, lowess=True, ci=False, line_kws={'color': 'black'})
g.map_diag(sns.distplot, kde_kws={'color': 'black'})
g.map_upper(corrdot)
311/8: boston_data
311/9:
x='INDUS'
y='TAX'
boston_df = boston_data[[x,y]]
boston_df
311/10: boston_df.corr()
311/11:
#Multivariate outlier analysis
fig, ax = plt.subplots(figsize=(11,8.5))
ax.scatter(boston_df[x], boston_df[y])
ax.set_xlabel(x)
ax.set_ylabel(y)
311/12:
pca = PCA(n_components=boston_df.shape[1], svd_solver= 'full')
df = pd.DataFrame(pca.fit_transform(boston_df),index=boston_df.index,columns=boston_df.columns) #[i.lower()+'_pca' for i in boston_df.columns]
plt.scatter(x=df[x],y=df[y])
plt.title('PCA Graph')
311/13:
def cov_matrix(data, verbose=False):
    covariance_matrix = np.cov(data, rowvar=False)
    if is_pos_def(covariance_matrix):
        inv_covariance_matrix = np.linalg.inv(covariance_matrix)
        if is_pos_def(inv_covariance_matrix):
            return covariance_matrix, inv_covariance_matrix
        else:
            print("Error: Inverse of Covariance Matrix is not positive definite!")
    else:
        print("Error: Covariance Matrix is not positive definite!")

def MahalanobisDist(inv_cov_matrix, mean_distr, data, verbose=False):
    inv_covariance_matrix = inv_cov_matrix
    vars_mean = mean_distr
    diff = data - vars_mean
    md = []
    for i in range(len(diff)):
        md.append(np.sqrt(diff[i].dot(inv_covariance_matrix).dot(diff[i])))
    return np.array(md)

def MD_detectOutliers(dist, extreme=False, verbose=False):
    k = 3. if extreme else 2.
    threshold = np.mean(dist) * k
    outliers = []
    for i in range(len(dist)):
        if dist[i] >= threshold:
            outliers.append(i)  # index of the outlier
    return np.array(outliers)

def MD_threshold(dist, extreme=False, verbose=False):
    k = 3. if extreme else 2.
    threshold = np.mean(dist) * k
    return threshold

# Check that matrix is positive definite
def is_pos_def(A):
    if np.allclose(A, A.T):
        try:
            np.linalg.cholesky(A)
            return True
        except np.linalg.LinAlgError:
            return False
    else:
        return False
311/14:
cov_matrix = np.cov(df.values,rowvar=False)
inv_cov = np.linalg.inv(cov_matrix)
mean = df.values.mean(axis=0)

# Check matrices are positive definite:https://en.wikipedia.org/wiki/Definiteness_of_a_matrix 
assert is_pos_def(cov_matrix) and is_pos_def(inv_cov)
# Check matrices are invereses
np.matmul(cov_matrix,inv_cov).astype(np.float16)
311/15:
md = MahalanobisDist(inv_cov, mean, df.values, verbose=False)
threshold = 2#MD_threshold(md, extreme = False)
print("Threshold: "+ str(threshold))
plt.hist(list(md),bins=40)
plt.show()
311/16:
plt.figure()
sns.distplot(np.square(md), bins = 40, kde= False)
plt.xlim([0.0,15])

plt.figure()
sns.distplot(md,
             bins = 40, 
             kde= True, 
            color = 'green')
# plt.xlim([0.0,5])
plt.xlabel('Mahalanobis dist')
311/17:
# classify what data is an outlier  
# boston_df['anomaly'] = df['anomaly'] = md>threshold
# boston_df[boston_df.anomaly]
len(boston_df[md>threshold])
311/18:
plt.figure(figsize=(10,8))
plt.scatter(boston_df[x][md<=threshold], boston_df[y][md<=threshold])
plt.scatter(boston_df[x][md>threshold], boston_df[y][md>threshold])
plt.show()

plt.figure(figsize=(10,8))
plt.scatter(df[x][md<=threshold], df[y][md<=threshold])
plt.scatter(df[x][md>threshold], df[y][md>threshold])
# plt.plot([threshold]*len(df[x]_pca),df.indus_pca,colour)
313/1:
#Import the libraries
import numpy as np
import pandas as pd
from sklearn.datasets import load_boston,load_iris
319/1:
#Load the data
boston = load_boston()

#Find the dic keys
boston.keys()
319/2:
#Import the libraries
import numpy as np
import pandas as pd
from sklearn.datasets import load_boston,load_iris
   1: %history -g -f filename
